{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: index.html\n",
      "Scraping: about.html\n",
      "Scraping: exercises_and_problems.html\n",
      "Scraping: chap1.html\n",
      "Scraping: chap2.html\n",
      "Scraping: chap3.html\n",
      "Scraping: chap4.html\n",
      "Scraping: chap5.html\n",
      "Scraping: chap6.html\n",
      "Scraping: sai.html\n",
      "Scraping: acknowledgements.html\n",
      "Scraping: faq.html\n",
      "Scraping: supporters.html\n",
      "Scraping: bugfinder.html\n",
      "Scraping: http://www.deeplearning.net/tutorial/gettingstarted.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://www.deeplearning.net/tutorial/gettingstarted.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://www.deeplearning.net/tutorial/gettingstarted.html\n",
      "Scraping: http://www.scipy.org/install.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://www.scipy.org/install.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://www.scipy.org/install.html\n",
      "Scraping: http://peekaboo-vision.blogspot.de/2010/09/mnist-for-ever.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://peekaboo-vision.blogspot.de/2010/09/mnist-for-ever.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://peekaboo-vision.blogspot.de/2010/09/mnist-for-ever.html\n",
      "Scraping: http://www.nature.com/nature/journal/v427/n6972/full/427297a.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v427/n6972/full/427297a.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v427/n6972/full/427297a.html\n",
      "Scraping: http://www.jmlr.org/proceedings/papers/v15/glorot11a.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://www.jmlr.org/proceedings/papers/v15/glorot11a.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://www.jmlr.org/proceedings/papers/v15/glorot11a.html\n",
      "Scraping: http://colah.github.io/about.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://colah.github.io/about.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://colah.github.io/about.html\n",
      "Scraping: http://deeplearning.net/tutorial/lenet.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://deeplearning.net/tutorial/lenet.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://deeplearning.net/tutorial/lenet.html\n",
      "Scraping: http://deeplearning.net/software/theano/tutorial/using_gpu.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://deeplearning.net/software/theano/tutorial/using_gpu.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://deeplearning.net/software/theano/tutorial/using_gpu.html\n",
      "Scraping: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
      "Scraping: http://deeplearning.net/software/theano/index.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://deeplearning.net/software/theano/index.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://deeplearning.net/software/theano/index.html\n",
      "Scraping: http://research.google.com/pubs/pub38115.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://research.google.com/pubs/pub38115.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://research.google.com/pubs/pub38115.html\n",
      "Scraping: http://caffe.berkeleyvision.org/model_zoo.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://caffe.berkeleyvision.org/model_zoo.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://caffe.berkeleyvision.org/model_zoo.html\n",
      "Scraping: http://research.google.com/pubs/VincentVanhoucke.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://research.google.com/pubs/VincentVanhoucke.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://research.google.com/pubs/VincentVanhoucke.html\n",
      "Scraping: http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html\n",
      "Scraping: http://googleblog.blogspot.ca/2012/08/building-search-engine-of-future-one.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://googleblog.blogspot.ca/2012/08/building-search-engine-of-future-one.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://googleblog.blogspot.ca/2012/08/building-search-engine-of-future-one.html\n",
      "Scraping: http://www.nature.com/nature/journal/v437/n7055/full/nature04072.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v437/n7055/full/nature04072.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v437/n7055/full/nature04072.html\n",
      "Scraping: http://www.nature.com/nature/journal/v404/n6780/abs/404841a0.html\n",
      "Error scraping http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v404/n6780/abs/404841a0.html: 404 Client Error: Not Found for url: http://neuralnetworksanddeeplearning.com/http://www.nature.com/nature/journal/v404/n6780/abs/404841a0.html\n",
      "Content saved to nn_deeplearning_content.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "class NeuralNetScraper:\n",
    "    def __init__(self, base_url='http://neuralnetworksanddeeplearning.com/'):\n",
    "        self.base_url = base_url\n",
    "        self.scraped_content = {}\n",
    "\n",
    "    def extract_text_by_topic(self, soup):\n",
    "        \"\"\"Extract main topics and subtopics along with their content.\"\"\"\n",
    "        content = {}\n",
    "        main_heading = soup.find('h1')  # Assuming chapter title is in <h1>\n",
    "        \n",
    "        if main_heading:\n",
    "            chapter_title = main_heading.get_text(strip=True)\n",
    "            content[chapter_title] = {}\n",
    "\n",
    "            # Find all headings and paragraphs under the main content area\n",
    "            for section in soup.find_all(['h2', 'h3', 'p']):\n",
    "                if section.name in ['h2', 'h3']:\n",
    "                    subtopic_title = section.get_text(strip=True)\n",
    "                    content[chapter_title][subtopic_title] = []\n",
    "                elif section.name == 'p':\n",
    "                    # Append paragraphs to the last seen subtopic\n",
    "                    if chapter_title in content and content[chapter_title]:\n",
    "                        last_subtopic = list(content[chapter_title].keys())[-1]\n",
    "                        content[chapter_title][last_subtopic].append(section.get_text(strip=True))\n",
    "        \n",
    "        return content\n",
    "\n",
    "    def scrape_page(self, page_name):\n",
    "        \"\"\"Scrape a single page and extract chapter/subtopic content.\"\"\"\n",
    "        url = self.base_url + page_name\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract chapter/subtopic content\n",
    "            page_content = self.extract_text_by_topic(soup)\n",
    "            \n",
    "            # Find all links for subtopics\n",
    "            subtopic_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.html') and a['href'] != page_name]\n",
    "            \n",
    "            return page_content, subtopic_links\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            return {}, []\n",
    "\n",
    "    def scrape_all_pages(self, start_page='index.html'):\n",
    "        \"\"\"Scrape all chapters and their subtopics starting from the index page.\"\"\"\n",
    "        pages_to_visit = [start_page]\n",
    "        visited_pages = set()\n",
    "\n",
    "        while pages_to_visit:\n",
    "            current_page = pages_to_visit.pop(0)\n",
    "            if current_page in visited_pages:\n",
    "                continue\n",
    "            \n",
    "            print(f\"Scraping: {current_page}\")\n",
    "            visited_pages.add(current_page)\n",
    "            \n",
    "            chapter_content, subtopics = self.scrape_page(current_page)\n",
    "            self.scraped_content.update(chapter_content)\n",
    "            pages_to_visit.extend(subtopics)\n",
    "\n",
    "    def save_to_json(self, filename='nn_deeplearning_content.json'):\n",
    "        \"\"\"Save the scraped content to a JSON file.\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.scraped_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Content saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    scraper = NeuralNetScraper()\n",
    "    scraper.scrape_all_pages()\n",
    "    scraper.save_to_json()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
