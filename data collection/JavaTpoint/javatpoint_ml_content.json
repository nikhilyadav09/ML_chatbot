{
    "history-of-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Machine Learning Tutorial": "The Machine Learning Tutorial covers both the fundamentals and more complex ideas of machine learning. Students and professionals in the workforce can benefit from our machine learning tutorial. A rapidly developing field of technology, machine learning allows computers to automatically learn from previous data. For building mathematical models and making predictions based on historical data or information, machine learning employs a variety of algorithms. It is currently being used for a variety of tasks, including speech recognition, email filtering, auto-tagging on Facebook, a recommender system, and image recognition. You will learn about the many different methods of machine learning, including reinforcement learning, supervised learning, and unsupervised learning, in this machine learning tutorial. Regression and classification models, clustering techniques, hidden Markov models, and various sequential models will all be covered.",
        "What is Machine Learning": "In the real world, we are surrounded by humans who can learn everything from their experiences with their learning capability, and we have computers or machines which work on our instructions. But can a machine also learn from experiences or past data like a human does? So here comes the role ofMachine Learning. Introduction to Machine Learning A subset of artificial intelligence known as machine learning focuses primarily on the creation of algorithms that enable a computer to independently learn from data and previous experiences. Arthur Samuel first used the term \"machine learning\" in 1959. It could be summarized as follows: Without being explicitly programmed, machine learning enables a machine to automatically learn from data, improve performance from experiences, and predict things. Machine learning algorithms create a mathematical model that, without being explicitly programmed, aids in making predictions or decisions with the assistance of sample historical data, or training data. For the purpose of developing predictive models, machine learning brings together statistics and computer science. Algorithms that learn from historical data are either constructed or utilized in machine learning. The performance will rise in proportion to the quantity of information we provide. A machine can learn if it can gain more data to improve its performance.",
        "How does Machine Learning work": "A machine learning system builds prediction models, learns from previous data, and predicts the output of new data whenever it receives it. The amount of data helps to build a better model that accurately predicts the output, which in turn affects the accuracy of the predicted output. Let's say we have a complex problem in which we need to make predictions. Instead of writing code, we just need to feed the data to generic algorithms, which build the logic based on the data and predict the output. Our perspective on the issue has changed as a result of machine learning. The Machine Learning algorithm's operation is depicted in the following block diagram:",
        "Features of Machine Learning:": "Machine learning uses data to detect various patterns in a given dataset.It can learn from past data and improve automatically.It is a data-driven technology.Machine learning is much similar to data mining as it also deals with the huge amount of the data.",
        "Need for Machine Learning": "The demand for machine learning is steadily rising. Because it is able to perform tasks that are too complex for a person to directly implement, machine learning is required. Humans are constrained by our inability to manually access vast amounts of data; as a result, we require computer systems, which is where machine learning comes in to simplify our lives. By providing them with a large amount of data and allowing them to automatically explore the data, build models, and predict the required output, we can train machine learning algorithms. The cost function can be used to determine the amount of data and the machine learning algorithm's performance. We can save both time and money by using machine learning. The significance of AI can be handily perceived by its utilization's cases, Presently, AI is utilized in self-driving vehicles, digital misrepresentation identification, face acknowledgment, and companion idea by Facebook, and so on. Different top organizations, for example, Netflix and Amazon have constructed AI models that are utilizing an immense measure of information to examine the client interest and suggest item likewise. Following are some key points which show the importance of Machine Learning: Rapid increment in the production of dataSolving complex problems, which are difficult for a humanDecision making in various sector including financeFinding hidden patterns and extracting useful information from data.",
        "Classification of Machine Learning": "At a broad level, machine learning can be classified into three types: Supervised learningUnsupervised learningReinforcement learning",
        "1) Supervised Learning": "In supervised learning, sample labeled data are provided to the machine learning system for training, and the system then predicts the output based on the training data. The system uses labeled data to build a model that understands the datasets and learns about each one. After the training and processing are done, we test the model with sample data to see if it can accurately predict the output. The mapping of the input data to the output data is the objective of supervised learning. The managed learning depends on oversight, and it is equivalent to when an understudy learns things in the management of the educator. Spam filtering is an example of supervised learning. Supervised learning can be grouped further in two categories of algorithms: ClassificationRegression",
        "2) Unsupervised Learning": "Unsupervised learning is a learning method in which a machine learns without any supervision. The training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision. The goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns. In unsupervised learning, we don't have a predetermined result. The machine tries to find useful insights from the huge amount of data. It can be further classifieds into two categories of algorithms: ClusteringAssociation",
        "3) Reinforcement Learning": "Reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action. The agent learns automatically with these feedbacks and improves its performance. In reinforcement learning, the agent interacts with the environment and explores it. The goal of an agent is to get the most reward points, and hence, it improves its performance. The robotic dog, which automatically learns the movement of his arms, is an example of Reinforcement learning.",
        "History of Machine Learning": "Before some years (about 40-50 years), machine learning was science fiction, but today it is the part of our daily life. Machine learning is making our day to day life easy fromself-driving carstoAmazon virtual assistant \"Alexa\". However, the idea behind machine learning is so old and has a long history. Below some milestones are given which have occurred in the history of machine learning:",
        "The early history of Machine Learning (Pre-1940):": "1834:In 1834, Charles Babbage, the father of the computer, conceived a device that could be programmed with punch cards. However, the machine was never built, but all modern computers rely on its logical structure.1936:In 1936, Alan Turing gave a theory that how a machine can determine and execute a set of instructions.",
        "The era of stored program computers:": "1940:In 1940, the first manually operated computer, \"ENIAC\" was invented, which was the first electronic general-purpose computer. After that stored program computer such as EDSAC in 1949 and EDVAC in 1951 were invented.1943:In 1943, a human neural network was modeled with an electrical circuit. In 1950, the scientists started applying their idea to work and analyzed how human neurons might work.",
        "Computer machinery and intelligence:": "1950:In 1950, Alan Turing published a seminal paper, \"Computer Machinery and Intelligence,\" on the topic of artificial intelligence.In his paper, he asked, \"Can machines think?\"",
        "Machine intelligence in Games:": "1952:Arthur Samuel, who was the pioneer of machine learning, created a program that helped an IBM computer to play a checkers game. It performed better more it played.1959:In 1959, the term \"Machine Learning\" was first coined byArthur Samuel.",
        "The first \"AI\" winter:": "The duration of 1974 to 1980 was the tough time for AI and ML researchers, and this duration was called asAI winter.In this duration, failure of machine translation occurred, and people had reduced their interest from AI, which led to reduced funding by the government to the researches.",
        "Machine Learning from theory to reality": "1959:In 1959, the first neural network was applied to a real-world problem to remove echoes over phone lines using an adaptive filter.1985:In 1985, Terry Sejnowski and Charles Rosenberg invented a neural networkNETtalk, which was able to teach itself how to correctly pronounce 20,000 words in one week.1997:The IBM'sDeep blueintelligent computer won the chess game against the chess expert Garry Kasparov, and it became the first computer which had beaten a human chess expert.",
        "Machine Learning at 21stcentury": "2006: Geoffrey Hinton and his group presented the idea of profound getting the hang of utilizing profound conviction organizations.The Elastic Compute Cloud (EC2) was launched by Amazon to provide scalable computing resources that made it easier to create and implement machine learning models. 2007: Participants were tasked with increasing the accuracy of Netflix's recommendation algorithm when the Netflix Prize competition began.Support learning made critical progress when a group of specialists utilized it to prepare a PC to play backgammon at a top-notch level. 2008: Google delivered the Google Forecast Programming interface, a cloud-based help that permitted designers to integrate AI into their applications.Confined Boltzmann Machines (RBMs), a kind of generative brain organization, acquired consideration for their capacity to demonstrate complex information conveyances. 2009: Profound learning gained ground as analysts showed its viability in different errands, including discourse acknowledgment and picture grouping.The expression \"Large Information\" acquired ubiquity, featuring the difficulties and open doors related with taking care of huge datasets. 2010: The ImageNet Huge Scope Visual Acknowledgment Challenge (ILSVRC) was presented, driving progressions in PC vision, and prompting the advancement of profound convolutional brain organizations (CNNs). 2011: On Jeopardy! IBM's Watson defeated human champions., demonstrating the potential of question-answering systems and natural language processing. 2012: AlexNet, a profound CNN created by Alex Krizhevsky, won the ILSVRC, fundamentally further developing picture order precision and laying out profound advancing as a predominant methodology in PC vision.Google's Cerebrum project, drove by Andrew Ng and Jeff Dignitary, utilized profound figuring out how to prepare a brain organization to perceive felines from unlabeled YouTube recordings. 2013: Ian Goodfellow introduced generative adversarial networks (GANs), which made it possible to create realistic synthetic data.Google later acquired the startup DeepMind Technologies, which focused on deep learning and artificial intelligence. 2014: Facebook presented the DeepFace framework, which accomplished close human precision in facial acknowledgment.AlphaGo, a program created by DeepMind at Google, defeated a world champion Go player and demonstrated the potential of reinforcement learning in challenging games. 2015: Microsoft delivered the Mental Toolbox (previously known as CNTK), an open-source profound learning library.The performance of sequence-to-sequence models in tasks like machine translation was enhanced by the introduction of the idea of attention mechanisms. 2016: The goal of explainable AI, which focuses on making machine learning models easier to understand, received some attention.Google's DeepMind created AlphaGo Zero, which accomplished godlike Go abilities to play without human information, utilizing just support learning. 2017: Move learning acquired noticeable quality, permitting pretrained models to be utilized for different errands with restricted information.Better synthesis and generation of complex data were made possible by the introduction of generative models like variational autoencoders (VAEs) and Wasserstein GANs.These are only a portion of the eminent headways and achievements in AI during the predefined period. The field kept on advancing quickly past 2017, with new leap forwards, strategies, and applications arising.",
        "Machine Learning at present:": "The field of machine learning has made significant strides in recent years, and its applications are numerous, including self-driving cars, Amazon Alexa, Catboats, and the recommender system. It incorporates clustering, classification, decision tree, SVM algorithms, and reinforcement learning, as well as unsupervised and supervised learning. Present day AI models can be utilized for making different expectations, including climate expectation, sickness forecast, financial exchange examination, and so on.",
        "Prerequisites": "Before learning machine learning, you must have the basic knowledge of followings so that you can easily understand the concepts of machine learning: Fundamental knowledge of probability and linear algebra.The ability to code in any computer language, especially in Python language.Knowledge of Calculus, especially derivatives of single variable and multivariate functions.",
        "Audience": "Our Machine learning tutorial is designed to help beginner and professionals.",
        "Problems": "We assure you that you will not find any difficulty while learning our Machine learning tutorial. But if there is any mistake in this tutorial, kindly post the problem or error in the contact form so that we can improve it.",
        "Latest Courses": ""
    },
    "applications-of-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Applications of Machine learning": "Machine learning is a buzzword for today's technology, and it is growing very rapidly day by day. We are using machine learning in our daily life even without knowing it such as Google Maps, Google assistant, Alexa, etc. Below are some most trending real-world applications of Machine Learning:",
        "1. Image Recognition:": "Image recognition is one of the most common applications of machine learning. It is used to identify objects, persons, places, digital images, etc. The popular use case of image recognition and face detection is,Automatic friend tagging suggestion: Facebook provides us a feature of auto friend tagging suggestion. Whenever we upload a photo with our Facebook friends, then we automatically get a tagging suggestion with name, and the technology behind this is machine learning'sface detectionandrecognition algorithm. It is based on the Facebook project named \"Deep Face,\" which is responsible for face recognition and person identification in the picture.",
        "2. Speech Recognition": "While using Google, we get an option of \"Search by voice,\" it comes under speech recognition, and it's a popular application of machine learning. Speech recognition is a process of converting voice instructions into text, and it is also known as \"Speech to text\", or \"Computer speech recognition.\" At present, machine learning algorithms are widely used by various applications of speech recognition.Google assistant,Siri,Cortana, andAlexaare using speech recognition technology to follow the voice instructions.",
        "3. Traffic prediction:": "If we want to visit a new place, we take help of Google Maps, which shows us the correct path with the shortest route and predicts the traffic conditions. It predicts the traffic conditions such as whether traffic is cleared, slow-moving, or heavily congested with the help of two ways: Real Time locationof the vehicle form Google Map app and sensorsAverage time has takenon past days at the same time. Everyone who is using Google Map is helping this app to make it better. It takes information from the user and sends back to its database to improve the performance.",
        "4. Product recommendations:": "Machine learning is widely used by various e-commerce and entertainment companies such asAmazon,Netflix, etc., for product recommendation to the user. Whenever we search for some product on Amazon, then we started getting an advertisement for the same product while internet surfing on the same browser and this is because of machine learning. Google understands the user interest using various machine learning algorithms and suggests the product as per customer interest. As similar, when we use Netflix, we find some recommendations for entertainment series, movies, etc., and this is also done with the help of machine learning.",
        "5. Self-driving cars:": "One of the most exciting applications of machine learning is self-driving cars. Machine learning plays a significant role in self-driving cars. Tesla, the most popular car manufacturing company is working on self-driving car. It is using unsupervised learning method to train the car models to detect people and objects while driving.",
        "6. Email Spam and Malware Filtering:": "Whenever we receive a new email, it is filtered automatically as important, normal, and spam. We always receive an important mail in our inbox with the important symbol and spam emails in our spam box, and the technology behind this is Machine learning. Below are some spam filters used by Gmail: Content FilterHeader filterGeneral blacklists filterRules-based filtersPermission filters Some machine learning algorithms such asMulti-Layer Perceptron,Decision tree, andNaïve Bayes classifierare used for email spam filtering and malware detection.",
        "7. Virtual Personal Assistant:": "We have various virtual personal assistants such asGoogle assistant,Alexa,Cortana,Siri. As the name suggests, they help us in finding the information using our voice instruction. These assistants can help us in various ways just by our voice instructions such as Play music, call someone, Open an email, Scheduling an appointment, etc. These virtual assistants use machine learning algorithms as an important part. These assistant record our voice instructions, send it over the server on a cloud, and decode it using ML algorithms and act accordingly.",
        "8. Online Fraud Detection:": "Machine learning is making our online transaction safe and secure by detecting fraud transaction. Whenever we perform some online transaction, there may be various ways that a fraudulent transaction can take place such asfake accounts,fake ids, andsteal moneyin the middle of a transaction. So to detect this,Feed Forward Neural networkhelps us by checking whether it is a genuine transaction or a fraud transaction. For each genuine transaction, the output is converted into some hash values, and these values become the input for the next round. For each genuine transaction, there is a specific pattern which gets change for the fraud transaction hence, it detects it and makes our online transactions more secure.",
        "9. Stock Market trading:": "Machine learning is widely used in stock market trading. In the stock market, there is always a risk of up and downs in shares, so for this machine learning'slong short term memory neural networkis used for the prediction of stock market trends.",
        "10. Medical Diagnosis:": "In medical science, machine learning is used for diseases diagnoses. With this, medical technology is growing very fast and able to build 3D models that can predict the exact position of lesions in the brain. ",
        "11. Automatic Language Translation:": "Nowadays, if we visit a new place and we are not aware of the language then it is not a problem at all, as for this also machine learning helps us by converting the text into our known languages. Google's GNMT (Google Neural Machine Translation) provide this feature, which is a Neural Machine Learning that translates the text into our familiar language, and it called as automatic translation. The technology behind the automatic translation is a sequence to sequence learning algorithm, which is used with image recognition and translates the text from one language to another language.",
        "Latest Courses": ""
    },
    "machine-learning-life-cycle": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Machine learning Life cycle": "Machine learning has given the computer systems the abilities to automatically learn without being explicitly programmed. But how does a machine learning system work? So, it can be described using the life cycle of machine learning. Machine learning life cycle is a cyclic process to build an efficient machine learning project. The main purpose of the life cycle is to find a solution to the problem or project. Machine learning life cycle involves seven major steps, which are given below: Gathering DataData preparationData WranglingAnalyse DataTrain the modelTest the modelDeployment The most important thing in the complete process is to understand the problem and to know the purpose of the problem. Therefore, before starting the life cycle, we need to understand the problem because the good result depends on the better understanding of the problem. In the complete life cycle process, to solve a problem, we create a machine learning system called \"model\", and this model is created by providing \"training\". But to train a model, we need data, hence, life cycle starts by collecting data.",
        "1. Gathering Data:": "Data Gathering is the first step of the machine learning life cycle. The goal of this step is to identify and obtain all data-related problems. In this step, we need to identify the different data sources, as data can be collected from various sources such asfiles,database,internet, ormobile devices. It is one of the most important steps of the life cycle. The quantity and quality of the collected data will determine the efficiency of the output. The more will be the data, the more accurate will be the prediction. This step includes the below tasks: Identify various data sourcesCollect dataIntegrate the data obtained from different sources By performing the above task, we get a coherent set of data, also called as adataset. It will be used in further steps.",
        "2. Data preparation": "After collecting the data, we need to prepare it for further steps. Data preparation is a step where we put our data into a suitable place and prepare it to use in our machine learning training. In this step, first, we put all data together, and then randomize the ordering of data. This step can be further divided into two processes: Data exploration:It is used to understand the nature of data that we have to work with. We need to understand the characteristics, format, and quality of data.A better understanding of data leads to an effective outcome. In this, we find Correlations, general trends, and outliers.Data pre-processing:Now the next step is preprocessing of data for its analysis.",
        "3. Data Wrangling": "Data wrangling is the process of cleaning and converting raw data into a useable format. It is the process of cleaning the data, selecting the variable to use, and transforming the data in a proper format to make it more suitable for analysis in the next step. It is one of the most important steps of the complete process. Cleaning of data is required to address the quality issues. It is not necessary that data we have collected is always of our use as some of the data may not be useful. In real-world applications, collected data may have various issues, including: Missing ValuesDuplicate dataInvalid dataNoise So, we use various filtering techniques to clean the data. It is mandatory to detect and remove the above issues because it can negatively affect the quality of the outcome.",
        "4. Data Analysis": "Now the cleaned and prepared data is passed on to the analysis step. This step involves: Selection of analytical techniquesBuilding modelsReview the result The aim of this step is to build a machine learning model to analyze the data using various analytical techniques and review the outcome. It starts with the determination of the type of the problems, where we select the machine learning techniques such asClassification,Regression,Cluster analysis,Association, etc. then build the model using prepared data, and evaluate the model. Hence, in this step, we take the data and use machine learning algorithms to build the model.",
        "5. Train Model": "Now the next step is to train the model, in this step we train our model to improve its performance for better outcome of the problem. We use datasets to train the model using various machine learning algorithms. Training a model is required so that it can understand the various patterns, rules, and, features.",
        "6. Test Model": "Once our machine learning model has been trained on a given dataset, then we test the model. In this step, we check for the accuracy of our model by providing a test dataset to it. Testing the model determines the percentage accuracy of the model as per the requirement of project or problem.",
        "7. Deployment": "The last step of machine learning life cycle is deployment, where we deploy the model in the real-world system. If the above-prepared model is producing an accurate result as per our requirement with acceptable speed, then we deploy the model in the real system. But before deploying the project, we will check whether it is improving its performance using available data or not. The deployment phase is similar to making the final report for a project.",
        "Latest Courses": ""
    },
    "machine-learning-installing-anaconda-and-python": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Installing Anaconda and Python": "To learn machine learning, we will use the Python programming language in this tutorial. So, in order to use Python for machine learning, we need to install it in our computer system with compatibleIDEs (Integrated Development Environment). In this topic, we will learn to install Python and an IDE with the help ofAnaconda distribution. Anaconda distribution is a free and open-source platform for Python/R programming languages. It can be easily installed on any OS such as Windows, Linux, and MAC OS. It provides more than 1500 Python/R data science packages which are suitable for developing machine learning and deep learning models. Anaconda distribution provides installation of Python with various IDE's such asJupyter Notebook,Spyder,Anaconda prompt,etc. Hence it is a very convenient packaged solution which you can easily download and install in your computer. It will automatically install Python and some basic IDEs and libraries with it. Below some steps are given to show the downloading and installing process of Anaconda and IDE:",
        "Step-1: Download Anaconda Python:": "To download Anaconda in your system, firstly, open your favorite browser and type Download Anaconda Python, and then click on the first link as given in the below image. Alternatively, you can directly download it by clicking on this link, https://www.anaconda.com/distribution/#download-section.",
        "Step- 2: Install Anaconda Python (Python 3.7 version):": "Once the downloading process gets completed, go to downloads → double click on the \".exe\" file (Anaconda3-2019.03-Windows-x86_64.exe) of Anaconda. It will open a setup window for Anaconda installations as given in below image, then click on Next.",
        "Step- 3: Open Anaconda Navigator": "After successful installation of Anaconda, use Anaconda navigator to launch a Python IDE such as Spyder and Jupyter Notebook.To open Anaconda Navigator, click on window Keyand search for Anaconda navigator, and click on it. Consider the below image:",
        "Step- 4: Close the Spyder IDE.": "",
        "Latest Courses": ""
    },
    "difference-between-artificial-intelligence-and-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Difference between Artificial intelligence and Machine learning": "Artificial intelligence and machine learning are the part of computer science that are correlated with each other. These two technologies are the most trending technologies which are used for creating intelligent systems. Although these are two related technologies and sometimes people use them as a synonym for each other, but still both are the two different terms in various cases. On a broad level, we can differentiate both AI and ML as: AI is a bigger concept to create intelligent machines that can simulate human thinking capability and behavior, whereas, machine learning is an application or subset of AI that allows machines to learn from data without being programmed explicitly.",
        "Artificial Intelligence": "Artificial intelligence is a field of computer science which makes a computer system that can mimic human intelligence. It is comprised of two words \"Artificial\" and \"intelligence\", which means \"a human-made thinking power.\" Hence we can define it as, Artificial intelligence is a technology using which we can create intelligent systems that can simulate human intelligence. The Artificial intelligence system does not require to be pre-programmed, instead of that, they use such algorithms which can work with their own intelligence. It involves machine learning algorithms such as Reinforcement learning algorithm and deep learning neural networks. AI is being used in multiple places such as Siri, Google, AlphaGo, AI in Chess playing, etc. Based on capabilities, AI can be classified into three types: Weak AIGeneral AIStrong AI Currently, we are working with weak AI and general AI. The future of AI is Strong AI for which it is said that it will be intelligent than humans.",
        "Machine learning": "Machine learning is about extracting knowledge from the data. It can be defined as, Machine learning is a subfield of artificial intelligence, which enables machines to learn from past data or experiences without being explicitly programmed. Machine learning enables a computer system to make predictions or take some decisions using historical data without being explicitly programmed. Machine learning uses a massive amount of structured and semi-structured data so that a machine learning model can generate accurate result or give predictions based on that data. Machine learning works on algorithm which learn by it's own using historical data. It works only for specific domains such as if we are creating a machine learning model to detect pictures of dogs, it will only give result for dog images, but if we provide a new data like cat image then it will become unresponsive. Machine learning is being used in various places such as for online recommender system, for Google search algorithms, Email spam filter, Facebook Auto friend tagging suggestion, etc. It can be divided into three types: Supervised learningReinforcement learningUnsupervised learning",
        "Key differences between Artificial Intelligence (AI) and Machine learning (ML):": "",
        "Latest Courses": ""
    },
    "how-to-get-datasets-for-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "How to get datasets for Machine Learning": "The field of ML depends vigorously on datasets for preparing models and making precise predictions. Datasets assume a vital part in the progress of AIML projects and are fundamental for turning into a gifted information researcher. In this article, we will investigate the various sorts of datasets utilized in AI and give a definite aid on where to track down them.",
        "What is a dataset?": "A datasetis a collection of data in which data is arranged in some order. A dataset can contain any data from a series of an array to a database table. Below table shows an example of the dataset: A tabular dataset can be understood as a database table or matrix, where each column corresponds to aparticular variable,and each row corresponds to thefields of the dataset.The most supported file type for a tabular dataset is\"Comma Separated File,\"orCSV.But to store a \"tree-like data,\" we can use the JSON file more efficiently.",
        "Types of data in datasets": "Numerical data:Such as house price, temperature, etc.Categorical data:Such as Yes/No, True/False, Blue/green, etc.Ordinal data:These data are similar to categorical data but can be measured on the basis of comparison.",
        "Types of datasets": "Machine learning incorporates different domains, each requiring explicit sorts of datasets. A few normal sorts of datasets utilized in machine learning include:",
        "Image Datasets:": "Image datasets contain an assortment of images and are normally utilized in computer vision tasks such as image classification, object detection, and image segmentation. Examples : ImageNetCIFAR-10MNIST",
        "Text Datasets:": "Text datasets comprise textual information, like articles, books, or virtual entertainment posts. These datasets are utilized in NLP techniques like sentiment analysis, text classification, and machine translation. Examples : Gutenberg Task datasetIMDb film reviews dataset",
        "Time Series Datasets:": "Time series datasets include information focuses gathered after some time. They are generally utilized in determining, abnormality location, and pattern examination.Examples : Securities exchange informationClimate informationSensor readings.",
        "Tabular Datasets:": "Tabular datasets are organized information coordinated in tables or calculation sheets. They contain lines addressing examples or tests and segments addressing highlights or qualities. Tabular datasets are utilized for undertakings like relapse and arrangement. The dataset given before in the article is an illustration of a tabular dataset.",
        "Need of Dataset": "Completely ready and pre-handled datasets are significant for machine learning projects.They give the establishment to prepare exact and solid models. Notwithstanding, working with enormous datasets can introduce difficulties regarding the board and handling.To address these difficulties, productive information the executive's strategies and are expected to handle calculations.",
        "Data Pre-processing:": "Data pre-processing is a fundamental stage in preparing datasets for machine learning. It includes changing raw data into a configuration reasonable for model training. Normal pre-processing procedures incorporate data cleaning to eliminate irregularities or blunders, standardization to scale data inside a particular reach, highlight scaling to guarantee highlights have comparative ranges, and taking care of missing qualities through ascription or evacuation. During the development of the ML project, the developers completely rely on the datasets. In building ML applications, datasets are divided into two parts: Training dataset:Test Dataset",
        "Training Dataset and Test Dataset:": "In machine learning, datasets are ordinarily partitioned into two sections: the training dataset and the test dataset. The training dataset is utilized to prepare the machine learning model, while the test dataset is utilized to assess the model's exhibition. This division surveys the model's capacity, to sum up to inconspicuous data. It is fundamental to guarantee that the datasets are representative of the issue space and appropriately split to stay away from inclination or overfitting.",
        "Popular sources for Machine Learning datasets": "Below is the list of datasets which are freely available for the public to work on it:",
        "1. Kaggle Datasets": "",
        "2. UCI Machine Learning Repository": "The UCI Machine Learning Repository is an important asset that has been broadly utilized by scientists and specialists beginning around 1987. It contains a huge collection of datasets sorted by machine learning tasks such as regression, classification, and clustering. Remarkable datasets in the storehouse incorporate the Iris dataset, Vehicle Assessment dataset, and Poker Hand dataset.",
        "3. Datasets via AWS": "",
        "4. Google's Dataset Search Engine": "Google's Dataset Web index helps scientists find and access important datasets from different sources across the web. It files datasets from areas like sociologies, science, and environmental science. Specialists can utilize catchphrases to find datasets, channel results in light of explicit standards, and access the datasets straightforwardly from the source.",
        "5. Microsoft Datasets": "The Microsoft has launched the\"Microsoft Research Open data\"repository with the collection of free datasets in various areas such asnatural language processing, computer vision, and domain-specific sciences.It gives admittance to assorted and arranged datasets that can be significant for machine learning projects.",
        "6. Awesome Public Dataset Collection": "",
        "7. Government Datasets": "There are different sources to get government-related data. Various countries publish government data for public use collected by them from different departments. The goal of providing these datasets is to increase transparency of government work among the people and to use the data in an innovative approach. Below are some links of government datasets: Indian Government datasetUS Government DatasetNorthern Ireland Public Sector DatasetsEuropean Union Open Data Portal",
        "8. Computer Vision Datasets": "",
        "9. Scikit-learn dataset": "Scikit-learn, a well-known machine learning library in Python, gives a few underlying datasets to practice and trial and error. These datasets are open through the sci-kit-learn Programming interface and can be utilized for learning different machine-learning calculations. Scikit-learn offers both toy datasets, which are little and improved, and genuine world datasets with greater intricacy. Instances of sci-kit-learn datasets incorporate the Iris dataset, the Boston Lodging dataset, and the Wine dataset.",
        "Data Ethics and Privacy:": "Data ethics and privacy are basic contemplations in machine learning projects. It is fundamental to guarantee that data is gathered and utilized morally, regarding privacy freedoms and observing pertinent regulations and guidelines. Data experts ought to go to lengths to safeguard data privacy, get appropriate assent, and handle delicate data mindfully. Assets, for example, moral rules and privacy structures can give direction on keeping up with moral practices in data assortment and use.",
        "Conclusion:": "In conclusion, datasets structure the groundwork of effective machine-learning projects. Understanding the various kinds of datasets, the significance of data pre-processing, and the job of training and testing datasets are key stages towards building powerful models. By utilizing well-known sources, for example, Kaggle, UCI Machine Learning Repository, AWS, Google's Dataset Search, Microsoft Datasets, and government datasets, data researchers and specialists can get to an extensive variety of datasets for their machine learning projects. It is fundamental to consider data ethics and privacy all through the whole data lifecycle to guarantee mindful and moral utilization of data. With the right datasets and moral practices, machine learning models can accomplish exact predictions and drive significant bits of knowledge.",
        "Latest Courses": ""
    },
    "data-preprocessing-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Data Preprocessing in Machine learning": "Data preprocessing is a process of preparing the raw data and making it suitable for a machine learning model. It is the first and crucial step while creating a machine learning model. When creating a machine learning project, it is not always a case that we come across the clean and formatted data. And while doing any operation with data, it is mandatory to clean it and put in a formatted way. So for this, we use data preprocessing task.",
        "Why do we need Data Preprocessing?": "A real-world data generally contains noises, missing values, and maybe in an unusable format which cannot be directly used for machine learning models. Data preprocessing is required tasks for cleaning the data and making it suitable for a machine learning model which also increases the accuracy and efficiency of a machine learning model. It involves below steps: Getting the datasetImporting librariesImporting datasetsFinding Missing DataEncoding Categorical DataSplitting dataset into training and test setFeature scaling",
        "1) Get the Dataset": "To create a machine learning model, the first thing we required is a dataset as a machine learning model completely works on data. The collected data for a particular problem in a proper format is known as thedataset. Dataset may be of different formats for different purposes, such as, if we want to create a machine learning model for business purpose, then dataset will be different with the dataset required for a liver patient. So each dataset is different from another dataset. To use the dataset in our code, we usually put it into a CSVfile. However, sometimes, we may also need to use an HTML or xlsx file.",
        "What is a CSV File?": "CSV stands for \"Comma-Separated Values\" files; it is a file format which allows us to save the tabular data, such as spreadsheets. It is useful for huge datasets and can use these datasets in programs. Here we will use a demo dataset for data preprocessing, and for practice, it can be downloaded from here, \"https://www.superdatascience.com/pages/machine-learning. For real-world problems, we can download datasets online from various sources such as https://www.kaggle.com/uciml/datasets, https://archive.ics.uci.edu/ml/index.php etc. We can also create our dataset by gathering data using various API with Python and put that data into a .csv file.",
        "2) Importing Libraries": "In order to perform data preprocessing using Python, we need to import some predefined Python libraries. These libraries are used to perform some specific jobs. There are three specific libraries that we will use for data preprocessing, which are: Numpy:Numpy Python library is used for including any type of mathematical operation in the code. It is the fundamental package for scientific calculation in Python. It also supports to add large, multidimensional arrays and matrices. So, in Python, we can import it as: Here we have usednm, which is a short name for Numpy, and it will be used in the whole program. Matplotlib:The second library ismatplotlib, which is a Python 2D plotting library, and with this library, we need to import a sub-librarypyplot. This library is used to plot any type of charts in Python for the code. It will be imported as below: Here we have used mpt as a short name for this library. Pandas:The last library is the Pandas library, which is one of the most famous Python libraries and used for importing and managing the datasets. It is an open-source data manipulation and analysis library. It will be imported as below: Here, we have used pd as a short name for this library. Consider the below image:",
        "3) Importing the Datasets": "Now we need to import the datasets which we have collected for our machine learning project. But before importing a dataset, we need to set the current directory as a working directory. To set a working directory in Spyder IDE, we need to follow the below steps: Save your Python file in the directory which contains dataset.Go to File explorer option in Spyder IDE, and select the required directory.Click on F5 button or run option to execute the file. Here, in the below image, we can see the Python file along with required dataset. Now, the current folder is set as a working directory.",
        "4) Handling Missing data:": "The next step of data preprocessing is to handle missing data in the datasets. If our dataset contains some missing data, then it may create a huge problem for our machine learning model. Hence it is necessary to handle missing values present in the dataset. Ways to handle missing data: There are mainly two ways to handle missing data, which are: By deleting the particular row:The first way is used to commonly deal with null values. In this way, we just delete the specific row or column which consists of null values. But this way is not so efficient and removing data may lead to loss of information which will not give the accurate output. By calculating the mean:In this way, we will calculate the mean of that column or row which contains any missing value and will put it on the place of missing value. This strategy is useful for the features which have numeric data such as age, salary, year, etc. Here, we will use this approach. To handle missing values, we will useScikit-learnlibrary in our code, which contains various libraries for building machine learning models. Here we will useImputerclass ofsklearn.preprocessinglibrary. Below is the code for it: Output: As we can see in the above output, the missing values have been replaced with the means of rest column values.",
        "5) Encoding Categorical data:": "Categorical data is data which has some categories such as, in our dataset; there are two categorical variable,Country, andPurchased. Since machine learning model completely works on mathematics and numbers, but if our dataset would have a categorical variable, then it may create trouble while building the model. So it is necessary to encode these categorical variables into numbers. For Country variable: Firstly, we will convert the country variables into categorical data. So to do this, we will useLabelEncoder()class frompreprocessinglibrary. Output: Explanation: In above code, we have importedLabelEncoderclass ofsklearn library. This class has successfully encoded the variables into digits. But in our case, there are three country variables, and as we can see in the above output, these variables are encoded into 0, 1, and 2. By these values, the machine learning model may assume that there is some correlation between these variables which will produce the wrong output. So to remove this issue, we will usedummy encoding. Dummy Variables: Dummy variables are those variables which have values 0 or 1. The 1 value gives the presence of that variable in a particular column, and rest variables become 0. With dummy encoding, we will have a number of columns equal to the number of categories. In our dataset, we have 3 categories so it will produce three columns having 0 and 1 values. For Dummy Encoding, we will useOneHotEncoderclass ofpreprocessinglibrary. Output: As we can see in the above output, all the variables are encoded into numbers 0 and 1 and divided into three columns. It can be seen more clearly in the variables explorer section, by clicking on x option as:",
        "6) Splitting the Dataset into the Training set and Test set": "In machine learning data preprocessing, we divide our dataset into a training set and test set. This is one of the crucial steps of data preprocessing as by doing this, we can enhance the performance of our machine learning model. Suppose, if we have given training to our machine learning model by a dataset and we test it by a completely different dataset. Then, it will create difficulties for our model to understand the correlations between the models. If we train our model very well and its training accuracy is also very high, but we provide a new dataset to it, then it will decrease the performance. So we always try to make a machine learning model which performs well with the training set and also with the test dataset. Here, we can define these datasets as:",
        "7) Feature Scaling": "Feature scaling is the final step of data preprocessing in machine learning. It is a technique to standardize the independent variables of the dataset in a specific range. In feature scaling, we put our variables in the same range and in the same scale so that no any variable dominate the other variable. Consider the below dataset:",
        "Latest Courses": ""
    },
    "supervised-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Supervised Machine Learning": "Supervised learning is the types of machine learning in which machines are trained using well \"labelled\" training data, and on basis of that data, machines predict the output. The labelled data means some input data is already tagged with the correct output. In supervised learning, the training data provided to the machines work as the supervisor that teaches the machines to predict the output correctly. It applies the same concept as a student learns in the supervision of the teacher. Supervised learning is a process of providing input data as well as correct output data to the machine learning model. The aim of a supervised learning algorithm is tofind a mapping function to map the input variable(x) with the output variable(y). In the real-world, supervised learning can be used forRisk Assessment, Image classification, Fraud Detection, spam filtering, etc.",
        "How Supervised Learning Works?": "In supervised learning, models are trained using labelled dataset, where the model learns about each type of data. Once the training process is completed, the model is tested on the basis of test data (a subset of the training set), and then it predicts the output. The working of Supervised learning can be easily understood by the below example and diagram: Suppose we have a dataset of different types of shapes which includes square, rectangle, triangle, and Polygon. Now the first step is that we need to train the model for each shape. If the given shape has four sides, and all the sides are equal, then it will be labelled as aSquare.If the given shape has three sides, then it will be labelled as atriangle.If the given shape has six equal sides then it will be labelled ashexagon. Now, after training, we test our model using the test set, and the task of the model is to identify the shape. The machine is already trained on all types of shapes, and when it finds a new shape, it classifies the shape on the bases of a number of sides, and predicts the output.",
        "Steps Involved in Supervised Learning:": "First Determine the type of training datasetCollect/Gather the labelled training data.Split the training dataset into trainingdataset, test dataset, and validation dataset.Determine the input features of the training dataset, which should have enough knowledge so that the model can accurately predict the output.Determine the suitable algorithm for the model, such as support vector machine, decision tree, etc.Execute the algorithm on the training dataset. Sometimes we need validation sets as the control parameters, which are the subset of training datasets.Evaluate the accuracy of the model by providing the test set. If the model predicts the correct output, which means our model is accurate.",
        "Types of supervised Machine learning Algorithms:": "Supervised learning can be further divided into two types of problems: 1. Regression Regression algorithms are used if there is a relationship between the input variable and the output variable. It is used for the prediction of continuous variables, such as Weather forecasting, Market Trends, etc. Below are some popular Regression algorithms which come under supervised learning: Linear RegressionRegression TreesNon-Linear RegressionBayesian Linear RegressionPolynomial Regression 2. Classification Classification algorithms are used when the output variable is categorical, which means there are two classes such as Yes-No, Male-Female, True-false, etc. Spam Filtering, Random ForestDecision TreesLogistic RegressionSupport vector Machines",
        "Advantages of Supervised learning:": "With the help of supervised learning, the model can predict the output on the basis of prior experiences.In supervised learning, we can have an exact idea about the classes of objects.Supervised learning model helps us to solve various real-world problems such asfraud detection, spam filtering, etc.",
        "Disadvantages of supervised learning:": "Supervised learning models are not suitable for handling the complex tasks.Supervised learning cannot predict the correct output if the test data is different from the training dataset.Training required lots of computation times.In supervised learning, we need enough knowledge about the classes of object.",
        "Latest Courses": ""
    },
    "unsupervised-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Unsupervised Machine Learning": "In the previous topic, we learned supervised machine learning in which models are trained using labeled data under the supervision of training data. But there may be many cases in which we do not have labeled data and need to find the hidden patterns from the given dataset. So, to solve such types of cases in machine learning, we need unsupervised learning techniques.",
        "What is Unsupervised Learning?": "As the name suggests, unsupervised learning is a machine learning technique in which models are not supervised using training dataset. Instead, models itself find the hidden patterns and insights from the given data. It can be compared to learning which takes place in the human brain while learning new things. It can be defined as: Unsupervised learning is a type of machine learning in which models are trained using unlabeled dataset and are allowed to act on that data without any supervision. Unsupervised learning cannot be directly applied to a regression or classification problem because unlike supervised learning, we have the input data but no corresponding output data. The goal of unsupervised learning is tofind the underlying structure of dataset, group that data according to similarities, and represent that dataset in a compressed format. Example:Suppose the unsupervised learning algorithm is given an input dataset containing images of different types of cats and dogs. The algorithm is never trained upon the given dataset, which means it does not have any idea about the features of the dataset. The task of the unsupervised learning algorithm is to identify the image features on their own. Unsupervised learning algorithm will perform this task by clustering the image dataset into the groups according to similarities between images.",
        "Why use Unsupervised Learning?": "Below are some main reasons which describe the importance of Unsupervised Learning: Unsupervised learning is helpful for finding useful insights from the data.Unsupervised learning is much similar as a human learns to think by their own experiences, which makes it closer to the real AI.Unsupervised learning works on unlabeled and uncategorized data which make unsupervised learning more important.In real-world, we do not always have input data with the corresponding output so to solve such cases, we need unsupervised learning.",
        "Working of Unsupervised Learning": "Working of unsupervised learning can be understood by the below diagram: Here, we have taken an unlabeled input data, which means it is not categorized and corresponding outputs are also not given. Now, this unlabeled input data is fed to the machine learning model in order to train it. Firstly, it will interpret the raw data to find the hidden patterns from the data and then will apply suitable algorithms such as k-means clustering, Decision tree, etc. Once it applies the suitable algorithm, the algorithm divides the data objects into groups according to the similarities and difference between the objects.",
        "Types of Unsupervised Learning Algorithm:": "The unsupervised learning algorithm can be further categorized into two types of problems: Clustering: Clustering is a method of grouping the objects into clusters such that objects with most similarities remains into a group and has less or no similarities with the objects of another group. Cluster analysis finds the commonalities between the data objects and categorizes them as per the presence and absence of those commonalities.Association: An association rule is an unsupervised learning method which is used for finding the relationships between variables in the large database. It determines the set of items that occurs together in the dataset. Association rule makes marketing strategy more effective. Such as people who buy X item (suppose a bread) are also tend to purchase Y (Butter/Jam) item. A typical example of Association rule is Market Basket Analysis.",
        "Unsupervised Learning algorithms:": "Below is the list of some popular unsupervised learning algorithms: K-means clusteringKNN (k-nearest neighbors)Hierarchal clusteringAnomaly detectionNeural NetworksPrinciple Component AnalysisIndependent Component AnalysisApriori algorithmSingular value decomposition",
        "Advantages of Unsupervised Learning": "Unsupervised learning is used for more complex tasks as compared to supervised learning because, in unsupervised learning, we don't have labeled input data.Unsupervised learning is preferable as it is easy to get unlabeled data in comparison to labeled data.",
        "Disadvantages of Unsupervised Learning": "Unsupervised learning is intrinsically more difficult than supervised learning as it does not have corresponding output.The result of the unsupervised learning algorithm might be less accurate as input data is not labeled, and algorithms do not know the exact output in advance.",
        "Latest Courses": ""
    },
    "difference-between-supervised-and-unsupervised-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Difference between Supervised and Unsupervised Learning": "Supervised and Unsupervised learning are the two techniques of machine learning. But both the techniques are used in different scenarios and with different datasets. Below the explanation of both learning methods along with their difference table is given.",
        "Supervised Machine Learning:": "Supervised learning is a machine learning method in which models are trained using labeled data. In supervised learning, models need to find the mapping function to map the input variable (X) with the output variable (Y). Supervised learning needs supervision to train the model, which is similar to as a student learns things in the presence of a teacher. Supervised learning can be used for two types of problems:ClassificationandRegression. Learn moreSupervised Machine Learning Example:Suppose we have an image of different types of fruits. The task of our supervised learning model is to identify the fruits and classify them accordingly. So to identify the image in supervised learning, we will give the input data as well as output for that, which means we will train the model by the shape, size, color, and taste of each fruit. Once the training is completed, we will test the model by giving the new set of fruit. The model will identify the fruit and predict the output using a suitable algorithm.",
        "Unsupervised Machine Learning:": "Unsupervised learning is another machine learning method in which patterns inferred from the unlabeled input data. The goal of unsupervised learning is to find the structure and patterns from the input data. Unsupervised learning does not need any supervision. Instead, it finds patterns from the data by its own. Learn moreUnsupervised Machine Learning Unsupervised learning can be used for two types of problems:ClusteringandAssociation. Example:To understand the unsupervised learning, we will use the example given above. So unlike supervised learning, here we will not provide any supervision to the model. We will just provide the input dataset to the model and allow the model to find the patterns from the data. With the help of a suitable algorithm, the model will train itself and divide the fruits into different groups according to the most similar features between them. The main differences between Supervised and Unsupervised learning are given below:",
        "Latest Courses": ""
    },
    "regression-analysis-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Regression Analysis in Machine learning": "Regression analysis is a statistical method to model the relationship between a dependent (target) and independent (predictor) variables with one or more independent variables. More specifically, Regression analysis helps us to understand how the value of the dependent variable is changing corresponding to an independent variable when other independent variables are held fixed. It predicts continuous/real values such astemperature, age, salary, price,etc. We can understand the concept of regression analysis using the below example: Example:Suppose there is a marketing company A, who does various advertisement every year and get sales on that. The below list shows the advertisement made by the company in the last 5 years and the corresponding sales: Now, the company wants to do the advertisement of $200 in the year 2019and wants to know the prediction about the sales for this year. So to solve such type of prediction problems in machine learning, we need regression analysis. Regression is asupervised learning techniquewhich helps in finding the correlation between variables and enables us to predict the continuous output variable based on the one or more predictor variables. It is mainly used forprediction, forecasting, time series modeling, and determining the causal-effect relationship between variables. In Regression, we plot a graph between the variables which best fits the given datapoints, using this plot, the machine learning model can make predictions about the data. In simple words,\"Regression shows a line or curve that passes through all the datapoints on target-predictor graph in such a way that the vertical distance between the datapoints and the regression line is minimum.\"The distance between datapoints and line tells whether a model has captured a strong relationship or not. Some examples of regression can be as: Prediction of rain using temperature and other factorsDetermining Market trendsPrediction of road accidents due to rash driving.",
        "Terminologies Related to the Regression Analysis:": "Dependent Variable:The main factor in Regression analysis which we want to predict or understand is called the dependent variable. It is also calledtarget variable.Independent Variable:The factors which affect the dependent variables or which are used to predict the values of the dependent variables are called independent variable, also called as apredictor.Outliers:Outlier is an observation which contains either very low value or very high value in comparison to other observed values. An outlier may hamper the result, so it should be avoided.Multicollinearity:If the independent variables are highly correlated with each other than other variables, then such condition is called Multicollinearity. It should not be present in the dataset, because it creates problem while ranking the most affecting variable.Underfitting and Overfitting:If our algorithm works well with the training dataset but not well with test dataset, then such problem is calledOverfitting. And if our algorithm does not perform well even with training dataset, then such problem is calledunderfitting.",
        "Why do we use Regression Analysis?": "As mentioned above, Regression analysis helps in the prediction of a continuous variable. There are various scenarios in the real world where we need some future predictions such as weather condition, sales prediction, marketing trends, etc., for such case we need some technology which can make predictions more accurately. So for such case we need Regression analysis which is a statistical method and used in machine learning and data science. Below are some other reasons for using Regression analysis: Regression estimates the relationship between the target and the independent variable.It is used to find the trends in data.It helps to predict real/continuous values.By performing the regression, we can confidently determine themost important factor, the least important factor, and how each factor is affecting the other factors.",
        "Types of Regression": "There are various types of regressions which are used in data science and machine learning. Each type has its own importance on different scenarios, but at the core, all the regression methods analyze the effect of the independent variable on dependent variables. Here we are discussing some important types of regression which are given below: Linear RegressionLogistic RegressionPolynomial RegressionSupport Vector RegressionDecision Tree RegressionRandom Forest RegressionRidge RegressionLasso Regression:",
        "Linear Regression:": "Linear regression is a statistical regression method which is used for predictive analysis.It is one of the very simple and easy algorithms which works on regression and shows the relationship between the continuous variables.It is used for solving the regression problem in machine learning.Linear regression shows the linear relationship between the independent variable (X-axis) and the dependent variable (Y-axis), hence called linear regression.If there is only one input variable (x), then such linear regression is calledsimple linear regression. And if there is more than one input variable, then such linear regression is calledmultiple linear regression.The relationship between variables in the linear regression model can be explained using the below image. Here we are predicting the salary of an employee on the basis ofthe year of experience. Below is the mathematical equation for Linear regression: Here, Y = dependent variables (target variables),X= Independent variables (predictor variables),a and b are the linear coefficients Some popular applications of linear regression are: Analyzing trends and sales estimatesSalary forecastingReal estate predictionArriving at ETAs in traffic.",
        "Logistic Regression:": "Logistic regression is another supervised learning algorithm which is used to solve the classification problems. Inclassification problems, we have dependent variables in a binary or discrete format such as 0 or 1.Logistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, Spam or not spam, etc.It is a predictive analysis algorithm which works on the concept of probability.Logistic regression is a type of regression, but it is different from the linear regression algorithm in the term how they are used.Logistic regression usessigmoid functionor logistic function which is a complex cost function. This sigmoid function is used to model the data in logistic regression. The function can be represented as: f(x)= Output between the 0 and 1 value.x= input to the functione= base of natural logarithm. When we provide the input values (data) to the function, it gives the S-curve as follows: It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values below the threshold level are rounded up to 0. There are three types of logistic regression: Binary(0/1, pass/fail)Multi(cats, dogs, lions)Ordinal(low, medium, high)",
        "Polynomial Regression:": "Polynomial Regression is a type of regression which models thenon-linear datasetusing a linear model.It is similar to multiple linear regression, but it fits a non-linear curve between the value of x and corresponding conditional values of y.Suppose there is a dataset which consists of datapoints which are present in a non-linear fashion, so for such case, linear regression will not best fit to those datapoints. To cover such datapoints, we need Polynomial regression.In Polynomial regression, the original features are transformed into polynomial features of given degree and then modeled using a linear model.Which means the datapoints are best fitted using a polynomial line. The equation for polynomial regression also derived from linear regression equation that means Linear regression equation Y= b0+ b1x, is transformed into Polynomial regression equation Y= b0+b1x+ b2x2+ b3x3+.....+ bnxn.Here Y is thepredicted/target output, b0, b1,... bnare the regression coefficients. x is ourindependent/input variable.The model is still linear as the coefficients are still linear with quadratic",
        "Support Vector Regression:": "Support Vector Machine is a supervised learning algorithm which can be used for regression as well as classification problems. So if we use it for regression problems, then it is termed as Support Vector Regression. Support Vector Regression is a regression algorithm which works for continuous variables. Below are some keywords which are used inSupport Vector Regression: Kernel:It is a function used to map a lower-dimensional data into higher dimensional data.Hyperplane:In general SVM, it is a separation line between two classes, but in SVR, it is a line which helps to predict the continuous variables and cover most of the datapoints.Boundary line:Boundary lines are the two lines apart from hyperplane, which creates a margin for datapoints.Support vectors:Support vectors are the datapoints which are nearest to the hyperplane and opposite class. In SVR, we always try to determine a hyperplane with a maximum margin, so that maximum number of datapoints are covered in that margin.The main goal of SVR is to consider the maximum datapoints within the boundary lines and the hyperplane (best-fit line) must contain a maximum number of datapoints. Consider the below image: Here, the blue line is called hyperplane, and the other two lines are known as boundary lines.",
        "Decision Tree Regression:": "Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the \"test\" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset.",
        "Ridge Regression:": "Ridge regression is one of the most robust versions of linear regression in which a small amount of bias is introduced so that we can get better long term predictions.The amount of bias added to the model is known asRidge Regression penalty. We can compute this penalty term by multiplying with the lambda to the squared weight of each individual features.The equation for ridge regression will be: A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called asL2 regularization.It helps to solve the problems if we have more parameters than samples.",
        "Lasso Regression:": "Lasso regression is another regularization technique to reduce the complexity of the model.It is similar to the Ridge Regression except that penalty term contains only the absolute weights instead of a square of weights.Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.It is also called asL1 regularization. The equation for Lasso regression will be:",
        "Latest Courses": ""
    },
    "linear-regression-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Linear Regression in Machine Learning": "Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such assales, salary, age, product price,etc. Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable. The linear regression model provides a sloped straight line representing the relationship between the variables. Consider the below image: Mathematically, we can represent a linear regression as: Here, Y= Dependent Variable (Target Variable)X= Independent Variable (predictor Variable)a0= intercept of the line (Gives an additional degree of freedom)a1 = Linear regression coefficient (scale factor to each input value).ε = random error The values for x and y variables are training datasets for Linear Regression model representation.",
        "Types of Linear Regression": "Linear regression can be further divided into two types of the algorithm: Simple Linear Regression:If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.Multiple Linear regression:If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.",
        "Linear Regression Line": "A linear line showing the relationship between the dependent and independent variables is called aregression line. A regression line can show two types of relationship: Positive Linear Relationship:If the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a relationship is termed as a Positive linear relationship. Negative Linear Relationship:If the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.",
        "Finding the best fit line:": "When working with linear regression, our main goal is to find the best fit line that means the error between predicted values and actual values should be minimized. The best fit line will have the least error. The different values for weights or the coefficient of lines (a0, a1) gives a different line of regression, so we need to calculate the best values for a0and a1to find the best fit line, so to calculate this we use cost function.",
        "Cost function-": "The different values for weights or coefficient of lines (a0, a1) gives the different line of regression, and the cost function is used to estimate the values of the coefficient for the best fit line.Cost function optimizes the regression coefficients or weights. It measures how a linear regression model is performing.We can use the cost function to find the accuracy of themapping function, which maps the input variable to the output variable. This mapping function is also known asHypothesis function. For Linear Regression, we use theMean Squared Error (MSE)cost function, which is the average of squared error occurred between the predicted values and actual values. It can be written as: For the above linear equation, MSE can be calculated as: Where, N=Total number of observationYi = Actual value(a1xi+a0)= Predicted value. Residuals:The distance between the actual value and predicted values is called residual. If the observed points are far from the regression line, then the residual will be high, and so cost function will high. If the scatter points are close to the regression line, then the residual will be small and hence the cost function.",
        "Gradient Descent:": "Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.A regression model uses gradient descent to update the coefficients of the line by reducing the cost function.It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function.",
        "Model Performance:": "The Goodness of fit determines how the line of regression fits the set of observations. The process of finding the best model out of various models is calledoptimization. It can be achieved by below method: 1. R-squared method: R-squared is a statistical method that determines the goodness of fit.It measures the strength of the relationship between the dependent and independent variables on a scale of 0-100%.The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.It is also called acoefficient of determination,orcoefficient of multiple determinationfor multiple regression.It can be calculated from the below formula:",
        "Assumptions of Linear Regression": "Below are some important assumptions of Linear Regression. These are some formal checks while building a Linear Regression model, which ensures to get the best possible result from the given dataset. Linear relationship between the features and target:Linear regression assumes the linear relationship between the dependent and independent variables.Small or no multicollinearity between the features:Multicollinearity means high-correlation between the independent variables. Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables. Or we can say, it is difficult to determine which predictor variable is affecting the target variable and which is not. So, the model assumes either little or no multicollinearity between the features or independent variables.Homoscedasticity Assumption:Homoscedasticity is a situation when the error term is the same for all the values of independent variables. With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot.Normal distribution of error terms:Linear regression assumes that the error term should follow the normal distribution pattern. If error terms are not normally distributed, then confidence intervals will become either too wide or too narrow, which may cause difficulties in finding coefficients.It can be checked using theq-q plot. If the plot shows a straight line without any deviation, which means the error is normally distributed.No autocorrelations:The linear regression model assumes no autocorrelation in error terms. If there will be any correlation in the error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if there is a dependency between residual errors.",
        "Latest Courses": ""
    },
    "simple-linear-regression-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Simple Linear Regression in Machine Learning": "Simple Linear Regression is a type of Regression algorithms that models the relationship between a dependent variable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression. The key point in Simple Linear Regression is that thedependent variable must be a continuous/real value. However, the independent variable can be measured on continuous or categorical values. Simple Linear regression algorithm has mainly two objectives: Model the relationship between the two variables.Such as the relationship between Income and expenditure, experience and Salary, etc.Forecasting new observations.Such as Weather forecasting according to temperature, Revenue of a company according to the investments in a year, etc.",
        "Simple Linear Regression Model:": "The Simple Linear Regression model can be represented using the below equation: Where, a0= It is the intercept of the Regression line (can be obtained putting x=0)a1= It is the slope of the regression line, which tells whether the line is increasing or decreasing.ε = The error term. (For a good model it will be negligible)",
        "Implementation of Simple Linear Regression Algorithm using Python": "Problem Statement example for Simple Linear Regression: Here we are taking a dataset that has two variables: salary (dependent variable) and experience (Independent variable). The goals of this problem is: We want to find out if there is any correlation between these two variablesWe will find the best fit line for the dataset.How the dependent variable is changing by changing the independent variable. In this section, we will create a Simple Linear Regression model to find out the best fitting line for representing the relationship between these two variables. To implement the Simple Linear regression model in machine learning using Python, we need to follow the below steps: Step-1: Data Pre-processing The first step for creating the Simple Linear Regression model isdata pre-processing. We have already done it earlier in this tutorial. But there will be some changes, which are given in the below steps: First, we will import the three important libraries, which will help us for loading the dataset, plotting the graphs, and creating the Simple Linear Regression model. Next, we will load the dataset into our code: By executing the above line of code (ctrl+ENTER), we can read the dataset on our Spyder IDE screen by clicking on the variable explorer option. The above output shows the dataset, which has two variables: Salary and Experience. After that, we need to extract the dependent and independent variables from the given dataset. The independent variable is years of experience, and the dependent variable is salary. Below is code for it: In the above lines of code, for x variable, we have taken -1 value since we want to remove the last column from the dataset. For y variable, we have taken 1 value as a parameter, since we want to extract the second column and indexing starts from the zero. By executing the above line of code, we will get the output for X and Y variable as: In the above output image, we can see the X (independent) variable and Y (dependent) variable has been extracted from the given dataset. Next, we will split both variables into the test set and training set. We have 30 observations, so we will take 20 observations for the training set and 10 observations for the test set. We are splitting our dataset so that we can train our model using a training dataset and then test the model using a test dataset. The code for this is given below: By executing the above code, we will get x-test, x-train and y-test, y-train dataset. Consider the below images: Test-dataset: Training Dataset: For simple linear Regression, we will not use Feature Scaling. Because Python libraries take care of it for some cases, so we don't need to perform it here. Now, our dataset is well prepared to work on it and we are going to start building a Simple Linear Regression model for the given problem. Step-2: Fitting the Simple Linear Regression to the Training Set: Now the second step is to fit our model to the training dataset. To do so, we will import theLinearRegressionclass of thelinear_modellibrary from thescikit learn. After importing the class, we are going to create an object of the class named as aregressor. The code for this is given below: In the above code, we have used afit()method to fit our Simple Linear Regression object to the training set. In the fit() function, we have passed the x_train and y_train, which is our training dataset for the dependent and an independent variable. We have fitted our regressor object to the training set so that the model can easily learn the correlations between the predictor and target variables. After executing the above lines of code, we will get the below output. Output: Step: 3. Prediction of test set result: dependent (salary) and an independent variable (Experience). So, now, our model is ready to predict the output for the new observations. In this step, we will provide the test dataset (new observations) to the model to check whether it can predict the correct output or not. We will create a prediction vectory_pred, andx_pred, which will contain predictions of test dataset, and prediction of training set respectively. On executing the above lines of code, two variables named y_pred and x_pred will generate in the variable explorer options that contain salary predictions for the training set and test set. Output: You can check the variable by clicking on the variable explorer option in the IDE, and also compare the result by comparing values from y_pred and y_test. By comparing these values, we can check how good our model is performing. Step: 4. visualizing the Training set results: Now in this step, we will visualize the training set result. To do so, we will use the scatter() function of the pyplot library, which we have already imported in the pre-processing step. Thescatter () functionwill create a scatter plot of observations. In the x-axis, we will plot the Years of Experience of employees and on the y-axis, salary of employees. In the function, we will pass the real values of training set, which means a year of experience x_train, training set of Salaries y_train, and color of the observations. Here we are taking a green color for the observation, but it can be any color as per the choice. Now, we need to plot the regression line, so for this, we will use theplot() functionof the pyplot library. In this function, we will pass the years of experience for training set, predicted salary for training set x_pred, and color of the line. Next, we will give the title for the plot. So here, we will use thetitle()function of thepyplotlibrary and pass the name (\"Salary vs Experience (Training Dataset)\". After that, we will assign labels for x-axis and y-axis usingxlabel() and ylabel() function. Finally, we will represent all above things in a graph using show(). The code is given below: Output: By executing the above lines of code, we will get the below graph plot as an output. In the above plot, we can see the real values observations in green dots and predicted values are covered by the red regression line. The regression line shows a correlation between the dependent and independent variable. The good fit of the line can be observed by calculating the difference between actual values and predicted values. But as we can see in the aboveplot, most of the observations are close to the regression line, hence our model is good for the training set. Step: 5. visualizing the Test set results: In the previous step, we have visualized the performance of our model on the training set. Now, we will do the same for the Test set. The complete code will remain the same as the above code, except in this, we will use x_test, and y_test instead of x_train and y_train. Here we are also changing the color of observations and regression line to differentiate between the two plots, but it is optional. Output: By executing the above line of code, we will get the output as: In the above plot, there are observations given by the blue color, and prediction is given by the red regression line. As we can see, most of the observations are close to the regression line, hence we can say our Simple Linear Regression is a good model and able to make good predictions.",
        "Latest Courses": ""
    },
    "multiple-linear-regression-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Multiple Linear Regression": "In the previous topic, we have learned about Simple Linear Regression, where a single Independent/Predictor(X) variable is used to model the response variable (Y). But there may be various cases in which the response variable is affected by more than one predictor variable; for such cases, the Multiple Linear Regression algorithm is used. Moreover, Multiple Linear Regression is an extension of Simple Linear regression as it takes more than one predictor variable to predict the response variable. We can define it as: Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable. Example: Prediction of CO2emission based on engine size and number of cylinders in a car. Some key points about MLR: For MLR, the dependent or target variable(Y) must be the continuous/real, but the predictor or independent variable may be of continuous or categorical form.Each feature variable must model the linear relationship with the dependent variable.MLR tries to fit a regression line through a multidimensional space of data-points.",
        "MLR equation:": "In Multiple Linear Regression, the target variable(Y) is a linear combination of multiple predictor variables x1, x2, x3, ...,xn. Since it is an enhancement of Simple Linear Regression, so the same is applied for the multiple linear regression equation, the equation becomes: Where, Y= Output/Response variable b0, b1, b2, b3, bn....= Coefficients of the model. x1, x2, x3, x4,...= Various Independent/feature variable",
        "Assumptions for Multiple Linear Regression:": "Alinear relationshipshould exist between the Target and predictor variables.The regression residuals must benormally distributed.MLR assumes little orno multicollinearity(correlation between the independent variable) in data.",
        "Implementation of Multiple Linear Regression model using Python:": "To implement MLR using Python, we have below problem: Problem Description: We have a dataset of50 start-up companies. This dataset contains five main information:R&D Spend, Administration Spend, Marketing Spend, State, and Profit for a financial year. Our goal is to create a model that can easily determine which company has a maximum profit, and which is the most affecting factor for the profit of a company. Since we need to find the Profit, so it is the dependent variable, and the other four variables are independent variables. Below are the main steps of deploying the MLR model: Data Pre-processing StepsFitting the MLR model to the training setPredicting the result of the test set Step-1: Data Pre-processing Step: The very first step isdata pre-processing, which we have already discussed in this tutorial. This process contains the below steps: Importing libraries:Firstly we will import the library which will help in building the model. Below is the code for it: Importing dataset:Now we will import the dataset(50_CompList), which contains all the variables. Below is the code for it: Output:We will get the dataset as: In above output, we can clearly see that there are five variables, in which four variables are continuous and one is categorical variable. Extracting dependent and independent Variables: Output: Out[5]: As we can see in the above output, the last column contains categorical variables which are not suitable to apply directly for fitting the model. So we need to encode this variable. Encoding Dummy Variables: As we have one categorical variable (State), which cannot be directly applied to the model, so we will encode it. To encode the categorical variable into numbers, we will use theLabelEncoderclass. But it is not sufficient because it still has some relational order, which may create a wrong model. So in order to remove this problem, we will useOneHotEncoder, which will create the dummy variables. Below is code for it: Here we are only encoding one independent variable, which is state as other variables are continuous. Output: As we can see in the above output, the state column has been converted into dummy variables (0 and 1).Here each dummy variable column is corresponding to the one State. We can check by comparing it with the original dataset. The first column corresponds to theCalifornia State, the second column corresponds to theFlorida State, and the third column corresponds to theNew York State. Now, we are writing a single line of code just to avoid the dummy variable trap: If we do not remove the first dummy variable, then it may introduce multicollinearity in the model. As we can see in the above output image, the first column has been removed. Now we will split the dataset into training and test set. The code for this is given below: The above code will split our dataset into a training set and test set. Output:The above code will split the dataset into training set and test set. You can check the output by clicking on the variable explorer option given in Spyder IDE. The test set and training set will look like the below image: Test set: Training set:",
        "Step: 2- Fitting our MLR model to the Training set:": "Now, we have well prepared our dataset in order to provide training, which means we will fit our regression model to the training set. It will be similar to as we did inSimple Linear Regressionmodel. The code for this will be: Output: Now, we have successfully trained our model using the training dataset. In the next step, we will test the performance of the model using the test dataset.",
        "Step: 3- Prediction of Test set results:": "The last step for our model is checking the performance of the model. We will do it by predicting the test set result. For prediction, we will create ay_predvector. Below is the code for it: By executing the above lines of code, a new vector will be generated under the variable explorer option. We can test our model by comparing the predicted values and test set values. Output: In the above output, we have predicted result set and test set. We can check model performance by comparing these two value index by index. For example, the first index has a predicted value of103015$profit and test/real value of103282$profit. The difference is only of267$, which is a good prediction, so, finally, our model is completed here. We can also check the score for training dataset and test dataset. Below is the code for it: Output:The score is: The above score tells that our model is 95% accurate with the training dataset and 93% accurate with the test dataset.",
        "Applications of Multiple Linear Regression:": "There are mainly two applications of Multiple Linear Regression: Effectiveness of Independent variable on prediction:Predicting the impact of changes:",
        "Latest Courses": ""
    },
    "backward-elimination-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "What is Backward Elimination?": "Backward elimination is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output. There are various ways to build a model in Machine Learning, which are: All-inBackward EliminationForward SelectionBidirectional EliminationScore Comparison Above are the possible methods for building the model in Machine learning, but we will only use here the Backward Elimination process as it is the fastest method.",
        "Steps of Backward Elimination": "Below are some main steps which are used to apply backward elimination process: Step-1:Firstly, We need to select a significance level to stay in the model. (SL=0.05) Step-2:Fit the complete model with all possible predictors/independent variables. Step-3:Choose the predictor which has the highest P-value, such that. If P-value >SL, go to step 4.Else Finish, and Our model is ready. Step-4:Remove that predictor. Step-5:Rebuild and fit the model with the remaining variables.",
        "Need for Backward Elimination: An optimal Multiple Linear Regression model:": "In the previous chapter, we discussed and successfully created our Multiple Linear Regression model, where we took4 independent variables (R&D spend, Administration spend, Marketing spend, and state (dummy variables)) and one dependent variable (Profit). But that model is not optimal, as we have included all the independent variables and do not know which independent model is most affecting and which one is the least affecting for the prediction. Unnecessary features increase the complexity of the model. Hence it is good to have only the most significant features and keep our model simple to get the better result. So, in order to optimize the performance of the model, we will use the Backward Elimination method. This process is used to optimize the performance of the MLR model as it will only include the most affecting feature and remove the least affecting feature. Let's start to apply it to our MLR model.",
        "Steps for Backward Elimination method:": "We will use the same model which we build in the previous chapter of MLR. Below is the complete code for it: From the above code, we got training and test set result as: The difference between both scores is 0.0154. Step: 1- Preparation of Backward Elimination: Importing the library:Firstly, we need to import thestatsmodels.formula.apilibrary, which is used for the estimation of various statistical models such as OLS(Ordinary Least Square). Below is the code for it: Adding a column in matrix of features:As we can check in our MLR equation (a), there is one constant term b0, but this term is not present in our matrix of features, so we need to add it manually. We will add a column having values x0= 1 associated with the constant term b0.To add this, we will useappendfunction ofNumpylibrary (nm which we have already imported into our code), and will assign a value of 1. Below is the code for it. Here we have used axis =1, as we wanted to add a column. For adding a row, we can use axis =0. Output:By executing the above line of code, a new column will be added into our matrix of features, which will have all values equal to 1. We can check it by clicking on the x dataset under the variable explorer option. As we can see in the above output image, the first column is added successfully, which corresponds to the constant term of the MLR equation. Step: 2: Now, we are actually going to apply a backward elimination process. Firstly we will create a new feature vectorx_opt, which will only contain a set of independent features that are significantly affecting the dependent variable.Next, as per the Backward Elimination process, we need to choose a significant level(0.5), and then need to fit the model with all possible predictors. So for fitting the model, we will create aregressor_OLSobject of new classOLSofstatsmodelslibrary. Then we will fit it by using thefit()method.Next we needp-valueto compare with SL value, so for this we will usesummary()method to get the summary table of all the values. Below is the code for it: Output:By executing the above lines of code, we will get a summary table. Consider the below image: In the above image, we can clearly see the p-values of all the variables. Herex1, x2 are dummy variables, x3 is R&D spend, x4 is Administration spend, and x5 is Marketing spend. From the table, we will choose the highest p-value, which is for x1=0.953 Now, we have the highest p-value which is greater than the SL value, so will remove the x1 variable (dummy variable) from the table and will refit the model. Below is the code for it: Output: As we can see in the output image, now five variables remain. In these variables, the highest p-value is 0.961. So we will remove it in the next iteration. Now the next highest value is 0.961 for x1 variable, which is another dummy variable. So we will remove it and refit the model. Below is the code for it: Output: In the above output image, we can see the dummy variable(x2) has been removed. And the next highest value is .602, which is still greater than .5, so we need to remove it. Now we will remove the Admin spend which is having .602 p-value and again refit the model. Output: As we can see in the above output image, the variable (Admin spend) has been removed. But still, there is one variable left, which ismarketing spendas it has a high p-value(0.60). So we need to remove it. Finally, we will remove one more variable, which has .60 p-value for marketing spend, which is more than a significant level.Below is the code for it: Output: As we can see in the above output image, only two variables are left. So only theR&D independent variableis a significant variable for the prediction. So we can now predict efficiently using this variable.",
        "Estimating the performance:": "In the previous topic, we have calculated the train and test score of the model when we have used all the features variables. Now we will check the score with only one feature variable (R&D spend). Our dataset now looks like: Below is the code for Building Multiple Linear Regression model by only using R&D spend: Output: After executing the above code, we will get the Training and test scores as: As we can see, the training score is 94% accurate, and the test score is also 94% accurate. The difference between both scores is.00149. This score is very much close to the previous score, i.e.,0.0154, where we have included all the variables. We got this result by using one independent variable (R&D spend) only instead of four variables. Hence, now, our model is simple and accurate.",
        "Latest Courses": ""
    },
    "machine-learning-polynomial-regression": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "ML Polynomial Regression": "Polynomial Regression is a regression algorithm that models the relationship between a dependent(y) and independent variable(x) as nth degree polynomial. The Polynomial Regression equation is given below: It is also called the special case of Multiple Linear Regression in ML. Because we add some polynomial terms to the Multiple Linear regression equation to convert it into Polynomial Regression.It is a linear model with some modification in order to increase the accuracy.The dataset used in Polynomial regression for training is of non-linear nature.It makes use of a linear regression model to fit the complicated and non-linear functions and datasets.Hence,\"In Polynomial regression, the original features are converted into Polynomial features of required degree (2,3,..,n) and then modeled using a linear model.\"",
        "Need for Polynomial Regression:": "The need of Polynomial Regression in ML can be understood in the below points: If we apply a linear model on alinear dataset, then it provides us a good result as we have seen in Simple Linear Regression, but if we apply the same model without any modification on anon-linear dataset, then it will produce a drastic output. Due to which loss function will increase, the error rate will be high, and accuracy will be decreased.So for such cases,where data points are arranged in a non-linear fashion, we need the Polynomial Regression model. We can understand it in a better way using the below comparison diagram of the linear dataset and non-linear dataset. In the above image, we have taken a dataset which is arranged non-linearly. So if we try to cover it with a linear model, then we can clearly see that it hardly covers any data point. On the other hand, a curve is suitable to cover most of the data points, which is of the Polynomial model.Hence,if the datasets are arranged in a non-linear fashion, then we should use the Polynomial Regression model instead of Simple Linear Regression.",
        "Equation of the Polynomial Regression Model:": "Simple Linear Regression equation:         y = b0+b1x         .........(a) Multiple Linear Regression equation:         y= b0+b1x+ b2x2+ b3x3+....+ bnxn.........(b) Polynomial Regression equation:         y= b0+b1x + b2x2+ b3x3+....+ bnxn..........(c) When we compare the above three equations, we can clearly see that all three equations are Polynomial equations but differ by the degree of variables. The Simple and Multiple Linear equations are also Polynomial equations with a single degree, and the Polynomial regression equation is Linear equation with the nth degree. So if we add a degree to our linear equations, then it will be converted into Polynomial Linear equations.",
        "Implementation of Polynomial Regression using Python:": "Here we will implement the Polynomial Regression using Python. We will understand it by comparing Polynomial Regression model with the Simple Linear Regression model. So first, let's understand the problem for which we are going to build the model. Problem Description:There is a Human Resource company, which is going to hire a new candidate. The candidate has told his previous salary 160K per annum, and the HR have to check whether he is telling the truth or bluff. So to identify this, they only have a dataset of his previous company in which the salaries of the top 10 positions are mentioned with their levels. By checking the dataset available, we have found that there is anon-linear relationship between the Position levels and the salaries. Our goal is to build aBluffing detector regressionmodel, so HR can hire an honest candidate. Below are the steps to build such a model.",
        "Steps for Polynomial Regression:": "The main steps involved in Polynomial Regression are given below: Data Pre-processingBuild a Linear Regression model and fit it to the datasetBuild a Polynomial Regression model and fit it to the datasetVisualize the result for Linear Regression and Polynomial Regression model.Predicting the output. Data Pre-processing Step: The data pre-processing step will remain the same as in previous regression models, except for some changes. In the Polynomial Regression model, we will not use feature scaling, and also we will not split our dataset into training and test set. It has two reasons: The dataset contains very less information which is not suitable to divide it into a test and training set, else our model will not be able to find the correlations between the salaries and levels.In this model, we want very accurate predictions for salary, so the model should have enough information. The code for pre-processing step is given below: Explanation: In the above lines of code, we have imported the important Python libraries to import dataset and operate on it.Next, we have imported the dataset 'Position_Salaries.csv', which contains three columns (Position, Levels, and Salary), but we will consider only two columns (Salary and Levels).After that, we have extracted the dependent(Y) and independent variable(X) from the dataset. For x-variable, we have taken parameters as [:,1:2], because we want 1 index(levels), and included :2 to make it as a matrix. Output: By executing the above code, we can read our dataset as: As we can see in the above output, there are three columns present (Positions, Levels, and Salaries). But we are only considering two columns because Positions are equivalent to the levels or may be seen as the encoded form of Positions. Here we will predict the output for level6.5because the candidate has 4+ years' experience as a regional manager, so he must be somewhere between levels 7 and 6. Building the Linear regression model: Now, we will build and fit the Linear regression model to the dataset. In building polynomial regression, we will take the Linear regression model as reference and compare both the results. The code is given below: In the above code, we have created the Simple Linear model usinglin_regsobject ofLinearRegressionclass and fitted it to the dataset variables (x and y). Output: Building the Polynomial regression model: Now we will build the Polynomial Regression model, but it will be a little different from the Simple Linear model. Because here we will usePolynomialFeaturesclass ofpreprocessinglibrary. We are using this class to add some extra features to our dataset. In the above lines of code, we have usedpoly_regs.fit_transform(x), because first we are converting our feature matrix into polynomial feature matrix, and then fitting it to the Polynomial regression model. The parameter value(degree= 2) depends on our choice. We can choose it according to our Polynomial features. After executing the code, we will get another matrixx_poly, which can be seen under the variable explorer option: Next, we have used another LinearRegression object, namelylin_reg_2, to fit ourx_polyvector to the linear model. Output: Visualizing the result for Linear regression: Now we will visualize the result for Linear regression model as we did in Simple Linear Regression. Below is the code for it: Output: In the above output image, we can clearly see that the regression line is so far from the datasets. Predictions are in a red straight line, and blue points are actual values. If we consider this output to predict the value of CEO, it will give a salary of approx. 600000$, which is far away from the real value. So we need a curved model to fit the dataset other than a straight line. Visualizing the result for Polynomial Regression Here we will visualize the result of Polynomial regression model, code for which is little different from the above model. Code for this is given below: In the above code, we have taken lin_reg_2.predict(poly_regs.fit_transform(x), instead of x_poly, because we want a Linear regressor object to predict the polynomial features matrix. Output: As we can see in the above output image, the predictions are close to the real values. The above plot will vary as we will change the degree. For degree= 3: If we change the degree=3, then we will give a more accurate plot, as shown in the below image. SO as we can see here in the above output image, the predicted salary for level 6.5 is near to 170K$-190k$, which seems that future employee is saying the truth about his salary. Degree= 4:Let's again change the degree to 4, and now will get the most accurate plot. Hence we can get more accurate results by increasing the degree of Polynomial. Predicting the final result with the Linear Regression model: Now, we will predict the final output using the Linear regression model to see whether an employee is saying truth or bluff. So, for this, we will use thepredict()method and will pass the value 6.5. Below is the code for it: Output: Predicting the final result with the Polynomial Regression model: Now, we will predict the final output using the Polynomial Regression model to compare with Linear model. Below is the code for it: Output: As we can see, the predicted output for the Polynomial Regression is [158862.45265153], which is much closer to real value hence, we can say that future employee is saying true.",
        "Latest Courses": ""
    },
    "classification-algorithm-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Classification Algorithm in Machine Learning": "As we know, the Supervised Machine Learning algorithm can be broadly classified into Regression and Classification Algorithms. In Regression algorithms, we have predicted the output for continuous values, but to predict the categorical values, we need Classification algorithms.",
        "What is the Classification Algorithm?": "The Classification algorithm is a Supervised Learning technique that is used to identify the category of new observations on the basis of training data. In Classification, a program learns from the given dataset or observations and then classifies new observation into a number of classes or groups. Such as,Yes or No, 0 or 1, Spam or Not Spam, cat or dog,etc. Classes can be called as targets/labels or categories. Unlike regression, the output variable of Classification is a category, not a value, such as \"Green or Blue\", \"fruit or animal\", etc. Since the Classification algorithm is a Supervised learning technique, hence it takes labeled input data, which means it contains input with the corresponding output. In classification algorithm, a discrete output function(y) is mapped to input variable(x). The best example of an ML classification algorithm isEmail Spam Detector. The main goal of the Classification algorithm is to identify the category of a given dataset, and these algorithms are mainly used to predict the output for the categorical data. Classification algorithms can be better understood using the below diagram. In the below diagram, there are two classes, class A and Class B. These classes have features that are similar to each other and dissimilar to other classes. The algorithm which implements the classification on a dataset is known as a classifier. There are two types of Classifications: Binary Classifier:If the classification problem has only two possible outcomes, then it is called as Binary Classifier.Examples:YES or NO, MALE or FEMALE, SPAM or NOT SPAM, CAT or DOG, etc.Multi-class Classifier:If a classification problem has more than two outcomes, then it is called as Multi-class Classifier.Example:Classifications of types of crops, Classification of types of music.",
        "Learners in Classification Problems:": "In the classification problems, there are two types of learners: Lazy Learners:Lazy Learner firstly stores the training dataset and wait until it receives the test dataset. In Lazy learner case, classification is done on the basis of the most related data stored in the training dataset. It takes less time in training but more time for predictions.Example:K-NN algorithm, Case-based reasoningEager Learners:Eager Learners develop a classification model based on a training dataset before receiving a test dataset. Opposite to Lazy learners, Eager Learner takes more time in learning, and less time in prediction.Example:Decision Trees, Na�ve Bayes, ANN.",
        "Types of ML Classification Algorithms:": "Classification Algorithms can be further divided into the Mainly two category: Linear ModelsLogistic RegressionSupport Vector MachinesNon-linear ModelsK-Nearest NeighboursKernel SVMNa�ve BayesDecision Tree ClassificationRandom Forest Classification",
        "Evaluating a Classification model:": "Once our model is completed, it is necessary to evaluate its performance; either it is a Classification or Regression model. So for evaluating a Classification model, we have the following ways: 1. Log Loss or Cross-Entropy Loss: It is used for evaluating the performance of a classifier, whose output is a probability value between the 0 and 1.For a good binary Classification model, the value of log loss should be near to 0.The value of log loss increases if the predicted value deviates from the actual value.The lower log loss represents the higher accuracy of the model.For Binary classification, cross-entropy can be calculated as: Where y= Actual output, p= predicted output. 2. Confusion Matrix: The confusion matrix provides us a matrix/table as output and describes the performance of the model.It is also known as the error matrix.The matrix consists of predictions result in a summarized form, which has a total number of correct predictions and incorrect predictions. The matrix looks like as below table: 3. AUC-ROC curve: ROC curve stands forReceiver Operating Characteristics Curveand AUC stands forArea Under the Curve.It is a graph that shows the performance of the classification model at different thresholds.To visualize the performance of the multi-class classification model, we use the AUC-ROC Curve.The ROC curve is plotted with TPR and FPR, where TPR (True Positive Rate) on Y-axis and FPR(False Positive Rate) on X-axis.",
        "Use cases of Classification Algorithms": "Classification algorithms can be used in different places. Below are some popular use cases of Classification Algorithms: Email Spam DetectionSpeech RecognitionIdentifications of Cancer tumor cells.Drugs ClassificationBiometric Identification, etc.",
        "Latest Courses": ""
    },
    "logistic-regression-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Logistic Regression in Machine Learning": "Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1,it gives the probabilistic values which lie between 0 and 1.Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereasLogistic regression is used for solving the classification problems.In Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1).The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:",
        "Logistic Function (Sigmoid Function):": "The sigmoid function is a mathematical function used to map the predicted values to probabilities.It maps any real value into another value within a range of 0 and 1.The value of the logistic regression must be between 0 and 1, which cannot go beyond this limit, so it forms a curve like the \"S\" form. The S-form curve is called the Sigmoid function or the logistic function.In logistic regression, we use the concept of the threshold value, which defines the probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value below the threshold values tends to 0.",
        "Assumptions for Logistic Regression:": "The dependent variable must be categorical in nature.The independent variable should not have multi-collinearity.",
        "Logistic Regression Equation:": "The Logistic regression equation can be obtained from the Linear Regression equation. The mathematical steps to get Logistic Regression equations are given below: We know the equation of the straight line can be written as: In Logistic Regression y can be between 0 and 1 only, so for this let's divide the above equation by (1-y): But we need range between -[infinity] to +[infinity], then take logarithm of the equation it will become: The above equation is the final equation for Logistic Regression.",
        "Type of Logistic Regression:": "On the basis of the categories, Logistic Regression can be classified into three types: Binomial:In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.Multinomial:In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as \"cat\", \"dogs\", or \"sheep\"Ordinal:In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as \"low\", \"Medium\", or \"High\".",
        "Python Implementation of Logistic Regression (Binomial)": "To understand the implementation of Logistic Regression in Python, we will use the below example: Example:There is a dataset given which contains the information of various users obtained from the social networking sites. There is a car making company that has recently launched a new SUV car. So the company wanted to check how many users from the dataset, wants to purchase the car. For this problem, we will build a Machine Learning model using the Logistic regression algorithm. The dataset is shown in the below image. In this problem, we will predict thepurchased variable (Dependent Variable)by usingage and salary (Independent variables). Steps in Logistic Regression:To implement the Logistic Regression using Python, we will use the same steps as we have done in previous topics of Regression. Below are the steps: Data Pre-processing stepFitting Logistic Regression to the Training setPredicting the test resultTest accuracy of the result(Creation of Confusion matrix)Visualizing the test set result. 1. Data Pre-processing step:In this step, we will pre-process/prepare the data so that we can use it in our code efficiently. It will be the same as we have done in Data pre-processing topic. The code for this is given below: By executing the above lines of code, we will get the dataset as the output. Consider the given image: Now, we will extract the dependent and independent variables from the given dataset. Below is the code for it: In the above code, we have taken [2, 3] for x because our independent variables are age and salary, which are at index 2, 3. And we have taken 4 for y variable because our dependent variable is at index 4. The output will be: Now we will split the dataset into a training set and test set. Below is the code for it: The output for this is given below: For test set: For training set: In logistic regression, we will do feature scaling because we want accurate result of predictions. Here we will only scale the independent variable because dependent variable have only 0 and 1 values. Below is the code for it: The scaled output is given below: 2. Fitting Logistic Regression to the Training set: We have well prepared our dataset, and now we will train the dataset using the training set. For providing training or fitting the model to the training set, we will import theLogisticRegressionclass of thesklearnlibrary. After importing the class, we will create a classifier object and use it to fit the model to the logistic regression. Below is the code for it: Output:By executing the above code, we will get the below output: Out[5]: Hence our model is well fitted to the training set. 3. Predicting the Test Result Our model is well trained on the training set, so we will now predict the result by using test set data. Below is the code for it: In the above code, we have created a y_pred vector to predict the test set result. Output:By executing the above code, a new vector (y_pred) will be created under the variable explorer option. It can be seen as: The above output image shows the corresponding predicted users who want to purchase or not purchase the car. 4. Test Accuracy of the result Now we will create the confusion matrix here to check the accuracy of the classification. To create it, we need to import theconfusion_matrixfunction of the sklearn library. After importing the function, we will call it using a new variablecm. The function takes two parameters, mainlyy_true( the actual values) andy_pred(the targeted value return by the classifier). Below is the code for it: Output: By executing the above code, a new confusion matrix will be created. Consider the below image: We can find the accuracy of the predicted result by interpreting the confusion matrix. By above output, we can interpret that 65+24= 89 (Correct Output) and 8+3= 11(Incorrect Output). 5. Visualizing the training set result Finally, we will visualize the training set result. To visualize the result, we will useListedColormapclass of matplotlib library. Below is the code for it: In the above code, we have imported theListedColormapclass of Matplotlib library to create the colormap for visualizing the result. We have created two new variablesx_setandy_setto replacex_trainandy_train. After that, we have used thenm.meshgridcommand to create a rectangular grid, which has a range of -1(minimum) to 1 (maximum). The pixel points we have taken are of 0.01 resolution. To create a filled contour, we have usedmtp.contourfcommand, it will create regions of provided colors (purple and green). In this function, we have passed theclassifier.predictto show the predicted data points predicted by the classifier. Output:By executing the above code, we will get the below output: The graph can be explained in the below points: In the above graph, we can see that there are someGreen pointswithin the green region andPurple pointswithin the purple region.All these data points are the observation points from the training set, which shows the result for purchased variables.This graph is made by using two independent variables i.e.,Age on the x-axisandEstimated salary on the y-axis.Thepurple point observationsare for which purchased (dependent variable) is probably 0, i.e., users who did not purchase the SUV car.Thegreen point observationsare for which purchased (dependent variable) is probably 1 means user who purchased the SUV car.We can also estimate from the graph that the users who are younger with low salary, did not purchase the car, whereas older users with high estimated salary purchased the car.But there are some purple points in the green region (Buying the car) and some green points in the purple region(Not buying the car). So we can say that younger users with a high estimated salary purchased the car, whereas an older user with a low estimated salary did not purchase the car. The goal of the classifier: We have successfully visualized the training set result for the logistic regression, and our goal for this classification is to divide the users who purchased the SUV car and who did not purchase the car. So from the output graph, we can clearly see the two regions (Purple and Green) with the observation points. The Purple region is for those users who didn't buy the car, and Green Region is for those users who purchased the car. Linear Classifier: As we can see from the graph, the classifier is a Straight line or linear in nature as we have used the Linear model for Logistic Regression. In further topics, we will learn for non-linear Classifiers. Visualizing the test set result: Our model is well trained using the training dataset. Now, we will visualize the result for new observations (Test set). The code for the test set will remain same as above except that here we will usex_test and y_testinstead ofx_train and y_train. Below is the code for it: Output: The above graph shows the test set result. As we can see, the graph is divided into two regions (Purple and Green). And Green observations are in the green region, and Purple observations are in the purple region. So we can say it is a good prediction and model. Some of the green and purple data points are in different regions, which can be ignored as we have already calculated this error using the confusion matrix (11 Incorrect output). Hence our model is pretty good and ready to make new predictions for this classification problem.",
        "Latest Courses": ""
    },
    "k-nearest-neighbor-algorithm-for-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "K-Nearest Neighbor(KNN) Algorithm for Machine Learning": "K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.K-NN is anon-parametric algorithm, which means it does not make any assumption on underlying data.It is also called alazy learner algorithmbecause it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.Example:Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category.",
        "Why do we need a K-NN Algorithm?": "Suppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset. Consider the below diagram:",
        "How does K-NN work?": "The K-NN working can be explained on the basis of the below algorithm: Step-1:Select the number K of the neighborsStep-2:Calculate the Euclidean distance ofK number of neighborsStep-3:Take the K nearest neighbors as per the calculated Euclidean distance.Step-4:Among these k neighbors, count the number of the data points in each category.Step-5:Assign the new data points to that category for which the number of the neighbor is maximum.Step-6:Our model is ready. Suppose we have a new data point and we need to put it in the required category. Consider the below image: Firstly, we will choose the number of neighbors, so we will choose the k=5.Next, we will calculate theEuclidean distancebetween the data points. The Euclidean distance is the distance between two points, which we have already studied in geometry. It can be calculated as: By calculating the Euclidean distance we got the nearest neighbors, as three nearest neighbors in category A and two nearest neighbors in category B. Consider the below image: As we can see the 3 nearest neighbors are from category A, hence this new data point must belong to category A.",
        "How to select the value of K in the K-NN Algorithm?": "Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for \"K\", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties.",
        "Advantages of KNN Algorithm:": "It is simple to implement.It is robust to the noisy training dataIt can be more effective if the training data is large.",
        "Disadvantages of KNN Algorithm:": "Always needs to determine the value of K which may be complex some time.The computation cost is high because of calculating the distance between the data points for all the training samples.",
        "Python implementation of the KNN algorithm": "To do the Python implementation of the K-NN algorithm, we will use the same problem and dataset which we have used in Logistic Regression. But here we will improve the performance of the model. Below is the problem description: Problem for K-NN Algorithm:There is a Car manufacturer company that has manufactured a new SUV car. The company wants to give the ads to the users who are interested in buying that SUV. So for this problem, we have a dataset that contains multiple user's information through the social network. The dataset contains lots of information but theEstimated SalaryandAgewe will consider for the independent variable and thePurchased variableis for the dependent variable. Below is the dataset: Steps to implement the K-NN algorithm: Data Pre-processing stepFitting the K-NN algorithm to the Training setPredicting the test resultTest accuracy of the result(Creation of Confusion matrix)Visualizing the test set result. Data Pre-Processing Step: The Data Pre-processing step will remain exactly the same as Logistic Regression. Below is the code for it: By executing the above code, our dataset is imported to our program and well pre-processed. After feature scaling our test dataset will look like: From the above output image, we can see that our data is successfully scaled. Fitting K-NN classifier to the Training data:Now we will fit the K-NN classifier to the training data. To do this we will import theKNeighborsClassifierclass ofSklearn Neighborslibrary. After importing the class, we will create theClassifierobject of the class. The Parameter of this class will ben_neighbors:To define the required neighbors of the algorithm. Usually, it takes 5.metric='minkowski':This is the default parameter and it decides the distance between the points.p=2:It is equivalent to the standard Euclidean metric.And then we will fit the classifier to the training data. Below is the code for it: Output: By executing the above code, we will get the output as: Predicting the Test Result:To predict the test set result, we will create ay_predvector as we did in Logistic Regression. Below is the code for it: Output: The output for the above code will be: Creating the Confusion Matrix:Now we will create the Confusion Matrix for our K-NN model to see the accuracy of the classifier. Below is the code for it: In above code, we have imported the confusion_matrix function and called it using the variable cm. Output:By executing the above code, we will get the matrix as below: In the above image, we can see there are 64+29= 93 correct predictions and 3+4= 7 incorrect predictions, whereas, in Logistic Regression, there were 11 incorrect predictions. So we can say that the performance of the model is improved by using the K-NN algorithm. Visualizing the Training set result:Now, we will visualize the training set result for K-NN model. The code will remain same as we did in Logistic Regression, except the name of the graph. Below is the code for it: Output: By executing the above code, we will get the below graph: The output graph is different from the graph which we have occurred in Logistic Regression. It can be understood in the below points: As we can see the graph is showing the red point and green points. The green points are for Purchased(1) and Red Points for not Purchased(0) variable.The graph is showing an irregular boundary instead of showing any straight line or any curve because it is a K-NN algorithm, i.e., finding the nearest neighbor.The graph has classified users in the correct categories as most of the users who didn't buy the SUV are in the red region and users who bought the SUV are in the green region.The graph is showing good result but still, there are some green points in the red region and red points in the green region. But this is no big issue as by doing this model is prevented from overfitting issues.Hence our model is well trained.Visualizing the Test set result:After the training of the model, we will now test the result by putting a new dataset, i.e., Test dataset. Code remains the same except some minor changes: such asx_train and y_trainwill be replaced byx_test and y_test.Below is the code for it: Output: The above graph is showing the output for the test data set. As we can see in the graph, the predicted output is well good as most of the red points are in the red region and most of the green points are in the green region. However, there are few green points in the red region and a few red points in the green region. So these are the incorrect observations that we have observed in the confusion matrix(7 Incorrect output).",
        "Latest Courses": ""
    },
    "machine-learning-support-vector-machine-algorithm": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Support Vector Machine Algorithm": "Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane: Example:SVM can be understood with the example that we have used in the KNN classifier. Suppose we see a strange cat that also has some features of dogs, so if we want a model that can accurately identify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will first train our model with lots of images of cats and dogs so that it can learn about different features of cats and dogs, and then we test it with this strange creature. So as support vector creates a decision boundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see the extreme case of cat and dog. On the basis of the support vectors, it will classify it as a cat. Consider the below diagram: SVM algorithm can be used forFace detection, image classification, text categorization,etc.",
        "Types of SVM": "SVM can be of two types: Linear SVM:Linear SVM is used for linearly separable data, which means if a dataset can be classified into two classes by using a single straight line, then such data is termed as linearly separable data, and classifier is used called as Linear SVM classifier.Non-linear SVM:Non-Linear SVM is used for non-linearly separated data, which means if a dataset cannot be classified by using a straight line, then such data is termed as non-linear data and classifier used is called as Non-linear SVM classifier.",
        "Hyperplane and Support Vectors in the SVM algorithm:": "Hyperplane:There can be multiple lines/decision boundaries to segregate the classes in n-dimensional space, but we need to find out the best decision boundary that helps to classify the data points. This best boundary is known as the hyperplane of SVM. The dimensions of the hyperplane depend on the features present in the dataset, which means if there are 2 features (as shown in image), then hyperplane will be a straight line. And if there are 3 features, then hyperplane will be a 2-dimension plane. We always create a hyperplane that has a maximum margin, which means the maximum distance between the data points. Support Vectors: The data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a Support vector.",
        "How does SVM works?": "Linear SVM: The working of the SVM algorithm can be understood by using an example. Suppose we have a dataset that has two tags (green and blue), and the dataset has two features x1 and x2. We want a classifier that can classify the pair(x1, x2) of coordinates in either green or blue. Consider the below image: So as it is 2-d space so by just using a straight line, we can easily separate these two classes. But there can be multiple lines that can separate these classes. Consider the below image: Hence, the SVM algorithm helps to find the best line or decision boundary; this best boundary or region is called as ahyperplane. SVM algorithm finds the closest point of the lines from both the classes. These points are called support vectors. The distance between the vectors and the hyperplane is called asmargin. And the goal of SVM is to maximize this margin. Thehyperplanewith maximum margin is called theoptimal hyperplane. Non-Linear SVM: If data is linearly arranged, then we can separate it by using a straight line, but for non-linear data, we cannot draw a single straight line. Consider the below image: So to separate these data points, we need to add one more dimension. For linear data, we have used two dimensions x and y, so for non-linear data, we will add a third dimension z. It can be calculated as: By adding the third dimension, the sample space will become as below image: So now, SVM will divide the datasets into classes in the following way. Consider the below image: Since we are in 3-d Space, hence it is looking like a plane parallel to the x-axis. If we convert it in 2d space with z=1, then it will become as: Hence we get a circumference of radius 1 in case of non-linear data. Python Implementation of Support Vector Machine Now we will implement the SVM algorithm using Python. Here we will use the same datasetuser_data, which we have used in Logistic regression and KNN classification. Data Pre-processing step Till the Data pre-processing step, the code will remain the same. Below is the code: After executing the above code, we will pre-process the data. The code will give the dataset as: The scaled output for the test set will be: Fitting the SVM classifier to the training set: Now the training set will be fitted to the SVM classifier. To create the SVM classifier, we will importSVCclass fromSklearn.svmlibrary. Below is the code for it: In the above code, we have usedkernel='linear', as here we are creating SVM for linearly separable data. However, we can change it for non-linear data. And then we fitted the classifier to the training dataset(x_train, y_train) Output: The model performance can be altered by changing the value ofC(Regularization factor), gamma, and kernel. Predicting the test set result:Now, we will predict the output for test set. For this, we will create a new vector y_pred. Below is the code for it: After getting the y_pred vector, we can compare the result ofy_predandy_testto check the difference between the actual value and predicted value. Output:Below is the output for the prediction of the test set: Creating the confusion matrix:Now we will see the performance of the SVM classifier that how many incorrect predictions are there as compared to the Logistic regression classifier. To create the confusion matrix, we need to import theconfusion_matrixfunction of the sklearn library. After importing the function, we will call it using a new variablecm. The function takes two parameters, mainlyy_true( the actual values) andy_pred(the targeted value return by the classifier). Below is the code for it: Output: As we can see in the above output image, there are 66+24= 90 correct predictions and 8+2= 10 correct predictions. Therefore we can say that our SVM model improved as compared to the Logistic regression model. Visualizing the training set result:Now we will visualize the training set result, below is the code for it: Output: By executing the above code, we will get the output as: As we can see, the above output is appearing similar to the Logistic regression output. In the output, we got the straight line as hyperplane because we haveused a linear kernel in the classifier. And we have also discussed above that for the 2d space, the hyperplane in SVM is a straight line. Visualizing the test set result: Output: By executing the above code, we will get the output as: As we can see in the above output image, the SVM classifier has divided the users into two regions (Purchased or Not purchased). Users who purchased the SUV are in the red region with the red scatter points. And users who did not purchase the SUV are in the green region with green scatter points. The hyperplane has divided the two classes into Purchased and not purchased variable.",
        "Latest Courses": ""
    },
    "machine-learning-naive-bayes-classifier": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Naïve Bayes Classifier Algorithm": "Naïve Bayes algorithm is a supervised learning algorithm, which is based onBayes theoremand used for solving classification problems.It is mainly used intext classificationthat includes a high-dimensional training dataset.Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.Some popular examples of Naïve Bayes Algorithm arespam filtration, Sentimental analysis, and classifying articles.",
        "Why is it called Naïve Bayes?": "The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as: Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.Bayes: It is called Bayes because it depends on the principle ofBayes' Theorem.",
        "Bayes' Theorem:": "Bayes' theorem is also known asBayes' RuleorBayes' law, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.The formula for Bayes' theorem is given as: Where, P(A|B) is Posterior probability: Probability of hypothesis A on the observed event B. P(B|A) is Likelihood probability: Probability of the evidence given that the probability of a hypothesis is true. P(A) is Prior Probability: Probability of hypothesis before observing the evidence. P(B) is Marginal Probability: Probability of Evidence.",
        "Working of Naïve Bayes' Classifier:": "Working of Naïve Bayes' Classifier can be understood with the help of the below example: Suppose we have a dataset ofweather conditionsand corresponding target variable \"Play\". So using this dataset we need to decide that whether we should play or not on a particular day according to the weather conditions. So to solve this problem, we need to follow the below steps: Convert the given dataset into frequency tables.Generate Likelihood table by finding the probabilities of given features.Now, use Bayes theorem to calculate the posterior probability. Problem: If the weather is sunny, then the Player should play or not? Solution: To solve this, first consider the below dataset: Frequency table for the Weather Conditions: Likelihood table weather condition: Applying Bayes'theorem: P(Yes|Sunny)= P(Sunny|Yes)*P(Yes)/P(Sunny) P(Sunny|Yes)= 3/10= 0.3 P(Sunny)= 0.35 P(Yes)=0.71 So P(Yes|Sunny) = 0.3*0.71/0.35=0.60 P(No|Sunny)= P(Sunny|No)*P(No)/P(Sunny) P(Sunny|NO)= 2/4=0.5 P(No)= 0.29 P(Sunny)= 0.35 So P(No|Sunny)= 0.5*0.29/0.35 =0.41 So as we can see from the above calculation thatP(Yes|Sunny)>P(No|Sunny) Hence on a Sunny day, Player can play the game.",
        "Advantages of Naïve Bayes Classifier:": "Naïve Bayes is one of the fast and easy ML algorithms to predict a class of datasets.It can be used for Binary as well as Multi-class Classifications.It performs well in Multi-class predictions as compared to the other Algorithms.It is the most popular choice fortext classification problems.",
        "Disadvantages of Naïve Bayes Classifier:": "Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.",
        "Applications of Naïve Bayes Classifier:": "It is used forCredit Scoring.It is used inmedical data classification.It can be used inreal-time predictionsbecause Naïve Bayes Classifier is an eager learner.It is used in Text classification such asSpam filteringandSentiment analysis.",
        "Types of Naïve Bayes Model:": "There are three types of Naive Bayes Model, which are given below: Gaussian: The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.Multinomial: The Multinomial Naïve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc.The classifier uses the frequency of words for the predictors.Bernoulli: The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks.",
        "Python Implementation of the Naïve Bayes algorithm:": "Now we will implement a Naive Bayes Algorithm using Python. So for this, we will use the \"user_data\"dataset, which we have used in our other classification model. Therefore we can easily compare the Naive Bayes model with the other models.",
        "Steps to implement:": "Data Pre-processing stepFitting Naive Bayes to the Training setPredicting the test resultTest accuracy of the result(Creation of Confusion matrix)Visualizing the test set result.",
        "1) Data Pre-processing step:": "In this step, we will pre-process/prepare the data so that we can use it efficiently in our code. It is similar as we did indata-pre-processing. The code for this is given below: In the above code, we have loaded the dataset into our program using \"dataset = pd.read_csv('user_data.csv'). The loaded dataset is divided into training and test set, and then we have scaled the feature variable. The output for the dataset is given as:",
        "2) Fitting Naive Bayes to the Training Set:": "After the pre-processing step, now we will fit the Naive Bayes model to the Training set. Below is the code for it: In the above code, we have used theGaussianNB classifierto fit it to the training dataset. We can also use other classifiers as per our requirement. Output:",
        "3) Prediction of the test set result:": "Now we will predict the test set result. For this, we will create a new predictor variabley_pred, and will use the predict function to make the predictions. Output: The above output shows the result for prediction vectory_predand real vector y_test. We can see that some predications are different from the real values, which are the incorrect predictions.",
        "4) Creating Confusion Matrix:": "Now we will check the accuracy of the Naive Bayes classifier using the Confusion matrix. Below is the code for it: Output: As we can see in the above confusion matrix output, there are 7+3= 10 incorrect predictions, and 65+25=90 correct predictions.",
        "5) Visualizing the training set result:": "Next we will visualize the training set result using Na�ve Bayes Classifier. Below is the code for it: Output: In the above output we can see that the Na�ve Bayes classifier has segregated the data points with the fine boundary. It is Gaussian curve as we have usedGaussianNBclassifier in our code.",
        "6) Visualizing the Test set result:": "Output: The above output is final output for test set data. As we can see the classifier has created a Gaussian curve to divide the \"purchased\" and \"not purchased\" variables. There are some wrong predictions which we have calculated in Confusion matrix. But still it is pretty good classifier.",
        "Latest Courses": ""
    },
    "regression-vs-classification-in-machine-learning": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Regression vs. Classification in Machine Learning": "Regression and Classification algorithms are Supervised Learning algorithms. Both the algorithms are used for prediction in Machine learning and work with the labeled datasets. But the difference between both is how they are used for different machine learning problems. The main difference between Regression and Classification algorithms that Regression algorithms are used topredict the continuousvalues such as price, salary, age, etc. and Classification algorithms are used topredict/Classify the discrete valuessuch as Male or Female, True or False, Spam or Not Spam, etc. Consider the below diagram:",
        "Classification:": "Classification is a process of finding a function which helps in dividing the dataset into classes based on different parameters. In Classification, a computer program is trained on the training dataset and based on that training, it categorizes the data into different classes. The task of the classification algorithm is to find the mapping function to map the input(x) to the discrete output(y). Example:The best example to understand the Classification problem is Email Spam Detection. The model is trained on the basis of millions of emails on different parameters, and whenever it receives a new email, it identifies whether the email is spam or not. If the email is spam, then it is moved to the Spam folder. Types of ML Classification Algorithms: Classification Algorithms can be further divided into the following types: Logistic RegressionK-Nearest NeighboursSupport Vector MachinesKernel SVMNa�ve BayesDecision Tree ClassificationRandom Forest Classification",
        "Regression:": "Regression is a process of finding the correlations between dependent and independent variables. It helps in predicting the continuous variables such as prediction ofMarket Trends, prediction of House prices, etc. The task of the Regression algorithm is to find the mapping function to map the input variable(x) to the continuous output variable(y). Example:Suppose we want to do weather forecasting, so for this, we will use the Regression algorithm. In weather prediction, the model is trained on the past data, and once the training is completed, it can easily predict the weather for future days. Types of Regression Algorithm: Simple Linear RegressionMultiple Linear RegressionPolynomial RegressionSupport Vector RegressionDecision Tree RegressionRandom Forest Regression",
        "Difference between Regression and Classification": "",
        "Latest Courses": ""
    },
    "machine-learning-interview-questions": {
        "Python": "",
        "AI, ML and Data Science": "",
        "Java": "",
        "B.Tech and MCA": "",
        "Web Technology": "",
        "Software Testing": "",
        "Technical Interview": "",
        "Java Interview": "",
        "Web Interview": "",
        "Database Interview": "",
        "Company Interviews": "",
        "Machine Learning": "",
        "Supervised Learning": "",
        "Classification": "",
        "Miscellaneous": "",
        "Related Tutorials": "",
        "Interview Questions": "",
        "Machine Learning Interview Questions": "A list of frequently askedmachine learning interview questions and answersare given below.",
        "1) What do you understand by Machine learning?": "Machine learning is the form of Artificial Intelligence that deals with system programming and automates data analysis to enable computers to learn and act through experiences without being explicitly programmed. For example, Robots are coded in such a way that they can perform the tasks based on data they collect from sensors. They automatically learn programs from data and improve with experiences.",
        "2) Differentiate between inductive learning and deductive learning?": "In inductive learning, the model learns by examples from a set of observed instances to draw a generalized conclusion. On the other side, in deductive learning, the model first applies the conclusion, and then the conclusion is drawn. Inductive learning is the method of using observations to draw conclusions.Deductive learning is the method of using conclusions to form observations. For example, if we have to explain to a kid that playing with fire can cause burns. There are two ways we can explain this to a kid; we can show training examples of various fire accidents or images of burnt people and label them as \"Hazardous\". In this case, a kid will understand with the help of examples and not play with the fire. It is the form of Inductive machine learning. The other way to teach the same thing is to let the kid play with the fire and wait to see what happens. If the kid gets a burn, it will teach the kid not to play with fire and avoid going near it. It is the form of deductive learning.",
        "3) What is the difference between Data Mining and Machine Learning?": "Data miningcan be described as the process in which the structured data tries to abstract knowledge or interesting unknown patterns. During this process, machine learning algorithms are used. Machine learningrepresents the study, design, and development of the algorithms which provide the ability to the processors to learn without being explicitly programmed.",
        "4) What is the meaning of Overfitting in Machine learning?": "Overfitting can be seen in machine learning when a statistical model describes random error or noise instead of the underlying relationship. Overfitting is usually observed when a model is excessively complex. It happens because of having too many parameters concerning the number of training data types. The model displays poor performance, which has been overfitted.",
        "5) Why overfitting occurs?": "The possibility of overfitting occurs when the criteria used for training the model is not as per the criteria used to judge the efficiency of a model.",
        "6) What is the method to avoid overfitting?": "Overfitting occurs when we have a small dataset, and a model is trying to learn from it. By using a large amount of data, overfitting can be avoided. But if we have a small database and are forced to build a model based on that, then we can use a technique known ascross-validation. In this method, a model is usually given a dataset of a known data on which training data set is run and dataset of unknown data against which the model is tested. The primary aim of cross-validation is to define a dataset to \"test\" the model in the training phase. If there is sufficient data, 'Isotonic Regression' is used to prevent overfitting.",
        "7) Differentiate supervised and unsupervised machine learning.": "In supervised machine learning, the machine is trained using labeled data. Then a new dataset is given into the learning model so that the algorithm provides a positive outcome by analyzing the labeled data. For example, we first require to label the data which is necessary to train the model while performing classification.In the unsupervised machine learning, the machine is not trained using labeled data and let the algorithms make the decisions without any corresponding output variables.",
        "8) How does Machine Learning differ from Deep Learning?": "Machine learning is all about algorithms which are used to parse data, learn from that data, and then apply whatever they have learned to make informed decisions.Deep learning is a part of machine learning, which is inspired by the structure of the human brain and is particularly useful in feature detection.",
        "9) How is KNN different from k-means?": "KNN or K nearest neighbors is a supervised algorithm which is used for classification purpose. In KNN, a test sample is given as the class of the majority of its nearest neighbors. On the other side, K-means is an unsupervised algorithm which is mainly used for clustering. In k-means clustering, it needs a set of unlabeled points and a threshold only. The algorithm further takes unlabeled data and learns how to cluster it into groups by computing the mean of the distance between different unlabeled points.",
        "10) What are the different types of Algorithm methods in Machine Learning?": "The different types of algorithm methods in machine earning are: Supervised LearningSemi-supervised LearningUnsupervised LearningTransductionReinforcement Learning",
        "11) What do you understand by Reinforcement Learning technique?": "Reinforcement learning is an algorithm technique used in Machine Learning. It involves an agent that interacts with its environment by producing actions & discovering errors or rewards. Reinforcement learning is employed by different software and machines to search for the best suitable behavior or path it should follow in a specific situation. It usually learns on the basis of reward or penalty given for every action it performs.",
        "12) What is the trade-off between bias and variance?": "Both bias and variance are errors. Bias is an error due to erroneous or overly simplistic assumptions in the learning algorithm. It can lead to the model under-fitting the data, making it hard to have high predictive accuracy and generalize the knowledge from the training set to the test set. Variance is an error due to too much complexity in the learning algorithm. It leads to the algorithm being highly sensitive to high degrees of variation in the training data, which can lead the model to overfit the data. To optimally reduce the number of errors, we will need to tradeoff bias and variance.",
        "13) How do classification and regression differ?": "",
        "14) What are the five popular algorithms we use in Machine Learning?": "Five popular algorithms are: Decision TreesProbabilistic NetworksNeural NetworksSupport Vector MachinesNearest Neighbor",
        "15) What do you mean by ensemble learning?": "Numerous models, such as classifiers are strategically made and combined to solve a specific computational program which is known as ensemble learning. The ensemble methods are also known as committee-based learning or learning multiple classifier systems. It trains various hypotheses to fix the same issue. One of the most suitable examples of ensemble modeling is the random forest trees where several decision trees are used to predict outcomes. It is used to improve the classification, function approximation, prediction, etc. of a model.",
        "16) What is a model selection in Machine Learning?": "The process of choosing models among diverse mathematical models, which are used to define the same data is known asModel Selection. Model learning is applied to the fields ofstatistics,data mining, andmachine learning.",
        "17) What are the three stages of building the hypotheses or model in machine learning?": "There are three stages to build hypotheses or model in machine learning: Model buildingIt chooses a suitable algorithm for the model and trains it according to the requirement of the problem.Applying the modelIt is responsible for checking the accuracy of the model through the test data.Model testingIt performs the required changes after testing and apply the final model.",
        "18) What according to you, is the standard approach to supervised learning?": "In supervised learning, the standard approach is to split the set of example into the training set and the test.",
        "19) Describe 'Training set' and 'training Test'.": "In various areas of information of machine learning, a set of data is used to discover the potentially predictive relationship, which is known as 'Training Set'. The training set is an example that is given to the learner. Besides, the 'Test set' is used to test the accuracy of the hypotheses generated by the learner. It is the set of instances held back from the learner. Thus, the training set is distinct from the test set.",
        "20) What are the common ways to handle missing data in a dataset?": "Missing data is one of the standard factors while working with data and handling. It is considered as one of the greatest challenges faced by the data analysts. There are many ways one can impute the missing values. Some of the common methods to handle missing data in datasets can be defined asdeleting the rows, replacing with mean/median/mode, predicting the missing values, assigning a unique category, using algorithms that support missing values, etc.",
        "21) What do you understand by ILP?": "ILP stands forInductive Logic Programming. It is a part of machine learning which uses logic programming. It aims at searching patterns in data which can be used to build predictive models. In this process, the logic programs are assumed as a hypothesis.",
        "22) What are the necessary steps involved in Machine Learning Project?": "There are several essential steps we must follow to achieve a good working model while doing a Machine Learning Project. Those steps may includeparameter tuning, data preparation, data collection, training the model, model evaluation, and prediction, etc.",
        "23) Describe Precision and Recall?": "Precision and Recall both are the measures which are used in the information retrieval domain to measure how good an information retrieval system reclaims the related data as requested by the user. Precisioncan be said as a positive predictive value. It is the fraction of relevant instances among the received instances. On the other side,recallis the fraction of relevant instances that have been retrieved over the total amount or relevant instances. The recall is also known assensitivity.",
        "24) What do you understand by Decision Tree in Machine Learning?": "Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data.",
        "25) What are the functions of Supervised Learning?": "ClassificationSpeech RecognitionRegressionPredict Time SeriesAnnotate Strings",
        "26) What are the functions of Unsupervised Learning?": "Finding clusters of the dataFinding low-dimensional representations of the dataFinding interesting directions in dataFinding novel observations/ database cleaningFinding interesting coordinates and correlations",
        "27) What do you understand by algorithm independent machine learning?": "Algorithm independent machine learning can be defined as machine learning, where mathematical foundations are independent of any particular classifier or learning algorithm.",
        "28) Describe the classifier in machine learning.": "A classifier is a case of a hypothesis or discrete-valued function which is used to assign class labels to particular data points. It is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value, the class.",
        "29) What do you mean by Genetic Programming?": "Genetic Programming (GP)is almost similar to anEvolutionary Algorithm, a subset of machine learning. Genetic programming software systems implement an algorithm that uses random mutation, a fitness function, crossover, and multiple generations of evolution to resolve a user-defined task. The genetic programming model is based on testing and choosing the best option among a set of results.",
        "30) What is SVM in machine learning? What are the classification methods that SVM can handle?": "SVM stands forSupport Vector Machine. SVM are supervised learning models with an associated learning algorithm which analyze the data used for classification and regression analysis. The classification methods that SVM can handle are: Combining binary classifiersModifying binary to incorporate multiclass learning",
        "31) How will you explain a linked list and an array?": "An array is a datatype which is widely implemented as a default type, in almost all the modern programming languages. It is used to store data of a similar type. But there are many use-cases where we don't know the quantity of data to be stored. For such cases, advanced data structures are required, and one such data structure islinked list. There are some points which explain how the linked list is different from an array:",
        "32) What do you understand by the Confusion Matrix?": "A confusion matrix is a table which is used for summarizing the performance of a classification algorithm. It is also known as theerror matrix. Where, TN= True NegativeTP= True PositiveFN= False NegativeFP= False Positive",
        "33) Explain True Positive, True Negative, False Positive, and False Negative in Confusion Matrix with an example.": "True PositiveWhen a model correctly predicts the positive class, it is said to be a true positive.For example, Umpire gives a Batsman NOT OUT when he is NOT OUT.True NegativeWhen a model correctly predicts the negative class, it is said to be a true negative.For example, Umpire gives a Batsman OUT when he is OUT.False PositiveWhen a model incorrectly predicts the positive class, it is said to be a false positive. It is also known as 'Type I' error.For example, Umpire gives a Batsman NOT OUT when he is OUT.False NegativeWhen a model incorrectly predicts the negative class, it is said to be a false negative. It is also known as 'Type II' error.For example, Umpire gives a Batsman OUT when he is NOT OUT.",
        "34) What according to you, is more important between model accuracy and model performance?": "Model accuracy is a subset of model performance. The accuracy of the model is directly proportional to the performance of the model. Thus, better the performance of the model, more accurate are the predictions.",
        "35) What is Bagging and Boosting?": "Bagging is a process in ensemble learning which is used for improving unstable estimation or classification schemes.Boosting methods are used sequentially to reduce the bias of the combined model.",
        "36) What are the similarities and differences between bagging and boosting in Machine Learning?": "Similarities of Bagging and Boosting Both are the ensemble methods to get N learns from 1 learner.Both generate several training data sets with random sampling.Both generate the final result by taking the average of N learners.Both reduce variance and provide higher scalability. Differences between Bagging and Boosting Although they are built independently, but for Bagging, Boosting tries to add new models which perform well where previous models fail.Only Boosting determines the weight for the data to tip the scales in favor of the most challenging cases.Only Boosting tries to reduce bias. Instead, Bagging may solve the problem of over-fitting while boosting can increase it.",
        "37) What do you understand by Cluster Sampling?": "Cluster Sampling is a process of randomly selecting intact groups within a defined population, sharing similar characteristics. Cluster sample is a probability where each sampling unit is a collection or cluster of elements. For example, if we are clustering the total number of managers in a set of companies, in that case, managers (sample) will represent elements and companies will represent clusters.",
        "38) What do you know about Bayesian Networks?": "Bayesian Networks also referred to as 'belief networks' or 'casual networks', are used to represent the graphical model for probability relationship among a set of variables. For example, a Bayesian network can be used to represent the probabilistic relationships between diseases and symptoms. As per the symptoms, the network can also compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference or learning in Bayesian networks. Bayesian networks which relate the variables (e.g., speech signals or protein sequences) are called dynamic Bayesian networks.",
        "39) Which are the two components of Bayesian logic program?": "A Bayesian logic program consists of two components: LogicalIt contains a set of Bayesian Clauses, which capture the qualitative structure of the domain.QuantitativeIt is used to encode quantitative information about the domain.",
        "40) Describe dimension reduction in machine learning.": "Dimension reduction is the process which is used to reduce the number of random variables under considerations. Dimension reduction can be divided into feature selection and extraction.",
        "41) Why instance-based learning algorithm sometimes referred to as Lazy learning algorithm?": "In machine learning,lazy learningcan be described as a method where induction and generalization processes are delayed until classification is performed. Because of the same property, an instance-based learning algorithm is sometimes called lazy learning algorithm.",
        "42) What do you understand by the F1 score?": "The F1 score represents the measurement of a model's performance. It is referred to as a weighted average of the precision and recall of a model. The results tending to1are considered as the best, and those tending to0are the worst. It could be used in classification tests, where true negatives don't matter much.",
        "43) How is a decision tree pruned?": "Pruning is said to occur in decision trees when the branches which may consist of weak predictive power are removed to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can occur bottom-up and top-down, with approaches such asreduced error pruningandcost complexity pruning. Reduced error pruning is the simplest version, and it replaces each node. If it is unable to decrease predictive accuracy, one should keep it pruned. But, it usually comes pretty close to an approach that would optimize for maximum accuracy.",
        "44) What are the Recommended Systems?": "Recommended System is a sub-directory of information filtering systems. It predicts the preferences or rankings offered by a user to a product. According to the preferences, it provides similar recommendations to a user. Recommendation systems are widely used inmovies, news, research articles, products, social tips, music,etc.",
        "45) What do you understand by Underfitting?": "Underfitting is an issue when we have a low error in both the training set and the testing set. Few algorithms work better for interpretations but fail for better predictions.",
        "46) When does regularization become necessary in Machine Learning?": "Regularization is necessary whenever the model begins to overfit/ underfit. It is a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and reduce cost term. It helps to reduce model complexity so that the model can become better at predicting (generalizing).",
        "47) What is Regularization? What kind of problems does regularization solve?": "A regularization is a form of regression, which constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, it discourages learning a more complex or flexible model to avoid the risk of overfitting. It reduces the variance of the model, without a substantial increase in its bias. Regularization is used to address overfitting problems as it penalizes the loss function by adding a multiple of an L1 (LASSO) or an L2 (Ridge) norm of weights vector w.",
        "48) Why do we need to convert categorical variables into factor? Which functions are used to perform the conversion?": "Most Machine learning algorithms require number as input. That is why we convert categorical values into factors to get numerical values. We also don't have to deal with dummy variables. The functionsfactor()andas.factor()are used to convert variables into factors.",
        "49) Do you think that treating a categorical variable as a continuous variable would result in a better predictive model?": "For a better predictive model, the categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.",
        "50) How is machine learning used in day-to-day life?": "Most of the people are already using machine learning in their everyday life. Assume that you are engaging with the internet, you are actually expressing your preferences, likes, dislikes through your searches. All these things are picked up by cookies coming on your computer, from this, the behavior of a user is evaluated. It helps to increase the progress of a user through the internet and provide similar suggestions. The navigation system can also be considered as one of the examples where we are using machine learning to calculate a distance between two places using optimization techniques. Surely, people are going to more engage with machine learning in the near future.",
        "Latest Courses": ""
    }
}