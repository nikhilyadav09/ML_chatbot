{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 related links.\n",
      "Scraping: https://builtin.com/auth/signup?destination=%2fmachine-learning%2fmachine-learning-basics\n",
      "Scraping: https://builtin.com/tag/machine-learning-algorithms\n",
      "Scraping: https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fbuiltin.com%2Fmachine-learning%2Fmachine-learning-basics&mini=true\n",
      "Scraping: https://builtin.com/machine-learning/unsupervised-learning\n",
      "Scraping: https://builtin.com/machine-learning/common-loss-functions\n",
      "Scraping: https://builtin.com/tag/machine-learning\n",
      "Scraping: https://builtin.com/data-science/supervised-machine-learning-classification\n",
      "Scraping: https://builtin.com/machine-learning/sigmoid-activation-function#:~:text=The%20sigmoid%20activation%20function%20has,chaining%20such%20matrix%20multiplications%20together.\n",
      "Scraping: https://builtin.com/data-science/machine-learning-models-python\n",
      "Scraping: https://builtin.com/machine-learning/machine-learning-models-explained\n",
      "Scraping: https://builtin.com/machine-learning/classification-machine-learning\n",
      "Scraping: https://builtin.com/auth/login?destination=%2fmachine-learning%2fmachine-learning-basics\n",
      "Scraping: https://builtin.com/machine-learning\n",
      "Scraping: https://builtin.com/data-science/regression-machine-learning\n",
      "Scraping: https://builtin.com/machine-learning/supervised-learning\n",
      "Data extraction completed and saved to 'extracted_machine_learning_content.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://builtin.com/machine-learning/machine-learning-basics\"\n",
    "\n",
    "# JSON object to store the extracted data\n",
    "extracted_data = {}\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Scrape the content of a single page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract topic and content\n",
    "        # Assuming topics are in <h2> tags and content in <p> tags\n",
    "        topics = soup.find_all('h2')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        for topic in topics:\n",
    "            topic_text = topic.get_text(strip=True)\n",
    "            related_content = []\n",
    "            \n",
    "            # Extract following paragraphs as content related to the topic\n",
    "            for sibling in topic.find_next_siblings():\n",
    "                if sibling.name == 'h2':  # Stop at the next topic\n",
    "                    break\n",
    "                if sibling.name == 'p':\n",
    "                    related_content.append(sibling.get_text(strip=True))\n",
    "            \n",
    "            extracted_data[topic_text] = related_content\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url} - Status Code: {response.status_code}\")\n",
    "\n",
    "def find_related_links(url):\n",
    "    \"\"\"Find all links related to machine learning from the base page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all links related to machine learning\n",
    "        # Assuming links contain the term \"machine-learning\"\n",
    "        links = soup.find_all('a', href=True)\n",
    "        related_links = [link['href'] for link in links if 'machine-learning' in link['href']]\n",
    "        \n",
    "        # Ensure full URLs\n",
    "        related_links = [link if link.startswith('http') else f\"https://builtin.com{link}\" for link in related_links]\n",
    "        return set(related_links)  # Use a set to avoid duplicates\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url} - Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Find related links\n",
    "related_links = find_related_links(base_url)\n",
    "print(f\"Found {len(related_links)} related links.\")\n",
    "\n",
    "# Scrape each related link\n",
    "for link in related_links:\n",
    "    print(f\"Scraping: {link}\")\n",
    "    scrape_page(link)\n",
    "\n",
    "# Save data as JSON\n",
    "with open('extracted_machine_learning_content.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Data extraction completed and saved to 'extracted_machine_learning_content.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
