{
    "Better Jobs. Better Matches. Happier You.": [],
    "What Is Machine Learning?": [
        "Machine learning is asubfield of artificial intelligencein which systems “learn” through data, statistics and trial and error to optimize processes and innovate at quicker rates. Through machine learning, computers can applyhuman-like reasoningand decision-making to help solve some of the world’s toughest problems, ranging from cancer research to climate change.",
        "Most computer programs rely on code to tell them what to execute or what information to retain. This is known as explicit knowledge, which contains anything that is easily written or recorded like textbooks, videos and manuals. With machine learning, computers gaintacit knowledge, or knowledge gained from personal experience and context. This type of knowledge is hard to transfer from one person to the next via written or verbal communication.",
        "Facial recognitionis a type of tacit knowledge. We recognize a person’s face, but it is hard for us to describe how or why we recognize it. We rely on our personal knowledge banks to connect the dots and immediately recognize a person based on their face. Another example is riding a bike: It’s much easier to show someone how to ride a bike than it is to explain it.",
        "Computers no longer have to rely on billions of lines of code to carry out calculations. Machine learning gives computers the power of tacit knowledge to make connections,discover patternsand make predictions based on what they learned in the past. Machine learning’s use of tacit knowledge has made it ago-to technologyfor almost every industry from fintech to government.",
        ""
    ],
    "Testing for Sentience in AI: The Gaming Problem": [],
    "35 Machine Learning Companies You Should Know": [],
    "8 Anomaly Detection Algorithms to Know": [],
    "How to Fight AI-Generated Fake News — With AI": [],
    "Understanding Feature Importance in Machine Learning": [],
    "A Solution to Leakage in Applied Machine Learning": [],
    "18 Machine Learning in Education Examples": [],
    "Bag-of-Words Model in NLP Explained": [],
    "What Is Retrieval Augmented Generation (RAG)?": [],
    "We’ve emailed a one-time link to your primary email address": [
        "Click on the link to sign in instantly to your LinkedIn account.",
        "If you don’t see the email in your inbox, check your spam folder."
    ],
    "When Is Unsupervised Learning Used?": [
        "We often use unsupervised learning to perform more complex processing tasks, such as clustering large quantities of data.",
        "Unsupervised learning is often used to perform more complex processing tasks, such as clustering large quantities of data. Unlabeled data is more plentiful than labeled data and requires no human intervention before training the unsupervised models, which adds to unsupervised learning’s usefulness.",
        "Unsupervised machine learning is particularly useful for uncovering unknown patterns of data that can be further analyzed for feasibility and correctness. Additionally, we use unsupervised learning to discover features within submitted data that can be categorized in unexpected ways. This process requires further analysis to ensure these categories make sense.",
        ""
    ],
    "What Are Different Types of Unsupervised Learning?": [
        "The most common use of unsupervised learning is in clustering wherein differentalgorithmscreate multiple functionalities.",
        "Unsupervised learning models areincredibly usefulwhen it comes to organizing elements of large data sets into clusters for further analysis. Several different clustering algorithms exist to allow data to be organized as necessary.",
        "Some examples of clustering algorithms include hierarchical clustering, K-means clustering,KNN clustering,principal component analysis, singular value decomposition and independent component analysis.",
        "Aside from clustering, unsupervised learning can also be used for association, which allows models to create relationships between elements within databases and uncover unseen relationships between variables.",
        "More on Built InWhat Is Deep Learning and How Does It Work?",
        ""
    ],
    "Unsupervised Learning Clustering Algorithm Examples": [],
    "What Is the Difference Between Supervised and Unsupervised Learning?": [
        "Supervised learningrelies on using labeled data sets to operate. Unsupervised learning does not.",
        "Supervised learning is less versatile than unsupervised learning in that it requires the inputs and outputs of a data set to be labeled to provide a correct example formachine learningmodels to weigh predictions against. In other words, supervised learning requires human intervention to label data before the model is trained.",
        "Unsupervised learning models take longer to train and may produce less accurate outcomes than supervised models but can utilize unlabeled data to provide results. Ultimately, unsupervised learning is best used for analyzing and clustering large quantities of output data for further analysis, using methods such asprincipal component analysis, association and dimensionality reduction."
    ],
    "What Are Loss Functions in Machine Learning?": [],
    "Loss Functions": [
        "Broadly speaking, loss functions can be grouped into two major categories concerning the types of problems we come across in the real world:classificationandregression. In classification problems, our task is to predict the respective probabilities of all classes the problem is dealing with. On the other hand, when it comes to regression, our task is to predict the continuous value concerning a given set of independent features to the learning algorithm.",
        "",
        "More on Loss Functions From Built In Expert ContributorsThink You Don’t Need Loss Functions in Deep Learning? Think Again.",
        ""
    ],
    "Loss Functions for Classification": [
        "",
        "This is the most common loss function used in classification problems. The cross-entropy loss decreases as the predicted probability converges to the actual label. It measures the performance of a classification model whose predicted output is a probability value between0and1.",
        "When the number of classes is2, it’sbinary classification.",
        "When the number of classes is more than2, it’smulti-class classification.",
        "We derive the cross-entropy loss formula from the regular likelihood function, but with logarithms added in.",
        "More From SparshAnscombe’s Quartet: What Is It and Why Do We Care?",
        "",
        "The second most common loss function used for classification problems and an alternative to the cross-entropy loss function is hinge loss, primarily developed for support vector machine (SVM) model evaluation.",
        "Hinge loss penalizes the wrong predictions and the right predictions that are not confident. It’s primarily used with SVM classifiers with class labels as-1and1. Make sure you change your malignant class labels from0to-1.",
        ""
    ],
    "Types of Classification Losses": [],
    "Loss Functions for Regression": [
        "",
        "We define MSE loss function as the average of squared differences between the actual and the predicted value. It’s the most commonly used regression loss function.",
        "The corresponding cost function is themeanof these squared errors (MSE). The MSE loss function penalizes the model for making large errors by squaring them and this property makes the MSE cost function less robust to outliers.Therefore, you shouldn’t use it if the data is prone to many outliers.",
        "Looking for More Machine Learning Help? We Got You.5 Open-Source Machine Learning Libraries Worth Checking Out",
        "",
        "We define MAE loss function as the average of absolute differences between the actual and the predicted value. It’s the second most commonly used regression loss function. It measures the average magnitude of errors in a set of predictions, without considering their directions.",
        "The corresponding cost function is themeanof these absolute errors (MAE). The MAE loss function is more robust to outliers compared to the MSE loss function.Therefore, youshoulduse it if the data is prone to many outliers.",
        "",
        "The Huber loss function is defined as the combination of MSE and MAE loss functions because it approachesMSE when ? ~ 0 and MAE when ? ~ ∞(large numbers). It is mean absolute error, which becomes quadratic when the error is small. To make the error quadratic depends on how small that error could be, which is controlled by a hyperparameter, ? (delta) that you can tune.",
        "The choice of the delta value is critical because it determines what you’re willing to consider an outlier. Hence, the Huber loss function could be less sensitive to outliers than the MSE loss function, depending on the hyperparameter value. Therefore, you can use the Huber loss function if  the data is prone to outliers. In addition, we might need to train hyperparameter delta, which is an iterative process.",
        "Looking for More Tutorials? Yeah, We Have Those.5 Deep Learning Activation Functions You Need to Know",
        "",
        "The log-cosh loss function is defined as the logarithm of the hyperbolic cosine of the prediction error. It’s another function used in regression tasks that’s much smoother than MSE loss. It has all the advantages of Huber loss because it’s twice differentiable everywhere, unlike Huber loss, because some learning algorithms like XGBoost use Newton’s method to find the optimum, and hence the second derivative (Hessian).",
        "“Log(cosh(x))is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x. This means that ‘logcosh’ works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction.”",
        "",
        "A quantile is a value below which a fraction of samples in a group falls. Machine learning models work by minimizing (or maximizing) an objective function. As the name suggests, we apply the quantile regression loss function to predict quantiles. For a set of predictions, the loss will be its average.",
        "Quantile loss functionturns out to be useful when we’re interested in predicting an interval instead of only point predictions.",
        ""
    ],
    "Types of Regression Losses": [],
    "Why Loss Functions in Machine Learning Are Important": [
        "As mentioned, loss functions help gauge how a machine learning model is performing with its given data, and how well it’s able to predict an expected outcome. Many machine learning algorithms use loss functions in the optimization process during training to evaluate and improve its output accuracy. Also, by minimizing a chosen loss function during optimization, this can help determine the best model parameters needed for given data."
    ],
    "Random Forest: A Complete Guide for Machine Learning": [],
    "77 Artificial Intelligence Examples Shaking Up Business Across Industries": [],
    "Introduction to the Correlation Matrix": [],
    "Covariance vs. Correlation: Differences to Know": [],
    "Machine Learning for Smarter Trading: 17 Companies You Should Know": [],
    "Supervised Machine Learning Classification": [
        "Insupervised machine learning, algorithms learn from labeled data. After understanding the data, the algorithm determines which label should be given to new data by associating patterns to the unlabeled new data.",
        "Supervised learning can be divided into two categories: classification and regression.",
        "",
        "Classificationpredicts the category the data belongs to. Some examples of classification include spam detection, churn prediction, sentiment analysis, dog breed detection and so on.",
        "",
        "Regressionpredicts a numerical value based on previously observed data. Some examples of regression include house price prediction, stock price prediction, height-weight prediction and so on.",
        "Dive DeeperThe Top 10 Machine Learning Algorithms Every Beginner Should Know",
        ""
    ],
    "5 Types of Classification Algorithms for Machine Learning": [
        "Classification is a technique for determining which class the dependent belongs to based on one or more independent variables.",
        "",
        "Logistic regressionis kind of likelinear regression, but is used when the dependent variable is not a number but something else (e.g., a “yes/no” response). It’s called regression but performs classification based on the regression and it classifies the dependent variable into either of the classes.",
        "Logistic regression is used for prediction of output which isbinary, as stated above. For example, if a credit card company builds a model to decide whether or not to issue a credit card to a customer, it will model for whether the customer is going to “default” or “not default” on their card.",
        "Firstly, linear regression is performed on the relationship between variables to get the model. The threshold for the classification line is assumed to be at 0.5.",
        "",
        "K-NN algorithmis one of the simplest classification algorithms and it is used to identify the data points that are separated into several classes to predict the classification of a new sample point. K-NN is a non-parametric,lazy learning algorithm. It classifies new cases based on a similarity measure (i.e., distance functions).",
        "K-NN works well with a small number of input variables (p), but struggles when the number of inputs is very large.",
        "",
        "Support vector is used for both regression and classification. It is based on the concept of decision planes that define decision boundaries. A decision plane (hyperplane) is one that separates between a set of objects having different class memberships.",
        "It performs classification by finding the hyperplane that maximizes the margin between the two classes with the help of support vectors.",
        "The learning of the hyperplane in SVM is done by transforming the problem using somelinear algebra(i.e., the example above is a linear kernel which has a linear separability between each variable).",
        "For higher dimensional data, other kernels are used as points and cannot be classified easily. They are specified in the next section.",
        "Kernel SVMtakes in a kernel function in the SVM algorithm and transforms it into the required form that maps data on a higher dimension which is separable.",
        "Types of kernel functions:",
        "Radial Basis Function (RBF) Kernel",
        "The RBF kernel SVM decision region is actually also a linear decision region. What RBF kernel SVM actually does is create non-linear combinations of  features to uplift the samples onto a higher-dimensional feature space where  a linear decision boundary can be used to separate classes.",
        "So, the rule of thumb is: use linear SVMs for linear problems, and nonlinear kernels such as the RBF kernel for non-linear problems.",
        "",
        "The naive Bayes classifier is based on Bayes’ theorem with the independence assumptions between predictors (i.e., it assumes the presence of a feature in a class is unrelated to any other feature). Even if these features depend on each other, or upon the existence of the other features, all of these properties independently. Thus, the name naive Bayes.",
        "Based on naive Bayes, Gaussian naive Bayes is used for classification based on the binomial (normal) distribution of data.",
        "1. Calculate Prior Probability",
        "P(class)= Number of data points in the class/Total no. of observations",
        "P(yellow)= 10/17",
        "P(green)= 7/17",
        "2. Calculate Marginal Likelihood",
        "P(data)= Number of data points similar to observation/Total no. of observations",
        "P(?)= 4/17",
        "The value is present in checking both the probabilities.",
        "3. Calculate Likelihood",
        "P(data/class)= Number of similar observations to the class/Total no. of points in the class.",
        "P(?/yellow)= 1/7",
        "P(?/green)= 3/10",
        "4. Posterior Probability for Each Class",
        "5. Classification",
        "Multinomial, Bernoulli naive Bayes are the other models used in calculating probabilities. Thus, a naive Bayes model is easy to build, with no complicated iterative parameter estimation, which makes it particularly useful for very large datasets.",
        "",
        "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. It follows Iterative Dichotomiser 3 (ID3) algorithm structure for determining the split.",
        "Entropy and information gain are used to construct a decision tree.",
        "Entropy is the degree or amount of uncertainty in the randomness of elements. In other words, it is a measure of impurity.",
        "Intuitively, it tells us about the predictability of a certain event. Entropy calculates the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero, and if the sample is equally divided it has an entropy of one.",
        "Information gain measures the relative change in entropy with respect to the independent attribute. It tries to estimate the information contained by each attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain (i.e., the most homogeneous branches).",
        "WhereGain(T, X)is the information gain by applying featureX.Entropy(T)is the entropy of the entire set, while the second term calculates the entropy after applying the featureX.",
        "Information gain ranks attributes for filtering at a given node in the tree. The ranking is based on the highest information gain entropy in each split.",
        "The disadvantage of a decision tree model is overfitting, as it tries to fit the model by going deeper in the training set and thereby reducing test accuracy.",
        "Overfitting in decision trees can be minimized by pruning nodes.",
        ""
    ],
    "What Is a Classifier?": [],
    "Ensemble Methods for Classification": [
        "An ensemble model is ateam of models. Technically, ensemble models comprise several supervised learning models that are individually trained and the results merged in various ways to achieve the final prediction. This result has higher predictive power than the results of any of its constituting learning algorithms independently.",
        "",
        "Random forestclassifier is an ensemble algorithm based on bagging i.e bootstrap aggregation. Ensemble methodscombines more than one algorithm of the same or different kind for classifying objects (i.e., an ensemble of SVM, naive Bayes or decision trees, for example.)",
        "The general idea is that a combination of learning models increases the overall result selected.",
        "Deep decision trees may suffer from overfitting, but random forests prevent overfitting by creating trees on random subsets. The main reason is that it takes the average of all the predictions, which cancels out the biases.",
        "Random forest adds additional randomness to the model while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.",
        "",
        "Gradient boosting classifier is a boosting ensemble method. Boosting is a way to combine (ensemble) weak learners, primarily to reduce prediction bias. Instead of creating a pool of predictors, as in bagging, boosting produces a cascade of them, where each output is the input for the following learner. Typically, in a bagging algorithm trees are grown in parallel to get the average prediction across all trees, where each tree is built on a sample of original data. Gradient boosting, on the other hand, takes a sequential approach to obtaining predictions instead of parallelizing the tree building process. In gradient boosting, each decision tree predicts the error of the previous decision tree — therebyboosting(improving) the error (gradient).",
        ""
    ],
    "Metrics to Measure Classification Model Performance": [
        "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It is a table with four different combinations of predicted and actual values in the case for a binary classifier.",
        "The confusion matrix for a multi-class classification problem can help you determine mistake patterns.",
        "For a binary classifier:",
        "A true positive is an outcome where the model correctly predicts the positiveclass. Similarly, a true negative is an outcome where the model correctly predicts the negative class.",
        "The terms false positive and false negative are used in determining how well the model is predicting with respect to classification. A false positive is an outcome where the modelincorrectlypredicts thepositiveclass. And a false negative is an outcome where the modelincorrectlypredicts thenegativeclass. The more values in main diagonal, the better the model, whereas the other diagonal gives the worst result for classification.",
        "False Positive",
        "False positive(type I error)— when you reject a true null hypothesis.",
        "This is an example in which the model mistakenly predicted thepositive class. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam. It’s like a warning sign that the mistake should be rectified as it’s not much of a serious concern compared to false negative.",
        "False Negative",
        "False negative(type II error)— when you accept a false null hypothesis.",
        "This is an example in which the model mistakenly predicted thenegative class. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam. It’s like a danger sign that the mistake should be rectified early as it’s more serious than a false positive.",
        "From the confusion matrix, we can infer accuracy, precision, recall and F-1 score.",
        "Accuracy",
        "Accuracy is the fraction of predictions our model got right.",
        "Accuracy can also be written as",
        "Accuracy alone doesn’t tell the full story when working with a class-imbalanced data set, where there is a significant disparity between the number of positive and negative labels. Precision and recall are better metrics for evaluating class-imbalanced problems.",
        "Precision",
        "Out of all the classes, precision is how much we predicted correctly.",
        "Precision should be as high as possible.",
        "Recall",
        "Out of all the positive classes, recall is how much we predicted correctly. It is also called sensitivity or true positive rate (TPR).",
        "Recall should be as high as possible.",
        "F-1 Score",
        "It is often convenient to combine precision and recall into a single metric called the F-1 score, particularly if you need a simple way to compare two classifiers. The F-1 score is the harmonic mean of precision and recall.",
        "The regular mean treats all values equally, while the harmonic mean gives much more weight to low values thereby punishing the extreme values more. As a result, the classifier will only get a high F-1 score if both recall and precision are high.",
        "",
        "ROC curve is an important classification evaluation metric. It tells us how well the model has accurately predicted. The ROC curve shows the sensitivity of the classifier by plotting the rate of true positives to the rate of false positives. If the classifier is outstanding, the true positive rate will increase, and the area under the curve will be close to one. If the classifier is similar to random guessing, the true positive rate will increase linearly with the false positive rate. The better the AUC measure, the better the model.",
        "",
        "The CAP of a model represents the cumulative number of positive outcomes along they-axis versus the corresponding cumulative number of a classifying parameters along thex-axis. The CAP is distinct from the receiver operating characteristic (ROC), which plots the true-positive rate against the false-positive rate. CAP curve is rarely used as compared to ROC curve.",
        "Consider a model that predicts whether a customer will purchase a product. If a customer is selected at random, there is a 50 percent chance they will buy the product. The cumulative number elements for which the customer buys would rise linearly toward a maximum value corresponding to the total number of customers. This distribution is called the “random” CAP. Its the blue line in the above diagram. A perfect prediction, on the other hand, determines exactly which customer will buy the product, such that the maximum customer buying the property will be reached with a minimum number of customer selection among the elements. This produces a steep line on the CAP curve that stays flat once the maximum is reached, which is the “perfect” CAP. It’s also called the “ideal” line and is the grey line in the figure above.",
        "In the end, a model should predict where it maximizes the correct predictions and gets closer to a perfect model line.",
        "",
        "References:Classifier Evaluation With CAP Curve in Python",
        "Classification Implementation:Github Repo."
    ],
    "What Is the Sigmoid Activation Function?": [],
    "Why Is the Sigmoid Function Important?": [
        "The importance of the sigmoid is to some degree historical. It’s one of the earliest activation functions that was used in neural networks. But what exactly are activation functions? Briefly, you can think of each layer in a neural network as a matrix that takes in an input vector and produces an output vector. The whole network is formed by chaining such matrix multiplications together.",
        "Just composing matrices isn’t enough, however. If we used only matrix multiplications, then our network could only ever represent linear functions, but we want it to learn any necessary functional relationship. To allow for this, we must apply some non-linear function after each matrix multiplication. That’s what an activation function does.",
        "Neural networks initially took inspiration from the brain, where neurons behave in a binary fashion: Either they fire or they don’t. Inspired by this, we might try applying an activation function that transforms a vector to be just zeroes and ones. We need our activation function to be smooth in order to apply backpropagation and learn, however. Technically, we require that the function be differentiable. To differentiatea function means to find its slope at each point. For a function to be differentiable, it must have a well-defined slope at each point. Non-differentiable functions have either sudden jumps or sharp turns.",
        "A true binary activation function like this wouldn’t even be continuous, so it won’t work for our purposes. A continuous function is one with no sudden jumps. All differentiable functions have to be continuous. A binary activation function would have to jump straight from zero to one at some point as we adjust the input, and hence is not continuous.",
        "This principle is what motivates the sigmoid function. It’s a smooth version of our idea above. It maps most inputs to be either very close to zero or very close to one, while still being differentiable.",
        "The sigmoid has some inefficiencies, which we’ll discuss, that have reduced its usage in more recent years. But it still plays a central role in binary classification, which we’ll also discuss. For now, let’s dig more into the function itself.",
        ""
    ],
    "Sigmoid Activation Function Formula": [
        "First, we should clear up some terminological confusion. Technically speaking, a “sigmoid” is any S-shaped curve that flattens out near its minimum and maximum values. For example, the hyperbolic tangent (tanh) is technically a sigmoid function:",
        "In modern machine learning parlance, however, “sigmoid activation function” typically refers specifically to the logistic sigmoid function:",
        "From here on out, when we say “sigmoid,” we just mean the logistic function. Its equation is:",
        "As we said before, it’s differentiable, non-linear, has a range from zero to one, and squishes most values toward the minimum or maximum.",
        ""
    ],
    "Components of Function": [
        "The input to the sigmoid is given by the valuex. The exponential term in the denominator means that asxgets large,e-xshrinks rapidly, approaching zero. Thus, the whole function quickly approaches one. Conversely, for smallx(i.e., large negativex),e-xgrows rapidly, approaching infinity. In this case, the whole function swiftly converges to zero.",
        "If you have some mathematical expertise, you may be aware that the exponential functionexis approximately linear for small values ofx. This is why the sigmoid looks almost like a straight line aroundx=0, but rapidly approaches zero or one as we move further away.",
        "The mathematically inclined may also know thatexis particularly easy to differentiate, the derivative ofexbeingexitself. This ease carries over to the sigmoid. Its derivative is also very simple to compute:",
        "This is convenient since, to train a neural network, we must know how changing the network weights will affect the final output. The slope (or rate of change) of the activation function is critical for calculating this, and it’s easy to determine for the sigmoid.",
        "More in Machine LearningGuide to Meta Learning",
        ""
    ],
    "Applications of Function": [
        "The sigmoid can be used simply as an activation function throughout a neural network, applying it to the outputs of each network layer. It isn’t used as much nowadays, however, because it has a couple of inefficiencies.",
        "The first is the problem of saturating gradients. Looking at its graph, we can see that the sigmoid has a strong slope in the middle, but at the ends, its slope is very shallow. This is a problem for learning. At a high level, when we run gradient descent, many of the neurons in our network will be outputting values in the shallow regions of the sigmoid. Changing the network weights will then have little effect on its overall output, and learning comes to a halt.",
        "In a little more detail, to run backpropagation and learn, we must take the gradient of the loss function with respect to each parameter in our network. At first, some neurons may be outputting values in the middle of the sigmoid range, where the slope is strong. But as we make updates, we move up or down this slope and quickly end up in a shallow region. The magnitude of our gradient then becomes smaller and smaller, meaning we take smaller and smaller learning steps. Learning is not very efficient this way.",
        "The other problem with the sigmoid is that it’s not symmetric about the origin. In the brain, neurons either fire or don’t, so we may have the intuition that neuron activations should be zero or one. Despite this, researchers have actually found that neural networks learn better when activations are centered around zero. This is one of the reasons it’s a good idea tostandardizeyour data (i.e., shift it to have mean zero) before feeding it into a neural network. It’s also one of the reasons forbatch normalization, a similar process where we standardize our network activations at intermediate layers rather than just at the start.",
        "If you look at the beginning of the previous section, you’ll see that the tanh function ranges from -1 to one and is centered around zero. For this reason, it’s often preferable to the sigmoid. It also has the problem of saturating gradients, though. The most common activation function nowadays is therectified linear unit(ReLU):",
        "This function has a strong slope everywhere to the right of zero, although it’s obviously not symmetric around zero. So, tanh has saturating gradients, and ReLU is non-symmetric. In practice, the former is a bigger problem than the latter. The moral here, though, is that the sigmoid is the worst of both worlds on these fronts.",
        "Despite all this, the sigmoid still has a place in modern machine learning: binary classification. In binary classification, we categorize inputs as one of two classes. If we’re using neural networks, the output of our network must be a number between zero and one, representing the probability that the input belongs to class one (with the probability for class two being immediately inferable).",
        "The output layer of such a network consists of a single neuron. Consider the output value of this neuron. Before applying any activation function, it can be any real number, which is no good. If we apply a ReLU, it will be positive (or zero). If we use tanh, it will be between -1 and one. None of these work. We must apply a sigmoid to this last neuron. We need a number between zero and one, and we still need the activation function to be smooth for the purposes of training. The sigmoid is the right choice.",
        "In these cases, we can still use some other activation function for the earlier layers in the network. It’s only at the very end that we need the sigmoid. The use of sigmoid in this way is still absolutely standard in machine learning and is unlikely to change anytime soon. Thus, the sigmoid lives on!"
    ],
    "Model Testing, Feature Selection and Hyperparameter Tuning": [],
    "Reading and Displaying Telco Data": [
        "To start, let’s import the Pandas library and read in the telco data into a Pandas data frame:",
        ""
    ],
    "Methods for Splitting Data for Training and Testing": [
        "The first testing method we will discuss is randomized train-test split. Let’s build a simple model that takes customer tenure and monthly charges as inputs and predicts whether or not the customer will churn. Here, our inputs will be tenure and monthly charges and our output will be churn. First, let’s convert the churn values to machine-readable binary integers using the np.where() method from the numpy package:",
        "Now, let’s import the train_test_split method from the model selection module in Scikit-learn:",
        "As explained in thedocumentation, the train_test_split method splits the data into random training and testing subsets. To perform the split, we first define our input and output in terms of variables called X and y, respectively:",
        "Next, we pass in these variables into the train_test_split method, which returns random subsets for training input and testing input as well as training output and testing output. In Python, when a method or function returns multiple values, they are typically tuples. We will need to unpack the return tuples and store the correct values in our training and testing variables:",
        "We can also specify a parameter called random_state. This is a shuffling parameter that controls how the data is randomly split. If we give it an integer value, we ensure that we reproduce the same split upon each run:",
        "It is then straightforward to train and test a model using training and testing subsets. For  example, to train a random forest:",
        "You can then use y_test and y_pred to evaluate the performance of your model. Remember to consider whether or not you have time-dependent data. If this is the case,  instead of a randomized split, split your data along a date. If you use a randomized split, you will include future data in your training set and your predictions will be biased.",
        "In practice, a randomized train-test split is useful for generating a hold-out validation set that you use for testing once after feature selection and model tuning has been completed. This ensures that your model isn’t overfit and reduces the chance of badly biasing your model.",
        "K-fold cross-validation is the process of splitting data into an integer number (K) parts and using one part for testing and the rest for training. This process is done iteratively until all data has been used for training and testing. The documentation for the cross-validation method can be foundhere.",
        "To implement K folds, we import KFold from the model selection module in Scikit-learn:",
        "Within the for loop, you can train and test your model on different folds. Here, we only used five folds, but you can change the n_splits parameter in the KFold method to be whatever number you’d like. The more folds you use the less bias there will be in your model output.",
        "This process allows you to analyze the stability of your model’s performance through metrics such as variance. It’s also typically used for tasks such as model tuning and feature selection, which we will cover shortly.",
        "Ideally, you want the most accurate model with the lowest variance in performance. Low performance variance means the model is more stable and consequently more reliable.",
        "Leave one out is similar to K-fold, but instead of using a randomly sampled subset for training and test, a single data point is used for testing while the rest are used for training. This process is also done iteratively until all of the data has been used for training and testing:",
        "This method is typically used for smaller data sets. In my experience, I have found this to be particularly useful for small imbalance data sets. Note that, since you are training your modelntimes, wherenis the size of the data, this approach can be computationally intensive for large data sets.",
        ""
    ],
    "Methods of Feature Selection for Model Building": [
        "Other than manual feature selection, which is typically done through exploratory data analysis and using domain expertise, you can use some Python packages for feature selection. Here, we will discuss the SelectKBest method. The documentation for SelectKBest can be foundhere. First, let’s import the necessary packages:",
        "We will select from monthly charges, tenure and total charges. First, we need to clean the total charges column:",
        "We should select features on a training set, so that we don’t bias our model. Let’s redefine our input and output:",
        "Now, let’s perform our randomized train-test split:",
        "Next, let’s define our selector object. We will pass in our features and the output:",
        "Now, we can plot the scores for our features. We generate scores for each feature by taking a negative log of the p values for each feature:",
        "We can see that tenure has the highest score, which makes sense. Intuitively, customers that have been with the company for longer are less likely to churn. I want to stress that, in practice, feature selection should be performed on training data. Further, to improve the reliability of the features selected you can run K-fold cross-validation and take the average score for each feature and use the results for feature selection.",
        "More in Data ScienceHow To Append Lists in Python",
        ""
    ],
    "Hyperparameter Tuning": [
        "In addition to model testing and feature selection, model hyperparameter tuning is another very important part of model building. The idea is to search for the model parameters that give the best performance.",
        "The RandomizedSearchCV method from Scikit-learn allows you to perform a randomized search over parameters for an estimator. The documentation for RandomizedSearchCV can be foundhere. Here, we will perform a randomized search for random forest parameters. We start by defining a grid of random forest parameter values. First, let’s specify a list of the number of trees we will use in the random forest:",
        "Then, we specify the number of features to consider at every split",
        "We also specify the maximum number of levels in a tree:",
        "The minimum number of samples required to split a node:",
        "The minimum number of samples required at each leaf node:",
        "And, finally, whether or not we will use bootstrap sampling:",
        "We can now specify a dictionary which will be our grid of parameters:",
        "Let’s also define a random forest model object:",
        "Similar to feature selection, model hyperparameter tuning should be done on the training data. To proceed, let’s import RandomizedSearchCV from Scikit-learn:",
        "Next, we define a RandomizedSearchCV object. In the object, we pass in our random forest model, the random_grid, and the number of iterations for each random search.",
        "Notice there is a parameter called cv, which is for cross-validation. We use this parameter to define the number of folds to be used for validation, just as we did for K-folds. Again, we’d like to find the set of random forest parameters that give the best model performance and model performance is calculated by RandomizedSearchCV using cross-validation.",
        "The parameter verbose displays output for each iteration. Since we have three folds and three iterations, we should see output for nine test runs:",
        "After defining the RandomizedSearchCV object, we can fit to our training data:",
        "Upon fitting, we can output the parameters that give the best performance:",
        "You can increase the number of iterations to search and test more parameters. The larger the number of iterations, the more likely you are to find a better-performing model from the set of hyperparameters. Obviously, the more parameters you search and test the longer the calculation will take.",
        "Keep in mind that on a typical machine or laptop, this process may become intractable for very large data sets and you may need to use distributed computing tools such as databricks. There are other hyperparameter tuning tools that are useful to know as well. For example, GridSearchCV performs an exhaustive search on the entire grid. This means that every possible combination of parameters gets tested. This method is a good option if you have sufficient computational power.",
        ""
    ],
    "Get Started With Model Building Now": [
        "Having a good understanding of which tools are available for building robust machine learning models is a skill every data scientist should have. Being able to prepare data for training and testing, to select features, and to tune model parameters is necessary for building stable models whose predictions are reliable. Further, having these tools in your back pocket can save significant labor hours since these methods automate what would otherwise be done manually. These techniques, when used correctly, can reduce the risk of model deterioration that can cost companies millions of dollars down the road.",
        "Understanding how to appropriately set up model testing is also vital for ensuring that you don’t overfit your models. For example, if you don’t correctly split your data for training and testing, your model tests can give you a false sense of model accuracy, which can be very expensive for a company. In the case of churn, if your model is deceptively accurate, you may incorrectly target customers with ads and discounts who aren’t actually likely to churn. This can result in millions lost in ad dollars.",
        "To reduce the risk of overfitting and overestimating model performance, it is crucial to have a hold-out test set (like what we generate from the randomized train test split) which we perform a single test on after model tuning and feature selection on the training data. Further, the cross-validation methods give us a good understanding of how stable our models are both in terms of model tuning and feature selection. Cross-validation allows us to see how performance varies across multiple tests. Ideally, we’d like to select the features and model parameters that give the best performance with the lowest variance in model performance. Low variance in model performance, means that model predictions are more reliable and lower risk.",
        "Feature selection is also very important since it can help filter down a potentially large number of inputs. No matter how accurate a model is, it will be very difficult to interpret how the values for 1,000 features, for example, determine whether or not a customer will churn. Feature selection, along with domain expertise, can help data scientists select and interpret the most important features for predicting an outcome.",
        "Hyperparameter tuning is also an essential step required for achieving optimal model performance. While default parameter values provided by Scikit-learn machine learning packages typically provide decent performance, model tuning is required in order to achieve very accurate models. For example, with churn, the more accurately you can target customers likely to leave a company, the less money is lost to incorrectly targeted customers and the more you increase the customer lifetime value of customers at high risk of leaving.",
        "Having a strong familiarity with tools available for setting up model testing, selecting features and performing model tuning is an invaluable skill set for data scientists in any industry. Having this knowledge can help data scientists build robust and reliable models that can add significant value to a company, resulting in savings in resources in terms of money and labor, as well as increase company profits. The code from this post is available onGitHub."
    ],
    "9 Machine Learning Models to Know": [],
    "Supervised Machine Learning Models Explained": [
        "Supervised learning involves learning a function that maps an input to an output based on example input-output pairs.",
        "For example, if I had a data set with two variables, age (input) and height (output), I could implement a supervised learning model to predict the height of a person based on their age.",
        "Within supervised learning, there are two sub-categories: regression and classification.",
        "More on Machine LearningA Deep Dive Into Non-Maximum Suppression (NMS)",
        ""
    ],
    "Regression Models for Machine Learning": [
        "In regression models, the output is continuous. Below are some of the most common types of regression models.",
        "",
        "The idea oflinear regressionis simply finding a line that best fits the data. Extensions of linear regression include multiple linear regression, or finding a plane of best fit, and polynomial regression, or finding a curve of best fit.",
        "",
        "Decision treesare a popular model, used in operations research, strategic planning and machine learning. Each square in a decision tree is called a node, and the more nodes you have, the more accurate your decision tree will generally be. The last nodes of the decision tree, where a decision is made, are called the leaves of the tree. Decision trees are intuitive and easy to build but fall short when it comes to accuracy.",
        "More on Machine LearningFull Tree vs. Complete Binary Tree: What’s the Difference?",
        "",
        "Random forestsare anensemble learningtechnique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped data sets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. What’s the point of this? By relying on a “majority wins” model, it reduces the risk of error from an individual tree.",
        "",
        "For example, if we created one decision tree, the third one, it would predict 0. But if we relied on the mode of all four decision trees, the predicted value would be 1. This is the power ofrandom forests.",
        "",
        "Aneural networkis a network of mathematical equations. It takes one or more input variables, and by going through a network of equations, results in one or more output variables. You can also say that a neural network takes in a vector of inputs and returns a vector of outputs, but I won’t get into matrices in this article.",
        "The blue circles represent the input layer,the black circles represent the hidden layers,and the green circles represent the output layer.Each node in the hidden layers represents both a linear function and an activation function that the nodes in the previous layer go through, ultimately leading to an output in the green circles.",
        ""
    ],
    "Machine Learning Classification Models": [
        "In classification models, the output is discrete. Below are some of the most common types of classification models.",
        "",
        "Logistic regressionis similar to linear regression but is used to model the probability of a finite number of outcomes, typically two. There are a number of reasons why logistic regression is used over linear regression when modeling probabilities of outcomes. In essence, a logistic equation is created in such a way that the output values can only be between 0 and 1 .",
        "",
        "Asupport vector machine is a supervised classification technique that can actually get pretty complicated but is pretty intuitive at the most fundamental level.",
        "Let’s assume that there are two classes of data. A support vector machine will find a hyperplane or a boundary between the two classes of data that maximizes the margin between the two classes. There are many planes that can separate the two classes, but only one plane can maximize the margin or distance between the classes.",
        "",
        "Naive Bayesis another popular classifier used in data science. The idea behind it is driven by Bayes Theorem:",
        "This equation is used to answer the following question: “What is the probability of y (my output variable) given X?” And because of the naive assumption that variables are independent given the class, you can say that:",
        "By removing the denominator, we can then say that P(y|X) is proportional to the right-hand side.",
        "Therefore, the goal is to find the class y with the maximum proportional probability.",
        "",
        "These models follow the same logic as previously explained. The only difference is that that output is discrete rather than continuous.",
        ""
    ],
    "Unsupervised Learning Machine Learning Models": [
        "Unlike supervised learning, unsupervised learningis used to draw inferences and find patterns from input data without references to labeled outcomes. Two main methods used in unsupervised learning include clustering and dimensionality reduction.",
        "",
        "Clusteringis an unsupervised technique that involves the grouping, or clustering, of data points. It’s frequently used for customer segmentation, fraud detection and document classification.",
        "Common clustering techniques includek-means clustering, hierarchical clustering, mean shift clustering, and density-based clustering. While each technique has a different method in finding clusters, they all aim to achieve the same thing.",
        "More on Machine LearningThe 7 Most Common Machine Learning Loss Functions Explained",
        ""
    ],
    "Dimensionality Reduction": [
        "Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. In simpler terms, it’s the process of reducing the dimension of your feature set, i.e. reducing the number of features. Mostdimensionality reduction techniques can be categorized as eitherfeature elimination orfeature extraction.",
        "A popular method of dimensionality reduction is called principal component analysis.",
        "",
        "In the simplest sense,PCAinvolves projecting higher dimensional data, such as three dimensions, to a smaller space, like two dimensions. This results in a lower dimension of data, two dimensions instead of three, while keeping all original variables in the model."
    ],
    "What Is Classification in Machine Learning?": [],
    "Types of Classification in Machine Learning": [
        "There are two types of learners in classification — lazy learners and eager learners.",
        "",
        "Lazy learners store the training data and wait until testing data appears. When it does, classification is conducted based on the most related stored training data. Compared to eager learners, lazy learners spend less training time but more time in predicting.",
        "Examples:K-nearest neighborand case-based reasoning.",
        "",
        "Eager learners construct a classification model based on the given training data before receiving data for classification. It must be able to commit to a single hypothesis that covers the entire instance space. Because of this, eager learners take a long time for training and less time for predicting.",
        "Examples:Decision tree, naive Bayes andartificial neural networks.",
        "More on Machine Learning:Top 10 Machine Learning Algorithms Every Beginner Should Know",
        ""
    ],
    "Classification Algorithms": [
        "There are a lot of classification algorithms to choose from. Picking the right one depends on the application and nature of the available data set. For example, if the classes are linearly separable, linear classifiers likelogistic regressionand Fisher’s linear discriminant can outperform sophisticated models and vice versa.",
        "",
        "Adecision treebuilds classification or regression models in the form of a tree structure. It utilizes an “if-then” rule set that is mutually exclusive and exhaustive for classification. The rules are learned sequentially using the training data one at a time. Each time a rule is learned, the tuples covered by the rules are removed. This process continues until it meets a termination condition.",
        "The tree is constructed in a top-down, recursive, divide-and-conquer manner. All attributes should be categorical. Otherwise, they should be discretized in advance. Attributes in the top of the tree have more impact in the classification, and they are identified using the information gain concept.",
        "A decision tree can be easily over-fitted generating too many branches and may reflect anomalies due to noise or outliers. An over-fitted model results in very poor performances on the unseen data, even though it gives off an impressive performance on training data. You can avoid this with pre-pruning, which halts tree construction early, or through post-pruning, which removes branches from the fully grown tree.",
        "",
        "Naive Bayes is aprobabilistic classifierinspired by the Bayes theorem under the assumption that attributes are conditionally independent.",
        "The classification is conducted by deriving the maximum posterior, which is the maximalP(Ci|X), with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption isn’t valid in most cases since the attributes are dependent, surprisingly, naive Bayes is able to perform impressively.",
        "Naive Bayes is a simple algorithm to implement and can yield good results in most cases. It can be easily scaled to larger data sets since it takes linear time, rather than the expensive iterative approximation that other types of classifiers use.",
        "Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction.This needs to be fixed explicitly using a Laplacian estimator.",
        "",
        "An artificial neural network is a set of connected input/output units, where each connection has a weight associated with it. A team of psychologists and neurobiologists founded it as a way to develop and test computational analogs of neurons. During the learning phase, the network learns by adjusting the weightsso as to be able to predict the correct class label of the input tuples.",
        "There are several network architectures available today, including feed-forward, convolutional and recurrent networks. The appropriate architecture depends on the application of the model. For most cases, feed-forward models give reasonably accurate results, but convolutional networks perform better for image processing.",
        "There can bemultiple hidden layersin the model depending on the complexity of the function that the model is going to map. These hidden layers will allow you to model complex relationships, such asdeep neural networks.",
        "However, when there are many hidden layers, it takes a lot of time to train and adjust the weights. The other disadvantage of this is thepoor interpretabilityof the model compared to others like decision trees. This is due to the unknown symbolic meaning behind the learned weights.",
        "But artificial neural networks have performed impressively in most real world applications. It has a high tolerance for noisy data and is able to classify untrained patterns. Usually, artificial neural networks perform better with continuous-valued inputs and outputs.",
        "All of the above algorithms are eager learners since they train a model in advance to generalize the training data and use it for prediction later.",
        "",
        "K-Nearest Neighbor is a lazy learning algorithm that stores all instances corresponding to training data points in n-dimensional space. When an unknown discrete data is received, it analyzes the closest k number of instances saved (nearest neighbors) and returns the most common class as the prediction. For real-valued data, it returns the mean of k nearest neighbors.",
        "In the distance-weighted nearest neighbor algorithm, it weighs the contribution of each of the k neighbors according to their distance using the following query, giving greater weight to the closest neighbors:",
        "Usually, KNN is robust to noisy data since it is averaging the k-nearest neighbors.",
        ""
    ],
    "Important Classification Algorithms to Know": [],
    "How to Evaluate a Classifier": [
        "After training the model, the most important part is to evaluate the classifier to verify its applicability.",
        "More on Machine Learning:How Does Backpropagation in a Neural Network Work?",
        "There are several methods to evaluate a classifier, but the most common way is the holdout method. In it, the given data set is divided into two partitions,test and train. Twenty percent of the data is used as a test and 80 percent is used to train. The train set will be used to train the model, and the unseen test data will be used to test its predictive power.",
        "",
        "Overfitting is a common problem in machine learning and it occurs in most models. K-fold cross-validation can be conducted to verify that the model is not overfitted. In this method, the data set is randomly partitioned into k-mutually exclusive subsets, each approximately equal in size. One is kept for testing while others are used for training. This process is iterated throughout the whole k folds.",
        "",
        "Precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.Precision and recallare used as a measurement of the relevance.",
        "",
        "AROC curveprovides a visual comparison of classification models, showing the trade-off between the true positive rate and the false positive rate.",
        "Thearea under the ROC curveis a measure of the accuracy of the model. When a model is closer to the diagonal, it is less accurate. A model with perfect accuracy will have an area of 1.0."
    ],
    "Machine Learning Classifier Evaluation Methods": [],
    "How Does Machine Learning Work?": [
        "Machine learning compiles input data, which can be data gathered from training sessions or other sources, such as data set search engines, .gov websites andopen data registrieslike that of Amazon Web Services. This data serves the same function that prior experiences do for humans, giving machine learning models historical information to work with when making future determinations.",
        "Algorithmsthen analyze this data, searching for patterns and trends that allow them to make accurate predictions. In this way, machine learning can glean insights from the past toanticipate future happenings. Typically, the larger the data set that a team can feed to machine learning software, the more accurate the predictions.",
        "The idea is that machine learning algorithms should be able to perform these tasks on their own, requiring minimalhuman intervention. This speeds up various processes as machine learning comes to automate many aspects of different industries.",
        ""
    ],
    "Types of Machine Learning": [
        "There is a range ofmachine learning typesthat vary based on several factors like data size and diversity. Below are a few of the most common types of machine learning under whichpopular machine learning algorithmscan be categorized.",
        "Supervised learninginvolves mathematical models of data that contain both input and output information. Machine learning computer programs are constantly fed these models, so the programs can eventually predict outputs based on a new set of inputs.",
        "Regressionandclassificationare two of the more popular analyses under supervised learning. Regression analysis is used to discover and predict relationships between outcome variables and one or more independent variables. Commonly known aslinear regression, this method provides training data to help systems with predicting and forecasting. Classification is used to train systems to identify an object and place it in a sub-category. For instance, email filters use machine learning to automate incoming email flows for primary, promotion and spam inboxes.",
        "Unsupervised learningcontains data only containing inputs and then adds structure to the data in the form ofclusteringor grouping. The method learns from previous test data that hasn’t been labeled or categorized and will then group the raw data based on commonalities.Cluster analysisuses unsupervised learning to sort through giant lakes of raw data and group certain data points together. Clustering is a popular tool fordata mining, and it is used in everything from genetic research to creating virtual social media communities of like-minded individuals.",
        "Semi-supervised learning falls in betweenunsupervised and supervised learning. With this technique, programs are fed a mixture of labeled and unlabeled data that not only speeds up the machine learning process, but helps machines identify objects and learn with increased accuracy.",
        "Typically, programmers introduce a small amount oflabeled datawith a large percentage of unlabeled information, and the computer will have to use the groups of structured data to cluster the rest of the information. Labeling supervised data is seen as a massive undertaking because of high costs and hundreds of hours spent.",
        ""
    ],
    "Machine Learning Examples and Applications": [
        "The financial services industry ischampioning machine learningfor its unique ability to speed up processes with a high rate of accuracy. What has taken humans hours, days or even weeks to accomplish can now be executed in minutes.Over $11.5 trillion in card transactionswere processed in 2023 by card brands like American Express. Ensuring these transactions are more secure, American Express hasembraced machine learningtodetect fraudand other digital threats.",
        "Additionally, machine learning is used by lending and credit card companies to manage and predict risk. These computer programs take into account a loan seeker’s past credit history, along with thousands of other data points like cell phone and rent payments, to deem the risk of the lending company. By taking other data points into account, lenders can offer loans to a much wider array of individuals who couldn’t get loans with traditional methods.",
        "Trading firmsare using machine learning to amass a huge lake of data and determine the optimal price points toexecute trades. These complex high-frequencytrading algorithmstake thousands, if not millions, of financial data points into account to buy and sell shares at the right moment.",
        "The healthcare industryuses machine learningto manage medical information, discover new treatments and even detect and predict disease. With machine learning computer systems, medical professionals can easily view patient medical records without having to dig through files or have chains of communication with other areas of the hospital. Updated medical systems can now pull up pertinent health information on each patient in the blink of an eye.",
        "AI and machine learning can automate maintaining health records, following up with patients and authorizing insurance — tasks that unnecessarilytake up vital healthcare resourcesfor 68 percent of physicians, according to the American Medical Association.",
        "Machine learning-enabled AI toolsare working alongside drug developers to generate drug treatments at faster rates. Essentially, these machine learning tools are fed millions of data points and configure them to help researchers view which compounds are successful and which aren’t. Instead of spending millions of human hours on each trial, machine learning technologies can produce successful drug compounds in weeks or months.",
        "Radiology and pathologydepartments all over the world use machine learning to analyze CT and X-ray scans and find diseases. After being fed thousands of images of diseases through a mixture of supervised, unsupervised or semi-supervised models, advanced machine learning systems candiagnose diseasesat higher rates than humans. Machine learning has also been used to predict deadly viruses, likeEbolaandMalaria, and is used by the CDC totrack instances of the flu virusevery year.",
        "Machine learning is employed by social media companies to create a sense of community and weed outbad actorsand malicious information. Machine learning fosters the former by looking at pages, tweets, topics and other features that an individual likes and suggesting other topics or community pages based on those likes. It’s essentially using your preferences as a way to power a social mediarecommendation engine.",
        "Thespread of misinformationin politics has prompted social media companies to use machine learning to quickly identify harmful patterns of false information, flag malicious bots, view reported content and delete when necessary.",
        "Theretail industry relies on machine learningfor its ability to optimize sales and gather data on individualized shopping preferences. Machine learning offers retailers andonline storesthe ability tomake purchase suggestionsbased on a user’s clicks, likes and past purchases. Once customers feel like retailers understand their needs, they are more likely to stay with that company and purchase more items.",
        "Visual search is becoming a huge part of the shopping experience, too. Instead of typing in queries, customers can nowupload an imageto show the computer exactly what they’re looking for. Machine learning will analyze the image and produce search results based on its findings.",
        "Machine learning has also been an asset inpredicting customer trendsand behaviors. These machines look holistically at individual purchases to determine what types of items are selling and what items will be selling in the future. For example, maybe a new food has been deemed a “super food.” A grocery store’s systems might identify increased purchases of that product and send customers coupons ortargeted advertisementsfor all variations of that item. Additionally, a system could look at individual purchases to send you future coupons.",
        ""
    ],
    "What Is Deep Learning?": [
        "Deep learningis a subfield within machine learning, and it’s gaining traction for its ability to extract features from data. Deep learning uses artificialneural networks(ANNs) to extract higher-level features from raw data. ANNs, though much different fromhuman brains, wereinspired bythe way humans biologically process information. The learning a computer does is considered “deep” because the networks use layering to learn from and interpret raw information.",
        "For example,deep learning is an important assetfor image processing in everything from e-commerce to medical imagery. Google is equipping its programs with deep learning to discover patterns in images to display the correct image for whatever you search. If you search for a winter jacket, Google’s machine and deep learning will team up to discover patterns in images — sizes, colors, shapes, relevant brand titles — that display relevant jackets.",
        "Deep learning is also making headwinds in radiology, pathology and any medical sector that relies heavily on imagery. The technology relies on its tacit knowledge — from studying millions of other scans — to immediately recognize disease or injury, saving doctors and hospitals bothtime and money.",
        ""
    ],
    "History of Machine Learning": [
        "The brief timeline below tracks the development of machine learning from its beginnings in the 1950s to its maturation during the twenty-first century.",
        "Alan Turing jumpstarts the debate around whether computers possess artificial intelligence in what is known today as theTuring Test. The test consists of three terminals — a computer-operated one and two human-operated ones. The goal is for the computer totrick a human interviewerinto thinking it is also human by mimicking human responses to questions.",
        "Arthur Samuel publicly reveals a computer that can determine the optimal moves to make in a checker match.",
        "Frank Rosenblatt creates the first neural network for computers, known as the perceptron. This invention enables computers to reproduce human ways of thinking, forming original ideas on their own.",
        "Samuel builds on previous versions of his checkers program, leading to an advanced system made for the IBM 7094 computer. In 1962, the computer defeats checkers master Robert Nealy in a match.",
        "Descending from a line of robots designed for lunar missions, the Stanford cart emerges in an autonomous format in 1979. The machine relies on3D visionand pauses after each meter of movement to process its surroundings. Without any human help, this robot successfully navigates a chair-filled room to cover 20 meters in five hours.",
        "Gerald Dejong explores the concept of explanation-based learning (EBL). This approach involves providing a computer withtraining data, which it analyzes to develop a rule for filtering out unnecessary information. The idea is that this data is to a computer what prior experience is to a human being.",
        "Researcher Terry Sejnowksi creates an artificial neural network of 300 neurons and 18,000 synapses. Called NetTalk, the program babbles like a baby when receiving a list of English words, but can more clearly pronounce thousands of words with long-term training.",
        "The 1990s marks a shift in the realm of machine learning. Scientists focus less on knowledge and more on data, building computers that can glean insights from larger data sets.",
        "Scientists at IBM develop a computer called Deep Blue that excels at makingchess calculations. The program defeats world chess champion Garry Kasparov over a six-match showdown.",
        "The term “deep learning” is coined by Geoffrey Hinton, a long-time computer scientist and researcher in the field of AI. He applies the term to the algorithms that enable computers to recognize specific objects whenanalyzing textand images.",
        "Microsoft releases a motion-sensing device called Kinect for the Xbox 360. The device contains cameras and sensors that allow it to recognize faces, voices and movements. As a result, Kinect removes the need for physical controllers since playersbecomethe controllers.",
        "IBM’s Watson competes onJeopardy!against two of the show’s most decorated champions. That same year, Google develops Google Brain, which earns a reputation for the categorization capabilities of its deep neural networks.",
        "Computer scientists at Google’s X lab design anartificial brainfeaturing a neural network of 16,000 computer processors. The network applies a machine learning algorithm to scan YouTube videos on its own, picking out the ones that contain content related to cats.",
        "Facebook unveils its newface recognition toolDeepFace. Composed of a deep network of millions of data points, DeepFace leverages 3D face modeling to recognize faces in images in a way very similar to that of humans.",
        "Amazon develops a machine learning platform while Microsoft releases its Distributed Machine Learning Toolkit. In response to the proliferation of machine learning and AI, over 3,000 AI and robotics researchers — including names like Stephen Hawking and Elon Musk — sign an open letter warning ofAI-powered warfare.",
        "Google’s AI algorithmAlphaGospecializes in the complex Chinese board game Go. The algorithm achieves a close victory against the game’s top player Ke Jie in 2017. This win comes a year after AlphaGo defeated grandmaster Lee Sedol, taking four out of the five games.",
        "OpenAI releases GPT-1 and Google releases BERT, twolanguage modelsbased ontransformer networks. These models signal the rise of language models, paving the way forlarge language modelsto take on increased importance in machine learning research.",
        "Microsoft introduces the Turing Natural Language Generation model, which contains 17 billion parameters. Google also releases a family ofconvolutional neural networkscalled EfficientNets, which perform just as well as larger models while remaining compact.",
        "OpenAI releases GPT-3, which contains 175 billion parameters. Combiningnatural language processingand machine learning, GPT-3 displays far more advanced abilities than GPT-2 in understanding human language andgenerating human-like text.",
        "OpenAI launches DALL-E, a multimodal tool that can produce images based on text prompts. However, it doesn’t become popular until the 2022 release ofDALL-E 2.",
        "Google DeepMind reveals AlphaTensor, a system meant to speed up the process of building optimal algorithms that can handle complex tasks. In addition, OpenAI releasesChatGPT, achatbotthat offers a more conversational form ofgenerative AIfor users.",
        "OpenAI releasesGPT-4and Anthropic releasesClaude AI, both of which can process and produce different data like text, images and audio. These tools lead efforts to makemultimodal AImore accessible and commonplace.",
        "Google releases a family of multimodal models calledGemini, along with a chatbot by the same name, which was formerly known as Bard. These models come in various sizes and with different capabilities, and are being incorporated into several Google products, including Gmail, Docs and itssearch engine."
    ],
    "Frequently Asked Questions": [],
    "What Is Regression in Machine Learning?": [],
    "Evaluating a Machine Learning Regression Algorithm": [
        "Let’s say you’ve developed an algorithm which predicts next week’s temperature. The temperature to be predicted depends on different properties such as humidity, atmospheric pressure, air temperature and wind speed. But how accurate are your predictions? How good is your algorithm?",
        "To evaluate your predictions, there are two important metrics to be considered:variance and bias.",
        "Variance is the amount by which the estimate of the target function changes if different trainingdatawere used. The target function $f$ establishes the relation between the input (properties) and the output variables (predicted temperature). When a different dataset is used the target function needs to remain stable with little variance because, for any given type of data, the model should be generic. In this case, the predicted temperature changes based on the variations in the training dataset. To avoid false predictions, we need to make sure the variance is low. For that reason, the model should be generalized to accept unseen features of temperature data and produce better predictions.",
        "Bias is the algorithm’s tendency to consistently learn the wrong thing by not taking into account all the information in the data. For the model to be accurate, bias needs to be low. If there are inconsistencies in the dataset like missing values, less number of data tuples or errors in the input data, the bias will be high and the predicted temperature will be wrong.",
        "Accuracy and error are the two other important metrics. The error is the difference between the actual value and the predicted value estimated by the model. Accuracy is the fraction of predictions our model got right.",
        "For a model to be ideal, it’s expected to have low variance, low bias and low error. To achieve this, we need to partition the dataset into train and test datasets. The model will then learn patterns from the training dataset and the performance will be evaluated on the test dataset. To reduce the error while the model is learning, we come up with an error function which will be reviewed in the following section. If the model memorizes/mimics the training data fed to it, rather than finding patterns, it will give false predictions on unseen data. The curve derived from the trained model would then pass through all the data points and the accuracy on the test dataset is low. This is calledoverfittingand is caused by high variance.",
        "On the flip side, if the model performs well on the test data but with low accuracy on the training data, then this leads to underfitting.",
        "There are various algorithms that are used to build a regression model, some work well under certain constraints and some don’t. Before diving into the regression algorithms, let’s see how it works.",
        ""
    ],
    "The Bias-Variance Trade-off": [],
    "Linear Regression in Machine Learning": [
        "Linear regressionfinds the linear relationship between the dependent variable and one or more independent variables using a best-fit straight line. Generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term). In this technique, the dependent variable is continuous, the independent variable(s) can be continuous or discrete, and the nature of the regression line is linear. Mathematically, the prediction using linear regression is given as:",
        "$$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + … + \\theta_nx_n$$",
        "Here, $y$ is the predicted value,",
        "$n$ is the total number of input features,",
        "$x_i$ is the input feature for $i^{th}$ value,",
        "$\\theta_i$ is the model parameter ($\\theta_0$ is the bias and the coefficients are $\\theta_1, \\theta_2, … \\theta_n$).",
        "The coefficient is like a volume knob, it varies according to the corresponding input attribute, which brings change in the final value. It signifies the contribution of the input variables in determining the best-fit line.",
        "Bias is a deviation induced to the line equation $y = mx$ for the predictions we make. We need to tune the bias to vary the position of the line that can fit best for the given data.",
        "Now, let’s see how linear regression adjusts the line between the data for accurate predictions.",
        "Imagine, you’re given a set of data and your goal is to draw the best-fit line which passes through the data. This is the step-by-step process you proceed with:",
        "In accordance with the number of input and output variables, linear regression is divided into three types: simple linear regression, multiple linear regression and multivariate linear regression.",
        "First, calculate the error/loss by subtracting the actual value from the predicted one. Since the predicted values can be on either side of the line, we square the difference to make it a positive value. The result is denoted by ‘Q’, which is known as the sum of squared errors.",
        "Mathematically:",
        "$$Q =\\sum_{i=1}^{n}(y_{predicted}-y_{original} )^2$$",
        "Our goal is to minimize the error function ‘Q.\" To get to that, we differentiate Q w.r.t ‘m’ and ‘c’ and equate it to zero. After a few mathematical derivations  ‘m’ will be",
        "$$m = \\frac{cov(x,y)}{var(x)}$$",
        "And ‘c’ will be,",
        "$$c = y^{-} - bx^{-}$$",
        "By plugging the above values into the linear equation, we get the best-fit line.",
        "Gradient descent is an optimization technique used to tune the coefficient and bias of a linear equation.",
        "Imagine you are on the top left of a u-shaped cliff and moving blind-folded towards the bottom center. You take small steps in the direction of the steepest slope. This is what gradient descent does — it is the derivative or the tangential line to a function that attempts to find local minima of a function.",
        "How does gradient descent help in minimizing thecost function?",
        "We take steps down the cost function in the direction of the steepest descent until we reach the minima, which in this case is the downhill. The size of each step is determined by the parameter $\\alpha$, called the learning rate. If it’s too big, the model might miss the local minimum of the function, and if it's too small, the model will take a long time to converge. Hence, $\\alpha$ provides the basis for finding the local minimum, which helps in finding the minimized cost function.",
        "‘Q’ the cost functionis differentiated w.r.t the parameters, $m$ and $c$ to arrive at the updated $m$ and $c$, respectively. The product of the differentiated value and learning rate is subtracted from the actual ones to minimize the parameters affecting the model.",
        "Mathematically, this is how parameters are updated using the gradient descent algorithm:",
        "$$m = m - \\alpha\\frac{d}{dm}Q$$",
        "$$c = c - \\alpha\\frac{d}{dc}Q$$",
        "where $Q =\\sum_{i=1}^{n}(y_{predicted}-y_{original} )^2$.",
        "This continues until the error is minimized.",
        ""
    ],
    "Simple Linear Regression in Machine Learning": [
        "Simple linear regression is one of the simplest (hence the name) yet powerful regression techniques. It has one input ($x$) and one output variable ($y$) and helps us predict the output from trained samples by fitting a straight line between those variables. For example, we can predict the grade of a student based upon the number of hours they study using simple linear regression.",
        "Mathematically, this is represented by the equation:",
        "$$y = mx +c$$",
        "where $x$ is the independent variable (input),",
        "$y$ is the dependent variable (output),",
        "$m$ is slope,",
        "and $c$ is an intercept.",
        "The above mathematical representation is called a linear equation.",
        "Example:Consider a linear equation with two variables, 3x + 2y = 0.",
        "The values which when substituted make the equation right, are the solutions. For the above equation, (-2, 3)  is one solution because when we replace x with -2 and y with +3 the equation holds true and we get 0.",
        "$$3 * -2 + 2 * 3 = 0$$",
        "A linear equation is always a straight line when plotted on a graph.",
        "In simple linear regression, we assume the slope and intercept to be coefficient and bias, respectively. These act as the parameters that influence the position of the line to be plotted between the data.",
        "Imagine you plotted the data points in various colors, below is the image that shows the best-fit line drawn using linear regression.",
        ""
    ],
    "Multiple Linear Regression in Machine Learning": [
        "Multiple linear regressionis similar to simple linear regression, but there is more than one independent variable. Every value of the independent variable x is associated with a value of the dependent variable y. As it’s a multi-dimensional representation, the best-fit line is a plane.",
        "Mathematically, it’s expressed by:",
        "$$y  = b_0 + b_1x_1 + b_2x_2  + b_3x_3$$",
        "Imagine you need to predict if a student will pass or fail an exam. We’d consider multiple inputs like the number of hours they spent studying, total number of subjects and hours they slept for the previous night. Since we have multiple inputs we would use multiple linear regression.",
        ""
    ],
    "Multivariate Linear Regression in Machine Learning": [
        "As the name implies, multivariate linear regression deals with multiple output variables. For example, if a doctor needs to assess a patient’s health using collected blood samples, the diagnosis includes predicting more than one value, like blood pressure, sugar level and cholesterol level.",
        "Related Reading:What is Linear Algebra? Basic Linear Algebra for Deep Learning",
        ""
    ],
    "Linear Regression in Python Code": [
        "Output:",
        ""
    ],
    "Polynomial Regression in Machine Learning": [
        "While the linear regression model is able to understand patterns for a given dataset by fitting in a simple linear equation, it might not might not be accurate when dealing with complex data. In those instances we need to come up with curves which adjust with the data rather than the lines. One approach is to use apolynomial regressionmodel. Here, the degree of the equation we derive from the model is greater than one. Mathematically, a polynomial model is expressed by:",
        "$$Y_{0} = b_{0}+ b_{1}x^{1} + … b_{n}x^{n}$$",
        "where $Y_{0}$ is the predicted value for the polynomial model with regression coefficients $b_{1}$ to $b_{n}$ for each degree and a bias of $b_{0}$.",
        "If n=1, the polynomial equation is said to be a linear equation.",
        "Using polynomial regression, we see how the curved lines fit flexibly between the data, but sometimes even these result in false predictions as they fail to interpret the input. For example, if your model is a fifth-degree polynomial equation that’s trying to fit data points derived from a quadratic equation, it will try to update all six coefficients (five coefficients and one bias), which lead to overfitting. Usingregularization, we improve the fit so the accuracy is better on the test dataset.",
        ""
    ],
    "Ridge and Lasso Regression in Machine Learning": [
        "To avoid overfitting, we useridge and lasso regressionin the presence of a large number of features. These are the regularization techniques used in the regression field. They work by penalizing the magnitude of coefficients of features along with minimizing the error between the predicted and actual observations. Coefficients evidently increase to fit with a complex model which might lead to overfitting, so when penalized, it puts a check on them to avoid such scenarios.",
        "Ridge regression/L2 regularization adds a penalty term ($\\lambda{w_{i}^2}$) to the cost function which avoids overfitting, hence our cost function is now expressed,",
        "$$ J(w) = \\frac{1}{n}(\\sum_{i=1}^n (\\hat{y}(i)-y(i))^2 + \\lambda{w_{i}^2})$$",
        "When lambda = 0, we get back to overfitting, and lambda = infinity adds too much weight and leads to underfitting. Therefore, $\\lambda$ needs to be chosen carefully to avoid both of these.",
        "In lassoregression/L1 regularization, an absolute value ($\\lambda{w_{i}}$) is added rather than a squared coefficient.  It stands for least selective shrinkage selective operator.",
        "The cost function would then be:",
        "$$ J(w) = \\frac{1}{n}(\\sum_{i=1}^n (\\hat{y}(i)-y(i))^2 + \\lambda{w_{i}})$$",
        "Read Next:10 Python Cheat Sheets Every Developer Should Know",
        ""
    ],
    "Summary of Machine Learning Regression": [],
    "What Are the Types of Supervised Learning?": [
        "Supervised learning can be completed through the use ofalgorithmslike naive Bayes anddecision trees, or tasks such asregressionandclassification. The use of various algorithms determine the types of supervised learning and the tasks that supervised learning is capable of completing.",
        "Back to Basics With Built In ExpertsArtificial Intelligence vs. Machine Learning vs. Deep Learning"
    ],
    "Types of Supervised Learning": [],
    "What Is the Difference Between Supervised and Unsupervised Learning": [
        "The biggest difference between supervised andunsupervised learningis the use of labeled data sets.",
        "Supervised learning is the act of training the data set to learn by making iterative predictions based on the data while adjusting itself to produce the correct outputs. By providing labeled data sets, the model already knows the answer it is trying to predict but doesn’t adjust the process until it produces an independent output.",
        "The difference between the prediction and the correct answer is the key to the models producing accurate predictions when using new data. Like most varieties of machine learning, supervised learning is typically used to predict outcomes from data. It is calculated throughPythonorRand can be time-consuming to train.",
        "Unsupervised learningdoes not make use of labeled data sets, meaning the models work on their own to uncover the inherent structure of the unlabeled data. Human intervention is still required to validate the output variables depending on the intended use of the data and if it makes sense for the data to be utilized.",
        "Unsupervised learning is typically used to uncover insights from massive volumes of data, detect anomalies or make recommendations and may have inaccurate results without human validation.",
        ""
    ],
    "What Is an Example of Supervised Learning?": [
        "Supervised learning can be used to make accurate predictions using data, such as predicting a new home’s price.",
        "In order for predictions to be made, input data must be gathered. To determine a new home’s price, for example, we need to know factors like location, square footage, outdoor space, number of floors, number of rooms and more. The home’s price represents the output, or label, while the factors like location, square footage and outdoor space represent the input data for the algorithm.",
        "Once the corresponding labels are set, data from thousands of other homes can be gathered and compared against the existing input data. By weighing the features against the prices of these other homes, the model can do the same for the home in question to determine an accurate price."
    ]
}