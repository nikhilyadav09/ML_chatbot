"Pearson is the world’s learning company, with presence across 70 countries worldwide. Our unique insights and world-class expertise comes from a long history of working closely with renowned teachers, authors and thought leaders, as a result of which, we have emerged as the preferred choice for millions of teachers and learners across the world."
Superior learning experience and improved outcomes are at the heart of everything we do. This product is the result of one such effort.Your feedback plays a critical role in the evolution of our products and you can contact us at - reachus@pearson.com. We look forward to it.
"This book is dedicated to the people without whom the dream could not have come true – my parents, Tarun Kumar Dutt and Srilekha Dutt; constant inspiration and support from my wife, Adity and unconditional love of my sons Deepro and Devarko."
My sincerest thanks and appreciation go to several people… My parents Subramanian and Lalitha
"My academic collaborator, mentor and brother — Dr Saptarsi Goswami and Mr Mrityunjoy Panday"
"Comparison – supervised, unsupervised, and reinforcement learning"
Key drivers of feature selection – feature relevance and redundancy
Concept of Probability – Frequentist and Bayesian Interpretation
Handling Continuous Numeric Features in Naïve Bayes Classifier
Appendix C: A Case Study on Machine Learning Application: Grouping Similar Service Requests and Classifying a New One
"Repeated requests from Computer Science and IT engineering students who are the readers of our previous books encouraged us to write this book on machine learning. The concept of machine learning and the huge potential of its application is still niche knowledge and not so well-spread among the student community. So, we thought of writing this book specifically for techies, college students, and junior managers so that they understood the machine learning concepts easily."
"They should not only use the machine learning software packages, but understand the concepts behind those packages. The application of machine learning is getting boosted day by day. From recommending products to the buyers, to predicting the future real estate market, to helping medical practitioners in diagnosis, to optimizing energy consumption, thus helping the cause of Green Earth, machine learning is finding its utility in every sphere of life."
Due care was taken to write this book in simple English and to present the machine learning concepts in an easily understandable way which can be used as a textbook for both graduate and advanced undergraduate classes in machine learning or as a reference text.
Readers of this book will gain a thorough understanding of machine learning concepts. Not only students but also
"software professionals will find a variety of techniques with sufficient discussions in this book that cater to the needs of the professional environments. Technical managers will get an insight into weaving machine learning into the overall software engineering process. Students, developers, and technical managers with a basic background in computer science will find the material in this book easily readable."
"Each chapter starts with an introductory paragraph, which gives overview of the chapter along with a chapter-coverage table listing out the topics covered in the chapter. Sample questions at the end of the chapter helps the students to prepare for the examination. Discussion points and Points to Ponder given inside the chapters help to clarify and understand the chapters easily and also explore the thinking ability of the students and professionals."
"A recap summary is given at the end of each chapter for quick review of the topics. Throughout this book, you will see many exercises and discussion questions. Please don’t skip these – they are important in order to understand the machine learning concepts fully."
"This book starts with an introduction to Machine Learning which lays the theoretical foundation for the remaining chapters. Modelling, Feature engineering, and basic probability are discussed as chapters before entering into the world of machine learning which helps to grip the machine learning concepts easily at a later point of time."
Bonus topics of machine learning exercise with multiple examples are discussed in Machine learning language of R &Python. Appendix A discusses Programming Machine Learning in R and Appendix B discusses Programming Machine Learning in Python.
"We are grateful to Pearson Education, who came forward to publish this book. Ms. Neha Goomer and Mr. Purushothaman Chandrasekaran of Pearson Education were always kind and understanding. Mr. Purushothaman reviewed this book with abundant patience and helped us say what we had wanted to, improvising each and every page of this book with care. Their patience and guidance were invaluable. Thank you very much Neha and Puru."
"The journey through traditional Project Management, Agile Project Management, Program and Portfolio Management along with use of artificial intelligence and machine learning in the field, has been very rewarding, as it has given us the opportunity to work for some of the best organizations in the world and learn from some of the best minds. Along the way, several individuals have been instrumental in providing us with the guidance, opportunities and insights we needed to excel in this field. We wish to personally thank Mr. Rajesh Balaji Ramachandran, Senior Vice-president, Cognizant Technology Solutions and Mr. Pradeep Shilige, Executive Vice President, Cognizant Technology Solutions; Mr. Alexis Samuel, Senior Vice-president, Cognizant Technology Solutions and Mr.Hariharan Mathrubutham,Vice-president, Cognizant Technology Solutions and Mr. Krishna Prasad Yerramilli, Assistant Vice-president, Cognizant TechnologySolutions for their inspiration and help in creating this book. They have immensely contributed to improve our skills."
This book wouldn’t have been possible without the constant inspiration and support of my lovely wife Adity. My parents have always been enthusiastic about this project and provided me continuous guidance at every necessary step. The unconditional love and affection my sons – Deepro and Devarko ushered on me constantly provided me the motivation to work hard on this crucial project.
This book is the culmination of all the learning that I gained from many highly reputed professionals in the industry. I was fortunate to work with them and gained knowledge from them which helped me molding my professional career. Prof.
Indranil Bose and Prof. Bodhibrata Nag from IIM Calcutta guided me enormously in different aspects of life and career. My heartfelt thanks go to all the wonderful people who contributed in many ways in conceptualizing this book and wished success of this project.
"This book is the result of all the learning I have gained from many highly reputed professionals in the industry. I was fortunate to work with them and in the process, acquire knowledge that helped me in molding my professional career. I thank Mr. Chandra Sekaran, Group Chief Executive,Tech and Ops, Cognizant, for his continuous encouragement and unabated passion for strategic value creation, whose advice was invaluable for working on this book. I am obliged to Mr."
"Ms. Meena Karthikeyan, Vice-president, Cognizant Technology Solutions; Ms. Kalyani Sekhar, Assistant Vice- president, Cognizant Technology Solutions and Mr.Balasubramanian Narayanan, Senior Director, Cognizant Technology Solutions guided me enormously in different aspects of professional life."
"My parents (Mr. Subramanian and Ms. Lalitha) have always been enthusiastic about this project. Their unconditional love and affection provided the much-needed moral support. My son, Shri Krishna, and daughter, Shri Siva Ranjani, constantly added impetus to my motivation to work hard."
"This book would not have been possible without the constant inspiration and support of my wife, Ramya. She was unfailingly supportive and encouraging during the long months that I had spent glued to my laptop while writing this book. Last and not the least, I beg forgiveness of all those who have been with me over the course of the years and whose names I have failed to mention here."
"First of all, I would like to thank the Almighty for everything. It is the constant support and encouragement from my family which made it possible for me to put my heart and soul in my first authoring venture. My parents have always been my role model, my wife a source of constant strength and my"
"daughters are my happiness. Without my family, I wouldn’t have got the luxury of time to spend on hours in writing the book. Any amount of thanks will fell short to express my gratitude and pleasure of being part of the family."
"I would also thank the duo who have been my academic collaborator, mentor and brother - Dr. Saptarsi Goswami and Mr. Mrityunjoy Panday - without them I would be so incomplete. My deep respects for my research guides and mentors Amlan sir and Basabi madam for the knowledge and support that I’ve been privileged to receive from them."
"My sincere gratitude to my mentors in my past organization, Cognizant Technology Solutions, Mr. Rajarshi Chatterjee, Mr. Manoj Paul, Mr. Debapriya Dasgupta and Mr. Balaji Venkatesan for the valuable learning that I received from them. I would thank all my colleagues at Institute of Engineering & Management, who have given me relentless support and encouragement."
"Last, but not the least, my students who are always a source of inspiration for me need special mention. I’m especially indebted to Goutam Bose, Sayan Bachhar and Piyush Nandi for their extreme support in reviewing the chapters and providing invaluable feedback to make them more student- friendly. Also, thanks to Gulshan, Sagarika, Samyak, Sahil, Priya, Nibhash, Attri, Arunima, Salman, Deepayan, Arnab, Swapnil, Gitesh, Ranajit, Ankur, Sudipta and Debankan for their great support."
"Saikat is working on AI and Machine Learning projects especially focusing on application of those in managing software projects. He is a regular speaker on machine learning topics and involved in reviewing machine learning and AI related papers. Saikat is also a ‘Project Management Professional (PMP)’ and ‘PMI Agile Certified Professional’ certified by Project Management Institute (PMI) USA and a Certified Scrum Master (CSM). He has more than Nineteen years of IT industry experience and has expertise in managing large scale multi-location and mission critical projects. Saikat holds a B.E. degree from Jadavpur University, Kolkata and at present he is working as Director in Cognizant Technology"
"Solutions. He is a guest lecturer in IIMC since 2016 for Software Project Management course in PGDBA class. Saikat is also an active speaker on Agile, Project management, Machine learning and other recent topics several forums. He is actively working with IIMC to develop management case studies which are taught in global business schools."
"Author of books published by Pearson which has been recognized as a reference books in various universities, title includes ‘PMI Agile Certified Practitioner—Excel with Ease’, ‘Software Engineering’, ‘Software Project Management’. In addition to this, he has edited two books of foreign authors to suit the needs of Indian universities: ‘Applying UML and patterns’ by Craig Larman and ‘Design pattern by Gamma’ (Gang of four). He is certified ‘Green Belt’ in six sigmamethodology. He is a certified master practitioner in Neuro Linguistic Programming (NLP)."
industry in diverse roles and working with stakeholders across the globe.
"Amit has done his Bachelor in Engineering from Indian Institute of Engineering Science and Technology (IIEST), Shibpur and his Master in Technology from Birla Institute of Technology and Science (BITS), Pilani. Currently he is pursuing his research in the University of Calcutta. Amit’s area of research includes machine learning and deep learning. He has many published research papers in the area of data analytics and machine learning published in referred international journals and conferences. He has also been a regular speaker in the area of software engineering, data analytics and machine learning."
"Modelling and Evaluation: Selecting a model; training model – holdout, k-fold cross-validation, bootstrap sampling; model representation and interpretability – under-fitting, over-fitting, bias-variance tradeoff; model performance evaluation – classification, regression, clustering; performance improvement"
Supervised learning – Classification: Basics of supervised learning – classification; k-Nearestneighbour; decision tree; random forest; support vector machine
"The objective of this chapter is to venture into the arena of machine learning. New comers struggle a lot to understand the philosophy of machine learning. Also, they do not know where to start from and which problem could be and should be solved using machine learning tools and techniques. This chapter intends to give the new comers a starting point to the journey in machine learning. It starts from a historical journey in this field and takes it forward to give a glimpse of the modern day applications."
"It has been more than 20 years since a computer program defeated the reigning world champion in a game which is considered to need a lot of intelligence to play. The computer program was IBM’s Deep Blue and it defeated world chess champion, Gary Kasparov. That was the time, probably, when the most number of people gave serious attention to a fast evolving field in computer science or more specifically artificial intelligence – i.e. machine learning (ML)."
"‌As of today, machine learning is a mature technology area finding its application in almost every sphere of life. It can recommend toys to toddlers much in the same way as it can suggest a technology book to a geek or a rich title in literature to a writer. It predicts the future market to help amateur traders compete with seasoned stock traders. It helps an oncologist find whether a tumour is malignant or benign. It helps in optimizing energy consumption thus helping the cause of Green Earth. Google has become one of the front-runners focusing a lot of its research on machine learning and artificial intelligence – Google self-driving car and Google Brain being two most ambitious projects of Google in its journey of innovation in the field of machine learning. In a nutshell, machine learning has become a way of life, no matter whichever sphere of life we closely look at. But where did it all start from?"
"The foundation of machine learning started in the 18th and 19th centuries. The first related work dates back to 1763. In that year, Thomas Bayes’s work ‘An Essay towards solving a Problem in the Doctrine of Chances’ was published two years after his death. This is the work underlying Bayes Theorem, a fundamental work on which a number of algorithms of machine learning is based upon. In 1812, the Bayes theorem was actually formalized by the French mathematician Pierre- Simon Laplace. The method of least squares, which is the foundational concept to solve regression problems, was formalized in 1805. In 1913, Andrey Markov came up with the concept of Markov chains."
"However, the real start of focused work in the field of machine learning is considered to be Alan Turing’s seminal work in 1950. In his paper ‘Computing Machinery and Intelligence’ (Mind, New Series, Vol. 59, No. 236, Oct., 1950, pp. 433–460), Turing posed the question ‘Can machines think?’ or in other words, ‘Do machines have intelligence?’."
"He was the first to propose that machines can ‘learn’ and become artificially intelligent. In 1952, Arthur Samuel of IBM laboratory started working on machine learning programs, and first developed programs that could play Checkers. In 1957, Frank Rosenblatt designed the first neural network program simulating the human brain. From then on, for the next 50 years, the journey of machine learning has been fascinating. A number of machine learning algorithms were formulated by different researchers, e.g. the nearest neighbour algorithm in 1969, recurrent neural network in 1982, support vector machines and random forest algorithms in 1995. The latest feather in the cap of machine learning development has been Google’s AlphaGo program, which has beaten professional human Go player using machine learning techniques."
The evolution of machine learning from 1950 is depicted in Figure 1.1.
"The rapid development in the area of machine learning has triggered a question in everyone’s mind – can machines learn better than human? To find its answer, the first step would be to understand what learning is from a human perspective."
"Then, more light can be shed on what machine learning is. In the end, we need to know whether machine learning has already surpassed or has the potential to surpass human learning in every facet of life."
"In cognitive science, learning is typically referred to as the process of gaining information through observation. And why do we need to learn? In our daily life, we need to carry out multiple activities. It may be a task as simple as walking down the street or doing the homework. Or it may be some complex task like deciding the angle in which a rocket should be launched so that it can have a particular trajectory. To do a task in a proper way, we need to have prior information on one or more things related to the task. Also, as we keep learning more or in other words acquiring more information, the efficiency in doing the tasks keep improving. For example, with more knowledge, the ability to do homework with less number of mistakes increases. In the same way, information from past rocket launches helps in taking the right precautions and makes more successful rocket launch. Thus, with more learning, tasks can be performed more efficiently."
"Thinking intuitively, human learning happens in one of the three ways – (1) either somebody who is an expert in the subject directly teaches us, (2) we build our own notion indirectly based on what we have learnt from the expert in the past, or (3) we do it ourselves, may be after multiple attempts, some being unsuccessful. The first type of learning, we may call, falls under the category of learning directly under expert guidance, the second type falls under learning guided by knowledge gained from experts and the third type is learning by self or self-learning. Let’s look at each of these types deeply using real-life examples and try to understand what they mean."
"An infant may inculcate certain traits and characteristics, learning straight from its guardians. He calls his hand, a ‘hand’, because that is the information he gets from his parents. The sky is ‘blue’ to him because that is what his parents have taught him. We say that the baby ‘learns’ things from his parents."
"The next phase of life is when the baby starts going to school. In school, he starts with basic familiarization of alphabets and digits. Then the baby learns how to form words from the alphabets and numbers from the digits. Slowly more complex learning happens in the form of sentences, paragraphs, complex mathematics, science, etc. The baby is able to learn all these things from his teacher who already has knowledge on these areas."
"Then starts higher studies where the person learns about more complex, application-oriented skills. Engineering"
"students get skilled in one of the disciplines like civil, computer science, electrical, mechanical, etc. medical students learn about anatomy, physiology, pharmacology, etc. There are some experts, in general the teachers, in the respective field who have in-depth subject matter knowledge, who help the students in learning these skills."
"‌Then the person starts working as a professional in some field. Though he might have gone through enough theoretical learning in the respective field, he still needs to learn more about the hands-on application of the knowledge that he has acquired. The professional mentors, by virtue of the knowledge that they have gained through years of hands-on experience, help all new comers in the field to learn on-job."
"In all phases of life of a human being, there is an element of guided learning. This learning is imparted by someone, purely because of the fact that he/she has already gathered the knowledge by virtue of his/her experience in that field. So guided learning is the process of gaining information from a person having sufficient knowledge due to the past experience."
"An essential part of learning also happens with the knowledge which has been imparted by teacher or mentor at some point of time in some other form/context. For example, a baby can group together all objects of same colour even if his parents have not specifically taught him to do so. He is able to do so because at some point of time or other his parents have told him which colour is blue, which is red, which is green, etc. A grown-up kid can select one odd word from a set of words because it is a verb and other words being all nouns. He could do this because of his ability to label the words as verbs or"
"nouns, taught by his English teacher long back. In a professional role, a person is able to make out to which customers he should market a campaign from the knowledge about preference that was given by his boss long back.In all these situations, there is no direct learning. It is some past information shared on some different context, which is used as a learning to make decisions."
"In many situations, humans are left to learn on their own. A classic example is a baby learning to walk through obstacles. He bumps on to obstacles and falls down multiple times till he learns that whenever there is an obstacle, he needs to cross over it. He faces the same challenge while learning to ride a cycle as a kid or drive a car as an adult. Not all things are taught by others. A lot of things need to be learnt only from mistakes made in the past. We tend to form a check list on things that we should do, and things that we should not do, based on our experiences."
Before answering the question ‘What is machine learning?’ more fundamental questions that peep into one’s mind are
"‌At the onset, it is important to formalize the definition of machine learning. This will itself address the first question, i.e. if machines really learn. There are multiple ways to define machine learning. But the one which is perhaps most relevant,concise and accepted universally is the one stated by Tom M. Mitchell, Professor of Machine Learning Department, School of Computer Science, Carnegie Mellon University. Tom M."
"What this essentially means is that a machine can be considered to learn if it is able to gather experience by doing a certain task and improve its performance in doing the similar tasks in the future. When we talk about past experience, it means past data related to the task. This data is an input to the machine from some source."
"In the context of the learning to play checkers, E represents the experience of playing the game, T represents the task of playing checkers and P is the performance measure indicated by the percentage of games won by the player. The same mapping can be applied for any other machine learning problem, for example, image classification problem. In context of image classification, E represents the past data with images having labels or assigned classes (for example whether the image is of a class cat or a class dog or a class elephant etc.), T is the task of assigning class to new, unlabelled images and P is the performance measure indicated by the percentage of images correctly classified.The first step in any project is defining your problem. Even if the most powerful algorithm is used, the results will be meaningless if the wrong problem is solved."
The basic machine learning process can be divided into three parts.Figure 1.2 is a schematic representation of the machine learning process.
"‌Let’s put the things in perspective of the human learning process and try to understand the machine learning process more clearly. Reason is, in some sense, machine learning process tries to emulate the process in which humans learn to a large extent."
"Let’s consider the situation of typical process of learning from classroom and books and preparing for the examination. It is a tendency of many students to try and memorize (we often call it ‘learn by heart’) as many things as possible. This may work well when the scope of learning is not so vast. Also, the kinds of questions which are asked in the examination are pretty much simple and straightforward. The questions can be answered by simply writing the same things which have been memorized. However, as the scope gets broader and the"
"questions asked in the examination gets more complex, the strategy of memorizing doesn’t work well. The number of topics may get too vast for a student to memorize. Also, the capability of memorizing varies from student to student."
"Together with that, since the questions get more complex, a direct reproduction of the things memorized may not help. The situation continues to get worse as the student graduates to higher classes."
"So, what we see in the case of human learning is that just by great memorizing and perfect recall, i.e. just based on knowledge input, students can do well in the examinations only till a certain stage. Beyond that, a better learning strategy needs to be adopted:"
to be able to deal with the vastness of the subject matter and the related issues in memorizing it
to be able to answer questions where a direct answer has not been learnt
"A good option is to figure out the key points or ideas amongst a vast pool of knowledge. This helps in creating an outline of topics and a conceptual mapping of those outlined topics with the entire knowledge pool. For example, a broad pool of knowledge may consist of all living animals and their characteristics such as whether they live in land or water, whether they lay eggs, whether they have scales or fur or none, etc. It is a difficult task for any student to memorize the characteristics of all living animals – no matter how much photographic memory he/she may possess. It is better to draw a notion about the basic groups that all living animals belong to and the characteristics which define each of the basic groups. The basic groups of animals are invertebrates and vertebrates. Vertebrates are further grouped as mammals, reptiles, amphibians, fishes, and birds. Here, we have mapped animal groups and their salient characteristics."
Amphibians: Semi-aquatic i.e. may live in water or land; smooth skin; lay eggs
Reptiles: Semi-aquatic like amphibians; scaly skin; lay eggs; cold-blooded
Mammals: Have hair or fur; have milk to feed their young; warm-blooded
This makes it easier to memorize as the scope now reduces to know the animal groups that the animals belong to. Rest of the answers about the characteristics of the animals may be derived from the concept of mapping animal groups and their characteristics.
"‌Moving to the machine learning paradigm, the vast pool of knowledge is available from the data input. However, rather than using it in entirety, a concept map, much in line with the animal group to characteristic mapping explained above, is drawn from the input data. This is nothing but knowledge abstraction as performed by the machine. In the end, the abstracted mapping from the input data can be applied to make critical conclusions. For example, if the group of an animal is given, understanding of the characteristics can be automatically made. Reversely, if the characteristic of an unknown animal is given, a definite conclusion can be made about the animal group it belongs to. This is generalization in context of machine learning."
"During the machine learning process, knowledge is fed in the form of input data. However, the data cannot be used in the original shape and form. As we saw in the example above, abstraction helps in deriving a conceptual map based on the"
"input data. This map, or a model as it is known in the machine learning paradigm, is summarized knowledge representation of the raw data. The model may be in any one of the following forms"
"Otherwise, the equation (or the model) is of no use. So, fitting the model, in this case, means finding the values of the unknown coefficients or constants of the equation or the model. This process of fitting the model based on the input data is known as training. Also, the input data based on which the model is being finalized is known as training data."
"The trained model is aligned with the training data too much, hence may not portray the actual trend.The test data possess certain characteristics apparently unknown to the training data."
"Hence, a precise approach of decision-making will not work. An approximate or heuristic approach, much like gut- feeling-based decision-making in human beings, has to be adopted. This approach has the risk of not making a correct decision – quite obviously because certain assumptions that are made may not be true in reality. But just like machines, same mistakes can be made by humans too when a decision is made based on intuition or gut-feeling – in a situation where exact reason-based decision-making is not possible."
"For defining a new problem, which can be solved using machine learning, a simple framework, highlighted below, can be used. This framework also helps in deciding whether the problem is a right candidate to be solved using machine learning. The framework involves answering three questions:"
"A number of information should be collected to know what is the problem.Use Tom Mitchell’s machine learning formalism stated above to define the T, P, and E for the problem."
What other problems have you seen or can you think of that are similar to the problem that you are trying to solve?
What is the motivation for solving the problem? What requirement will it fulfil?
"For example, does this problem solve any long-standing business issue like finding out potentially fraudulent transactions? Or the purpose is more trivial like trying to suggest some movies for upcoming weekend."
Consider the benefits of solving the problem. What capabilities does it enable?It is important to clearly understand the benefits of solving the problem. These benefits can be articulated to sell the project.
How will the solution to the problem be used and the life time of the solution is expected to have?Try to explore how to solve the problem manually.
"Detail out step-by-step data collection, data preparation, and program design to solve the problem. Collect all these details and update the previous sections of the problem definition, especially the assumptions.solution will be used."
"Sony created a series of robotic pets called Aibo. It was built in 1998. Although most models sold were dog-like, other inspirations included lion-cubs. It could express emotions and could also recognize its owner. In 2006, Aibo was added to the Carnegie Mellon University’s ‘Robot Hall of Fame’. A new generation of Aibo was launched in Japan in January 2018."
Unsupervised learning – Also called descriptive learning. A machine finds patterns in unknown objects by grouping similar objects together.Reinforcement learning – A machine learns to act on its own to achieve the given goals.
"Many video games are based on artificial intelligence technique called Expert System. This technique can imitate areas of human behaviour, with a goal to mimic the human ability of senses, perception, and reasoning."
"The major motivation of supervised learning is to learn from past information. So what kind of past information does the machine need for supervised learning? It is the information about the task which the machine has to execute. In context of the definition of machine learning, this past information is the experience. Let’s try to understand it with an example."
"‌Say a machine is getting images of different objects as input and the task is to segregate the images by either shape or colour of the object. If it is by shape, the images which are of round-shaped objects need to be separated from images of"
"triangular-shaped objects, etc. If the segregation needs to happen based on colour, images of blue objects need to be separated from images of green objects. But how can the machine know what is round shape, or triangular shape? Same way, how can the machine distinguish image of an object based on whether it is blue or green in colour? A machine is very much like a little child whose parents or adults need to guide him with the basic information on shape and colour before he can start doing the task. A machine needs the basic information to be provided to it. This basic input, or the experience in the paradigm of machine learning, is given in the form of training data . Training data is the past information on a specific task. In context of the image segregation problem, training data will have past data on different aspects or features on a number of images, along with a tag on whether the image is round or triangular, or blue or green in colour. The tag is called ‘ label’ and we say that the training data is labelled in case of supervised learning."
"Figure 1.4 is a simple depiction of the supervised learning process. Labelled training data containing past information comes as an input. Based on the training data, the machine builds a predictive model that can be used on test data to assign a label for each record in the test data."
"Let’s try to understand these two areas of supervised learning, i.e. classification and regression in more details."
"Let’s discuss how to segregate the images of objects based on the shape . If the image is of a round object, it is put under one category, while if the image is of a triangular object, it is put under another category. In which category the machine should put an image of unknown category, also called a test data in machine learning parlance, depends on the information it gets from the past data, which we have called as training data."
"Since the training data has a label or category defined for each and every image, the machine has to map a new image or test data to a set of images to which it is similar to and assign the same label or category to the test data."
"So we observe that the whole problem revolves around assigning a label or category or class to a test data based on the label or category or class information that is imparted by the training data. Since the target objective is to assign a class label, this type of problem as classification problem. Figure1.5 depicts the typical process of classification."
"‌There are number of popular machine learning algorithms which help in solving classification problems. To name a few, Naïve Bayes, Decision tree, and k-Nearest Neighbour algorithms are adopted by many machine learning practitioners."
"A critical classification problem in context of banking domain is identifying potential fraudulent transactions. Since there are millions of transactions which have to be scrutinized and assured whether it might be a fraud transaction, it is not possible for any human being to carry out this task. Machine learning is effectively leveraged to do this task and this is a classic case of classification. Based on the past transaction data, specifically the ones labelled as fraudulent, all new incoming transactions are marked or labelled as normal orsuspicious. The suspicious transactions are subsequently segregated for a closer review."
"In summary, classification is a type of supervised learning where a target feature, which is of type categorical, is predicted for test data based on the information imparted by training data. The target categorical feature is known as class."
Image classification Prediction of disease Win–loss prediction of games
"Machine learning saves life – ML can spot 52% of breast cancer cells, a year before patients are diagnosed.US Postal Service uses machine learning for handwriting recognition. Facebook’s news feed uses machine learning to personalize each member’s feed."
"In linear regression, the objective is to predict numerical features like real estate or stock price, temperature, marks in an examination, sales revenue, etc. The underlying predictor variable and the target variable are continuous in nature. In case of linear regression, a straight line relationship is ‘fitted’ between the predictor variables and the target variables, using the statistical concept of least squares method. As in the case of least squares method, the sum of square of error between"
"actual and predicted values of the target variable is tried to be minimized. In case of simple linear regression, there is only one predictor variable whereas in case of multiple linear regression, multiple predictor variables can be included in the model."
"Let’s take the example of yearly budgeting exercise of the sales managers. They have to give sales prediction for the next year based on sales figure of previous years vis-à-vis investment being put in. Obviously, the data related to past as well as the data to be predicted are continuous in nature. In a basic approach, a simple linear regression model can be applied with investment as predictor variable and sales revenue as the target variable."
where ‘x’ is the predictor variable and ‘y’ is the target variable.
"The input data come from a famous multivariate data set named Iris introduced by the British statistician and biologist Ronald Fisher. The data set consists of 50 samples from each of three species of Iris – Iris setosa, Iris virginica, and Iris versicolor. Four features were measured for each sample – sepal length, sepal width, petal length, and petal width. These features can uniquely discriminate the different species of the flower."
"The Iris data set is typically used as a training data for solving the classification problem of predicting the flower species based on feature values. However, we can also demonstrate regression using this data set, by predicting the value of one feature using another feature as predictor. In Figure 1.6, petal length is a predictor variable which, when fitted in the simple linear regression model, helps in predicting the value of the target variable sepal length."
"‌Unlike supervised learning, in unsupervised learning, there is no labelled training data to learn from and no prediction to be"
"made. In unsupervised learning, the objective is to take a dataset as input and try to find natural groupings or patterns within the data elements or records. Therefore, unsupervised learning is often termed as descriptive model and the process of unsupervised learning is referred as pattern discovery or knowledge discovery. One critical application of unsupervised learning is customer segmentation."
"Clustering is the main type of unsupervised learning. It intends to group or organize similar objects together. For that reason, objects belonging to the same cluster are quite similar to each other while objects belonging to different clusters are quite dissimilar. Hence, the objective of clustering to discover the intrinsic grouping of unlabelled data and form clusters, as depicted in Figure 1.7. Different measures of similarity can be applied for clustering. One of the most commonly adopted similarity measure is distance. Two data items are considered as a part of the same cluster if the distance between them is less. In the same way, if the distance between the data items is high, the items do not generally belong to the same cluster.This is also known as distance-based clustering. Figure 1.8 depicts the process of clustering at a high level."
"Other than clustering of data and getting a summarized view from it, one more variant of unsupervised learning is association analysis. As a part of association analysis, the association between data elements is identified. Let’s try to understand the approach of association analysis in context of one of the most common examples, i.e. market basket analysis as shown in Figure 1.9. From past transaction data in a grocery store, it may be observed that most of the customers who have bought item A, have also bought item B and item C or at least one of them. This means that there is a strong association of the event ‘purchase of item A’ with the event ‘purchase of item B’, or ‘purchase of item C’. Identifying these sorts of associations is the goal of association analysis. This helps in boosting up sales pipeline, hence a critical input for the sales group. Critical applications of association analysis include market basket analysis and recommender systems."
‌We have seen babies learn to walk without any prior knowledge of how to do it. Often we wonder how they really
"‌First they notice somebody else walking around, for example parents or anyone living around. They understand that legs have to be used, one at a time, to take a step. While walking, sometimes they fall down hitting an obstacle, whereas other times they are able to walk smoothly avoiding bumpy obstacles. When they are able to walk overcoming the obstacle, their parents are elated and appreciate the baby with loud claps / or may be a chocolates. When they fall down while circumventing an obstacle, obviously their parents do not give claps or chocolates. Slowly a time comes when the babies learn from mistakes and are able to walk with much ease."
"In the same way, machines often learn to do tasks autonomously. Let’s try to understand in context of the example of the child learning to walk. The action tried to be achieved is walking, the child is the agent and the place with hurdles on which the child is trying to walk resembles the environment. It tries to improve its performance of doing the task. When a sub-task is accomplished successfully, a reward is given. When a sub-task is not executed correctly, obviously no reward is given. This continues till the machine is able to complete execution of the whole task. This process of learning is known as reinforcement learning. Figure 1.10 captures the high-level process of reinforcement learning."
"One contemporary example of reinforcement learning is self-driving cars. The critical information which it needs to take care of are speed and speed limit in different road segments, traffic conditions, road conditions, weather conditions, etc. The tasks that have to be taken care of are start/stop, accelerate/decelerate, turn to left / right, etc.Further details on reinforcement learning have been kept out of the scope of this book."
"‌While Deep Blue used brute force to defeat the human chess champion, AlphaGo used RL to defeat the best human Go player.RL is an effective tool for personalized online marketing. It considers the demographic details and browsing history of the user real-time to show most relevant advertisements."
"‌Comparison – supervised, unsupervised, and reinforcement learning‌"
"‌Machine learning should not be applied to tasks in which humans are very effective or frequent human intervention is needed. For example, air traffic control is a very complex task needing intense human involvement. At the same time, for very simple tasks which can be implemented using traditional programming paradigms, there is no sense of using machine learning. For example, simple rule-driven or formula-based applications like price calculator engine, dispute tracking application, etc. do not need machine learning techniques."
"Machine learning should be used only when the business process has some lapses. If the task is already optimized, incorporating machine learning will not serve to justify the return on investment."
"For situations where training data is not sufficient, machine learning cannot be used effectively. This is because, with small training data sets, the impact of bad data is exponentially worse. For the quality of prediction or recommendation to be good, the training data should be sizeable."
"Wherever there is a substantial amount of past data, machine learning can be used to generate actionable insight from the data. Though machine learning is adopted in multiple forms in every business domain, we have covered below three major domains just to give some idea about what type of actions can be done using machine learning."
"In the banking industry, fraudulent transactions, especially the ones related to credit cards, are extremely prevalent. Since the volumes as well as velocity of the transactions are extremely"
"high, high performance machine learning solutions are implemented by almost all leading banks across the globe. The models work on a real-time basis, i.e. the fraudulent transactions are spotted and prevented right at the time of occurrence. This helps in avoiding a lot of operational hassles in settling the disputes that customers will otherwise raise against those fraudulent transactions."
"Customers of a bank are often offered lucrative proposals by other competitor banks. Proposals like higher bank interest, lower processing charge of loans, zero balance savings accounts, no overdraft penalty, etc. are offered to customers, with the intent that the customer switches over to the competitor bank. Also, sometimes customers get demotivated by the poor quality of services of the banks and shift to competitor banks. Machine learning helps in preventing or at least reducing the customer churn. Both descriptive and predictive learning can be applied for reducing customer churn. Using descriptive learning, the specific pockets of problem, i.e. a specific bank or a specific zone or a specific type of offering like car loan, may be spotted where maximum churn is happening. Quite obviously, these are troubled areas where further investigation needs to be done to find and fix the root cause. Using predictive learning, the set of vulnerable customers who may leave the bank very soon, can be identified. Proper action can be taken to make sure that the customers stay back."
"‌Insurance industry is extremely data intensive. For that reason, machine learning is extensively used in the insurance industry. Two major areas in the insurance industry where machine learning is used are risk prediction during new customer"
"onboarding and claims management. During customer onboarding, based on the past information the risk profile of a new customer needs to be predicted. Based on the quantum of risk predicted, the quote is generated for the prospective customer. When a customer claim comes for settlement, past information related to historic claims along with the adjustor notes are considered to predict whether there is any possibility of the claim to be fraudulent. Other than the past information related to the specific customer, information related to similar customers, i.e. customer belonging to the same geographical location, age group, ethnic group, etc., are also considered to formulate the model."
"Wearable device data form a rich source for applying machine learning and predict the health conditions of the person real time. In case there is some health issue which is predicted by the learning model, immediately the person is alerted to take preventive action. In case of some extreme problem, doctors or healthcare providers in the vicinity of the person can be alerted. Suppose an elderly person goes for a morning walk in a park close to his house. Suddenly, while walking, his blood pressure shoots up beyond a certain limit, which is tracked by the wearable. The wearable data is sent to a remote server and a machine learning algorithm is constantly analyzing the streaming data. It also has the history of the elderly person and persons of similar age group. The model predicts some fatality unless immediate action is taken. Alert can be sent to the person to immediately stop walking and take rest. Also, doctors and healthcare providers can be alerted to be on standby."
"The algorithms related to different machine learning tasks are known to all and can be implemented using any language/platform. It can be implemented using a Java platform or C / C++ language or in .NET. However, there are certain languages and tools which have been developed with a focus for implementing machine learning. Few of them, which are most widely used, are covered below."
"Python is one of the most popular, open source programming language widely adopted by machine learning community. It was designed by Guido van Rossum and was first released in 1991. The reference implementation of Python, i.e. CPython, is managed by Python Software Foundation, which is a non- profit organization."
"‌Python has very strong libraries for advanced mathematical functionalities (NumPy), algorithms and mathematical tools (SciPy) and numerical plotting (matplotlib). Built on these libraries, there is a machine learning library named scikit- learn, which has various classification, regression, and clustering algorithms embedded in it."
"developed at Bell Laboratories. Currently, it is supported by the R Foundation for statistical computing.R is a very simple programming language with a huge set of libraries available for different stages of machine learning."
"Some of the libraries standing out in terms of popularity are plyr/dplyr (for data transformation), caret (‘Classification and Regression Training’ for classification), RJava (to facilitate integration with Java), tm (for text mining), ggplot2 (for data visualization). Other than the libraries, certain packages like Shiny and R Markdown have been developed around R to develop interactive web applications, documents and dashboards on R without much effort."
"MATLAB (matrix laboratory) is a licenced commercial software with a robust support for a wide range of numerical computing. MATLAB has a huge user base across industry and academia. MATLAB is developed by MathWorks, a company founded in 1984. Being proprietary software, MATLAB is developed much more professionally, tested rigorously, and has comprehensive documentation."
MATLAB also provides extensive support of statistical functions and has a huge number of machine learning algorithms in-built. It also has the ability to scale up for large datasets by parallel processing on clusters and cloud.
"SAS (earlier known as ‘Statistical Analysis System’) is another licenced commercial software which provides strongsupport for machine learning functionalities. Developed in C by SAS Institute, SAS had its first release in the year 1976."
"SAS is a software suite comprising different components. The basic data management functionalities are embedded in the Base SAS component whereas the other components like SAS/INSIGHT, Enterprise Miner, SAS/STAT, etc. help in specialized functions related to data mining and statistical analysis."
"‌There are a host of other languages and tools that also support machine learning functionalities. Owned by IBM, SPSS (originally named as Statistical Package for the Social Sciences) is a popular package supporting specialized data mining and statistical analysis. Originally popular for statistical analysis in social science (as the name reflects), SPSS is now popular in other fields as well."
"Released in 2012, Julia is an open source, liberal licence programming language for numerical analysis and computational science. It has baked in all good things of MATLAB, Python, R, and other programming languages used for machine learning for which it is gaining steady attention from machine learning development community. Another big point in favour of Julia is its ability to implement high- performance machine learning algorithms."
"Machine learning is a field which is relatively new and still evolving. Also, the level of research and kind of use of machine learning tools and technologies varies drastically from country to country. The laws and regulations, cultural"
"background, emotional maturity of people differ drastically in different countries. All these factors make the use of machine learning and the issues originating out of machine learning usage are quite different."
"The biggest fear and issue arising out of machine learning is related to privacy and the breach of it. The primary focus of learning is on analyzing data, both past and current, and coming up with insight from the data. This insight may be related to people and the facts revealed might be private enough to be kept confidential. Also, different people have a different preference when it comes to sharing of information. While some people may be open to sharing some level of information publicly, some other people may not want to share it even to all friends and keep it restricted just to family members. Classic examples are a birth date (not the day, but the date as a whole), photographs of a dinner date with family, educational background, etc. Some people share them with all in the social platforms like Facebook while others do not, or if they do, they may restrict it to friends only. When machine learning algorithms are implemented using those information, inadvertently people may get upset. For example, if there is a learning algorithm to do preference-based customer segmentation and the output of the analysis is used for sending targeted marketing campaigns, it will hurt the emotion of people and actually do more harm than good. In certain countries, such events may result in legal actions to be taken by the people affected."
"Even if there is no breach of privacy, there may be situations where actions were taken based on machine learning may create an adverse reaction. Let’s take the example of knowledge discovery exercise done before starting an election campaign. If a specific area reveals an ethnic majority orskewness of a certain demographic factor, and the campaign pitch carries a message keeping that in mind, it might actually upset the voters and cause an adverse result."
So a very critical consideration before applying machine learning is that proper human judgement should be exercised before using any outcome from machine learning. Only then the decision taken will be beneficial and also not result in any adverse impact.
MULTIPLE-CHOICE QUESTIONS (1 MARK EACH):Machine learning is field.
"All of the aboveA computer program is said to learn from E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with E."
Algorithmhas been used to train vehicles to steer correctly and autonomously on road.
Any hypothesis find an approximation of the target function over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples. This is called.
Factors which affect performance of a learner system does not include
A model of language consists of the categories which does not include
This type of learning to be used when there is no idea about the class or label of a particular data
The model learns and updates itself through reward/punishment in case of
SHORT-ANSWER TYPE QUESTIONS (5 MARKS EACH):What is human learning? Give any two examples.
What are the types of human learning? Are there equivalent forms of machine learning?
What is machine learning? What are key tasks of machine learning?Explain the concept of penalty and reward in reinforcement learning.
Define machine learning and explain the different elements with a real example.Explain the process of abstraction with an example.
What is regression? Give example of some practical problems solved using regression.Explain the process of clustering in details.
What is machine learning? Explain any two business applications of machine learning. What are the possible ethical issues of machine learning applications?
"What are the different types of unsupervised learning? Explain them difference with a sample application in each area.Explain, in details, the process of machine learning."
"This chapter gives a detailed view of how to understand the incoming data and create basic understanding about the nature and quality of the data. This information, in turn, helps to select and then how to apply the model. So, the knowledge imparted in this chapter helps a beginner take the first step towards effective modelling and solving a machine learning problem."
"In the last chapter, we got introduced to machine learning. In the beginning, we got a glimpse of the journey of machine learning as an evolving technology. It all started as a proposition from the renowned computer scientist Alan Turing – machines can ‘learn’ and become artificially intelligent."
"Gradually, through the next few decades path-breaking innovations came in from Arthur Samuel, Frank Rosenblatt, John Hopfield, Christopher Watkins, Geoffrey Hinton and many other computer scientists. They shaped up concepts of"
"‌Neural Networks, Recurrent Neural Network, Reinforcement Learning, Deep Learning, etc. which took machine learning to new heights. In parallel, interesting applications of machine learning kept on happening, with organizations like IBM and Google taking a lead. What started with IBM’s Deep Blue beating the world chess champion Gary Kasparov, continued with IBM’s Watson beating two human champions in a Jeopardy competition.Google also started with a series of innovations applying machine learning. The Google Brain, Sibyl, Waymo, AlphaGo programs – are all extremely advanced applications of machine learning which have taken the technology a few notches up. Now we can see an all- pervasive presence of machine learning technology in all walks of life.We have also seen the types of human learning and how that, in some ways, can be related to the types of machine learning – supervised, unsupervised, and reinforcement."
"Supervised learning, as we saw, implies learning from past data, also called training data, which has got known values or classes. Machines can ‘learn’ or get ‘trained’ from the past data and assign classes or values to unknown data, termed as test data. This helps in solving problems related to prediction. This is much like human learning through expert guidance as happens for infants from parents or students through teachers. So, supervised learning in case of machines can be perceived as guided learning from human inputs. Unsupervised machine learning doesn’t have labelled data to learn from. It tries to find patterns in unlabelled data. This is much like human beings trying to group together objects of similar shape. This learning is not guided by labelled inputs but uses the knowledge gained from the labels themselves. Last but not the least is reinforcement learning in which machine tries to learnby itself through penalty/ reward mechanism – again pretty much in the same way as human self-learning happens."
"Lastly, we saw some of the applications of machine learning in different domains such as banking and finance, insurance, and healthcare. Fraud detection is a critical business case which is implemented in almost all banks across the world and uses machine learning predominantly. Risk prediction for new customers is a similar critical case in the insurance industry which finds the application of machine learning. In the healthcare sector, disease prediction makes wide use of machine learning, especially in the developed countries."
"While development in machine learning technology has been extensive and its implementation has become widespread, to start as a practitioner, we need to gain some basic understanding. We need to understand how to apply the array of tools and technologies available in the machine learning to solve a problem. In fact, that is going to be very specific to the kind of problem that we are trying to solve. If it is a prediction problem, the kind of activities that will be involved is going to be completely different vis-à-vis if it is a problem where we are trying to unfold a pattern in a data without any past knowledge about the data. So how a machine learning project looks like or what are the salient activities that form the core of a machine learning project will depend on whether it is in the area of supervised or unsupervised or reinforcement learning area. However, irrespective of the variation, some foundational knowledge needs to be built before we start with the core machine learning concepts and key algorithms. In this section, we will have a quick look at a few typical machine learning activities and focus on some of the foundational concepts that all practitioners need to gain aspre-requisites before starting their journey in the area of machine learning."
"‌No man is perfect. The same is applicable for machines. To increase the level of accuracy of a machine, human participation should be added to the machine learning process. In short, incorporating human intervention is the recipe for the success of machine learning."
Find potential issues in data.Figure 2.1 depicts the four-step process of machine learning.
Table 2.1 contains a summary of steps and activities involved:
"In this chapter, we will cover the first part, i.e. preparing to model. The remaining parts, i.e. learning, performance evaluation, and performance improvement will be covered in Chapter 3."
"may be on some entity or some subject area. For example (Fig. 2.2), we may have a data set on students in which each record consists of information about a specific student. Again, we can have a data set on student performance which has records providing performance, i.e. marks on the individual subjects."
"Each row of a data set is called a record. Each data set also has multiple attributes, each of which gives information on a specific characteristic. For example, in the data set on students, there are four attributes namely Roll Number, Name, Gender, and Age, each of which understandably is a specific characteristic about the student entity. Attributes can also be termed as feature, variable, dimension or field. Both the data sets, Student and Student Performance, are having four features or dimensions; hence they are told to have four- dimensional data space. A row or record represents a point in the four-dimensional data space as each row has specific values for each of the four attributes or features. Value of an attribute, quite understandably, may vary from record to record. For example, if we refer to the first two records in the Student data set, the value of attributes Name, Gender, and Age are different (Fig. 2.3)."
"Now that a context of data sets is given, let’s try to understand the different types of data that we generally come across in machine learning problems. Data can broadly be divided into following two types:"
"example, if we consider the quality of performance of students in terms of ‘Good’, ‘Average’, and ‘Poor’, it falls under the category of qualitative data. Also, name or roll number of students are information that cannot be measured using some scale of measurement. So they would fall under qualitative data. Qualitative data is also called categorical data."
Qualitative data can be further subdivided into two types as follows:
"Blood group: A, B, O, AB, etc.Nationality: Indian, American, British, etc."
"Gender: Male, Female, OtherA special case of nominal data is when only two labels are possible, e.g. pass/fail as a result of an examination. This sub-type of nominal data is called ‘dichotomous’."
"Grades: A, B, C, etc.Hardness of Metal: ‘Very Hard’, ‘Hard’, ‘Soft’, etc."
"Like nominal data, basic counting is possible for ordinal data. Hence, the mode can be identified. Since ordering is possible in case of ordinal data, median, and quartiles can be identified in addition. Mean can still not be calculated."
"Ratio datathe case of difference between 15.5°C and 21.5°C. Other examples include date, time, etc."
"For interval data, mathematical operations such as addition and subtraction are possible. For that reason, for interval data, the central tendency can be measured by mean, median, or mode. Standard deviation can also be calculated."
"However, interval data do not have something called a ‘true zero’ value. For example, there is nothing called ‘0 temperature’ or ‘no temperature’. Hence, only addition and subtraction applies for interval data. The ratio cannot be applied. This means, we can say a temperature of 40°C is equal to the temperature of 20°C + temperature of 20°C. However, we cannot say the temperature of 40°C means it is twice as hot as in temperature of 20°C."
"Also, these variables can be added, subtracted, multiplied, or divided. The central tendency can be measured by mean, median, or mode and methods of dispersion such as standard deviation. Examples of ratio data include height, weight, age, salary, etc.Figure 2.4 gives a summarized view of different types of data that we may find in a typical machine learning problem."
"‌Apart from the approach detailed above, attributes can also be categorized into types based on a number of values that can be assigned. The attributes can be either discrete or continuous based on this factor."
"Discrete attributes can assume a finite or countably infinite number of values. Nominal attributes such as roll number, street number, pin code, etc. can have a finite number of values whereas numeric attributes such as count, rank of students, etc. can have countably infinite values. A special type of discrete attribute which can assume two values only is called binary attribute. Examples of binary attribute include male/ female, positive/negative, yes/no, etc.Continuous attributes can assume any possible value which is a real number. Examples of continuous attribute include length, height, weight, price, etc."
"By now, we understand that in machine learning, we come across two basic data types – numeric and categorical. With this context in mind, we can delve deeper into understanding a data set. We need to understand that in a data set, which of the attributes are numeric and which are categorical in nature. This is because, the approach of exploring numeric data is different than the approach of exploring categorical data. In case of a standard data set, we may have the data dictionary available for reference. Data dictionary is a metadata repository, i.e. the repository of all information related to the structure of each data element contained in the data set. The data dictionary gives detailed information on each of the attributes – the description as well as the data type and other relevant details."
"In case the data dictionary is not available, we need to use standard library function of the machine learning tool that we are using and get the details. For the time being, let us move ahead with a standard data set from UCI machine learning repository."
The data set that we take as a reference is the Auto MPG data set available in the UCI repository. Figure 2.5 is a snapshot of the first few rows of the data set.
"‌As is quite evident from the data, the attributes such as ‘mpg’, ‘cylinders’, ‘displacement’, ‘horsepower’, ‘weight’, ‘acceleration’, ‘model year’, and ‘origin’ are all numeric. Out of these attributes, ‘cylinders’, ‘model year’, and ‘origin’ are"
"discrete in nature as the only finite number of values can be assumed by these attributes. The remaining of the numeric attributes, i.e. ‘mpg’, ‘displacement’, ‘horsepower’, ‘weight’, and ‘acceleration’ can assume any real value."
"Since the attributes ‘cylinders’ or ‘origin’ have a small number of possible values, one may prefer to treat it as a categorical or qualitative attribute and explore in that way. Anyways, we will treat these attributes as numeric or quantitative as we are trying to show data exploration and related nuances in this section."
"Hence, these attributes are continuous in nature. The only remaining attribute ‘car name’ is of type categorical, or more specifically nominal. This data set is regarding prediction of fuel consumption in miles per gallon, i.e. the numeric attribute ‘mpg’ is the target attribute.With this understanding of the data set attributes, we can start exploring the numeric and categorical attributes separately."
"There are two most effective mathematical plots to explore numerical data – box plot and histogram. We will explore all these plots one by one, starting with the most critical one, which is the box plot."
"If the above set of numbers represents marks of 5 students in a class, the mean marks, or the falling in the middle of the range is 61.4."
"Median, on contrary, is the value of the element appearing in the middle of an ordered list of data elements. If we consider the above 5 data elements, the ordered list would be – 21, 34, 67, 89, and 96. Since there are 5 data elements, the 3rd element in the ordered list is considered as the median. Hence, the median value of this set of data is 67."
"‌There might be a natural curiosity to understand why two measures of central tendency are reviewed. The reason is mean and median are impacted differently by data values appearing at the beginning or at the end of the range. Mean being calculated from the cumulative sum of data values, is impacted if too many data elements are having values closer to the far end of the range, i.e. close to the maximum or minimum values. It is especially sensitive to outliers, i.e. the values which are unusually high or low, compared to the other values. Mean is likely to get shifted drastically even due to the"
"presence of a small number of outliers. If we observe that for certain attributes the deviation between values of mean and median are quite high, we should investigate those attributes further and try to find out the root cause along with the need for remediation."
"With a bit of investigation, we can find out that the problem is occurring because of the 6 data elements, as shown in Figure 2.7, do not have value for the attribute ‘horsepower’."
"‌For that reason, the attribute ‘horsepower’ is not treated as a numeric. That’s why the operations applicable on numeric variables, like mean or median, are failing. So we have to first remediate the missing values of the attribute ‘horsepower’ before being able to do any kind of exploration. However, we will cover the approach of remediation of missing values a little later."
"Now that we have explored the central tendency of the different numeric attributes, we have a clear idea of which attributes have a large deviation between mean and median. Let’s look closely at those attributes. To drill down more, we need to look at the entire range of values of the attributes, though not at the level of data elements as that may be too vast to review manually. So we will take a granular view of the data spread in the form of"
"Attribute 2 values : 34, 46, 59, 39, and 52Both the set of values have a mean and median of 46."
"Variance , where x is the variable orattribute whose variance is to be measured and n is the number of observations or values of variable x."
"Larger value of variance or standard deviation indicates more dispersion in the data and vice versa. In the above example, let’s calculate the variance of attribute 1 and that of attribute 2. For attribute 1,"
"‌So it is quite clear from the measure that attribute 1 values are quite concentrated around the mean while attribute 2 values are extremely spread out. Since this data was small, a visual inspection and understanding were possible and that matches with the measured value."
"then that median of the first half is known as first quartile or Q1. In the same way, if the second half of the data is divided into two halves, then that median of the second half is known as third quartile or Q3. The overall median is also known as second quartile or Q2. So, any data set has five values - minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum."
‌Quantiles refer to specific points in a data set which divide the data set into equal parts or equally sized quantities.
"There are specific variants of quantile, the one dividing data set into four parts being termed as quartile. Another such popular variant is percentile, which divides the data set into 100 parts."
"However, we still cannot ascertain whether there is any outlier present in the data. For that, we can better adopt some means to visualize the data. Box plot is an excellent visualization medium for numeric data."
"The central rectangle or the box spans from first to third quartile (i.e. Q1 to Q3), thus giving the inter-quartile range (IQR).Median is given by the line or band within the box."
"The lower whisker extends up to 1.5 times of the inter-quartile range (or IQR) from the bottom of the box, i.e. the first quartile or Q1. However, the actual length of the lower whisker depends on the lowest data value that falls within (Q1 – 1.5 times of IQR). Let’s try to understand this with an example. Say for a specific set of data, Q1 = 73, median = 76 and Q3 = 79. Hence, IQR will be 6 (i.e. Q3 – Q1).So, lower whisker can extend maximum till (Q1 – 1.5 × IQR) = 73 – 1.5 × 6 = 64. However, say there are lower range data values such as 70, 63, and 60. So, the lower whisker will come at 70 as this is the lowest data value larger than 64."
"The upper whisker extends up to 1.5 as times of the inter-quartile range (or IQR) from the top of the box, i.e. the third quartile or Q3. Similar to lower whisker, the actual length of the upper whisker will also depend on the highest data value that falls within (Q3 + 1.5 times of IQR). Let’s try to understand this with an example. For the same set of data mentioned in the above point, upper whisker can extend maximum till (Q3 + 1.5 × IQR) = 79+ 1.5 × 6 = 88. If there is higher range of data values like 82, 84, and 89. So, the upper whisker will come at 84 as this is the highest data value lower than 88."
"The data values coming beyond the lower or upper whiskers are the ones which are of unusually low or high values respectively. These are the outliers, which may deserve special consideration."
There are different variants of box plots. The one covered above is the Tukey box plot. Famous mathematician JohnW. Tukey introduced this type of box plot in 1969.
"‌Let’s visualize the box plot for the three attributes - ‘cylinders’, ‘displacement’, and ‘origin’. We will also review the box plot of another attribute in which the deviation between mean and median is very little and see what the basicdifference in the respective box plots is. Figure 2.10 presents the respective box plots."
"The box plot for attribute ‘cylinders’ looks pretty weird in shape. The upper whisker is missing, the band for median falls at the bottom of the box, even the lower whisker is pretty small compared to the length of the box! Is everything right?"
"The answer is a big YES, and you can figure it out if you delve a little deeper into the actual data values of the attribute. The attribute ‘cylinders’ is discrete in nature having values from 3 to 8. Table 2.2 captures the frequency and cumulative frequency of it."
"‌As can be observed in the table, the frequency is extremely high for data value 4. Two other data values where the frequency is quite high are 6 and 8. So now if we try to find the quartiles, since the total frequency is 398, the first quartile (Q1), median (Q2), and third quartile (Q3) will be at a cumulative frequency 99.5 (i.e. average of 99th and 100th observation), 199 and 298.5 (i.e. average of 298th and 299th observation), respectively. This way Q1 = 4, median = 4 and Q3 = 8. Since there is no data value beyond 8, there is no upper whisker. Also, since both Q1 and median are 4, the band for median falls on the bottom of the box. Same way, though the lower whisker could have extended till -2 (Q1 – 1.5 × IQR"
"= 4 – 1.5 × 4 = –2), in reality, there is no data value lower than"
"Like the box plot for attribute ‘cylinders’, the box plot for attribute ‘cylinders’ also looks pretty weird in shape. Here the lower whisker is missing and the band for median falls at the bottom of the box! Let’s verify if everything right?"
"Just like the attribute ‘cylinders’, attribute ‘origin’ is discrete in nature having values from 1 to 3. Table 2.3 capturesthe frequency and cumulative frequency (i.e. a summation of frequencies of all previous intervals) of it."
"The box plot for the attribute ‘displacement’ looks better than the previous box plots. However, still, there are few small abnormalities, the cause of which needs to be reviewed.Firstly, the lower whisker is much smaller than an upper whisker. Also, the band for median is closer to the bottom of the box."
"‌Let’s take a closer look at the summary data of the attribute ‘displacement’. The value of first quartile, Q1 = 104.2, median"
157.8) = 81%] of the permissible length. This is why the upper whisker is much longer than the lower whisker.The box plot for the attribute ‘model. year’ looks perfect. Let’s validate is it really what expected to be.
"For the attribute ‘model.year’: First quartile, Q1 = 73 Median, Q2 = 76"
"So, the difference between median and Q1 is exactly equal to Q3 and median (both are 3). That is why the band for the median is exactly equidistant from the bottom and top of the box."
Difference between Q1 and minimum data value (i.e. 70) is also same as maximum data value (i.e. 82) and Q3 (both are 3). So both lower and upper whiskers are expected to be of the same size which is 33% [3 / (1.5 × 6)] of the permissible length.
"Histograms might be of different shapes depending on the nature of the data, e.g. skewness. Figure 2.11 provides a depiction of different shapes of the histogram that are generally created. These patterns give us a quick understanding of the data and thus act as a great data exploration tool."
"Let’s now examine the histograms for the different attributes of Auto MPG data set presented in Figure 2.12. The histograms for ‘mpg’ and ‘weight’ are right-skewed. The histogram for ‘acceleration’ is symmetric and unimodal, whereas the one for ‘model.year’ is symmetric and uniform.For the remaining attributes, histograms are multimodal in nature."
"Now let’s dig deep into one of the histograms, say the one for the attribute ‘acceleration’. The histogram is composed of a number of bars, one bar appearing for each of the ‘bins’. The height of the bar reflects the total count of data elements whose value falls within the specific bin value, or the frequency. Talking in context of the histogram for acceleration, each ‘bin’ represents an acceleration value interval of 2 units."
"So the second bin, e.g., reflects acceleration value of 10 to 12 units. The corresponding bar chart height reflects the count of"
"all data elements whose value lies between 10 and 12 units. Also, it is evident from the histogram that it spans over the acceleration value of 8 to 26 units. The frequency of data elements corresponding to the bins first keep on increasing, till it reaches the bin of range 14 to 16 units. At this range, the bar is tallest in size. So we can conclude that a maximum number of data elements fall within this range. After this range, the bar size starts decreasing till the end of the whole range at the acceleration value of 26 units.Please note that when the histogram is uniform, as in the case of attribute ‘model. year’, it gives a hint that all values are equally likely to occur."
"‌We have seen there are multiple ways to explore numeric data. However, there are not many options for exploring categorical data. In the Auto MPG data set, attribute ‘car.name’ is categorical in nature. Also, as we discussed earlier, we may consider ‘cylinders’ as a categorical variable instead of a numeric variable."
The first summary which we may be interested in noting is how many unique names are there for the attribute ‘car name’
or how many unique values are there for ‘cylinders’ attribute. We can get this as follows:
We may also look for a little more details and want to get a table consisting the categories of the attribute and count of the data elements falling into that category. Tables 2.4 and 2.5 contain these details.
"In the same way, we may also be interested to know the proportion (or percentage) of count of data elements belonging to a category. Say, e.g., for the attributes ‘cylinders’, the proportion of data elements belonging to the category 4 is 204÷ 398 = 0.513, i.e. 51.3%. Tables 2.6 and 2.7 contain the summarization of the categorical attributes by proportion of data elements."
"‌Last but not the least, as we have read in the earlier section on types of data, statistical measure “mode” is applicable on categorical attributes. As we know, like mean and median, mode is also a statistical measure for central tendency of a data. Mode of a data is the data value which appears most often. In context of categorical attribute, it is the category which has highest number of data values. Since mean and median cannot be applied for categorical variables, mode is the sole measure of central tendency."
"Let’s try to find out the mode for the attributes ‘car name’ and ‘cylinders’. For cylinders, since the number of categories is less and we have the entire table listed above, we can see that the mode is 4, as that is the data value for which frequency is highest. More than 50% of data elements belong to the category 4. However, it is not so evident for the attribute ‘car name’ from the information given above. When we probe and try to find the mode, it is found to be category ‘ford pinto’ for which frequency is of highest value 6."
"An attribute may have one or more modes. Frequency distribution of an attribute having single mode is called ‘unimodal’, two modes are called ‘bimodal’ and multiple modes are called ‘multimodal’."
Till now we have been exploring single attributes in isolation. One more important angle of data exploration is to explore relationship between attributes. There are multiple plots to enable us explore the relationship between variables. The basic and most commonly used plot is scatter plot.
"A scatter plot helps in visualizing bivariate relationships, i.e. relationship between two variables. It is a two-dimensional plot in which points or dots are drawn on coordinates provided by values of the attributes. For example, in a data set there are two attributes – attr_1 and attr_2. We want to understand the relationship between two attributes, i.e. with a change in value of one attribute, say attr_1, how does the value of the other attribute, say attr_2, changes. We can draw a scatter plot, with attr_1 mapped to x-axis and attr_2 mapped in y-axis. So, every point in the plot will have value of attr_1 in the x-coordinate and value of attr_2 in the y-coordinate. As in a two- dimensional plot, attr_1 is said to be the independent variable and attr_2 as the dependent variable."
"Let’s take a real example in this context. In the data set Auto MPG, there is expected to be some relation between the attributes ‘displacement’ and ‘mpg’. Let’s try to verify our intuition using the scatter plot of ‘displacement’ and ‘mpg’."
Let’s map ‘displacement’ as the x-coordinate and ‘mpg’ as the
"As is evident in the scatter plot, there is a definite relation between the two variables. The value of ‘mpg’ seems to steadily decrease with the increase in the value of ‘displacement’. It may come in our mind that what is the extent of relationship? Well, it can be reviewed by calculatingthe correlation between the variables. Refer to chapter 5 if you want to find more about correlation and how to calculate it."
"One more interesting fact to notice is that there are certain data values which stand-out of the others. For example, there is one data element which has a mpg of 37 for a displacement of 250. This record is completely different from other data elements having similar displacement value but mpg value in the range of 15 to 25. This gives an indication that of presence of outlier data values."
"In Figure 2.14, the pair wise relationship among the features – ‘mpg’, ‘displacement’, ‘horsepower’, ‘weight’, and ‘acceleration’ have been captured. As you can see, in most of the cases, there is a significant relationship between the attribute pairs. However, in some cases, e.g. between attributes ‘weight’ and ‘acceleration’, the relationship doesn’t seem to be very strong."
"‌Two-way cross-tabulations (also called cross-tab or contingency table) are used to understand the relationship of two categorical attributes in a concise way. It has a matrix format that presents a summarized view of the bivariate frequency distribution. A cross-tab, very much like a scatter plot, helps to understand how much the data values of one attribute changes with the change in data values of anotherattribute. Let’s try to see with examples, in context of the Auto MPG data set."
"Let’s assume the attributes ‘cylinders’, ‘model.year’, and ‘origin’ as categorical and try to examine the variation of one with respect to the other. As we understand, attribute ‘cylinders’ reflects the number of cylinders in a car and assumes values 3, 4, 5, 6, and 8. Attribute ‘model.year’ captures the model year of each of the car and ‘origin’ gives the region of the car, the values for origin 1, 2, and 3 corresponding to North America, Europe, and Asia. Below are the cross-tabs. Let’s try to understand what information they actually provide."
"‌The first cross-tab, i.e. the one showing relationship between attributes ‘model. year’ and ‘origin’ help us understand the number of vehicles per year in each of the regions North America, Europe, and Asia. Looking at it in another way, we can get the count of vehicles per region over the different years. All these are in the context of the sample data given in the Auto MPG data set."
"Moving to the second cross-tab, it gives the number of 3, 4, 5, 6, or 8 cylinder cars in every region present in the sample data set. The last cross-tab presents the number of 3, 4, 5, 6, or 8 cylinder cars every year."
We may also want to create cross-tabs with a more summarized view like have a cross-tab giving a number of cars having 4 or less cylinders and more than 4 cylinders in each region or by the years. This can be done by rolling up data values by the attribute ‘cylinder’. Tables 2.8–2.10 present cross-tabs for different attribute combinations.
"Certain data elements without a value or data with a missing value.Data elements having value surprisingly different from the other elements, which we term as outliers."
There are multiple factors which lead to these data quality issues. Following are some of them:
"‌The issues in data quality, as mentioned above, need to be remediated, if the right amount of efficiency has to be achieved in the learning activity. Out of the two major areas mentioned above, the first one can be remedied by proper sampling technique. This is a completely different area – covered as a specialized subject area in statistics. We will not cover that in this book. However, human errors are bound to happen, no matter whatever checks and balances we put in.Hence, proper remedial steps need to be taken for the second area mentioned above. We will discuss how to handle outliers and missing values."
"Outliers are data elements with an abnormally high value which may impact prediction accuracy, especially in regression models. Once the outliers are identified and the decision has been taken to amend those values, you may consider one of the following approaches. However, if the outliers are natural, i.e. the value of the data element is surprisingly high or low because of a valid reason, then we should not amend it."
"In a data set, one or more data elements may have missing values in multiple records. As discussed above, it can be caused by omission on part of the surveyor or a person who is collecting sample data or by the responder, primarily due to his/her unwillingness to respond or lack of understanding needed to provide a response. It may happen that a specific question (based on which the value of a data element originates) is not applicable to a person or object with respect to which data is collected. There are multiple strategies to handle missing value of data elements. Some of those strategies have been discussed below."
"However, this will not be possible if the proportion of records having data elements with missing value is really high as that will reduce the power of model because of reduction in the training data size."
"For example, in context of the attribute ‘horsepower’ of the Auto MPG data set, since the attribute is quantitative, we take a mean or median of the remaining data element values and assign that to all data elements having a missing value. So, we may assign the mean, which is 104.47 and assign it to all the six data elements. The other approach is that we can take a similarity based mean or median. If we refer to the six observations with missing values for attribute ‘horsepower’ as depicted in Table 2.11, ‘cylinders’ is the attribute which is logically most connected to ‘horsepower’ because with the increase in number of cylinders of a car, the horsepower of the"
"car is expected to increase. So, for five observations, we can use the mean of data elements of the ‘horsepower’ attribute having cylinders = 4; i.e. 78.28 and for one observation which has cylinders = 6, we can use a similar mean of data elements with cylinders = 6, i.e. 101.5, to impute value to the missing data elements."
"For example, let’s assume that the weight of a Russian student having age 12 years and height 5 ft. is missing. Then the weight of any other Russian student having age close to 12 years and height close to 5 ft. can be assigned."
"Till the end of the 1990s, very few domains were explored which included data sets with a high number of attributes or features. In general, the data sets used in machine learning used to be in few 10s. However, in the last two decades, there has been a rapid advent of computational biology like genome projects. These projects have produced extremely high- dimensional data sets with 20,000 or more features being very common. Also, there has been a wide-spread adoption of social networking leading to a need for text classification for customer behaviour analysis."
"High-dimensional data sets need a high amount of computational space and time. At the same time, not all features are useful – they degrade the performance of machine learning algorithms. Most of the machine learning algorithms perform better if the dimensionality of data set, i.e. the number of features in the data set, is reduced. Dimensionality reduction helps in reducing irrelevance and redundancy in features. Also, it is easier to understand a model if the number of features involved in the learning activity is less."
"capture the maximum amount of variability in the data. However, the only challenge is that the original attributes are lost due to the transformation.Another commonly used technique which is used for dimensionality reduction is Singular Value Decomposition (SVD)."
"Feature subset selection or simply called feature selection, both for supervised as well as unsupervised learning, try to find out the optimal subset of the entire feature set which significantly reduces computational cost without any major impact on the learning accuracy. It may seem that a feature subset may lead to loss of useful information as certain features are going to be excluded from the final set of features used for learning. However, for elimination only features which are not relevant or redundant are selected."
"‌A feature is considered as irrelevant if it plays an insignificant role (or contributes almost no information) in classifying or grouping together a set of data instances. All irrelevant features are eliminated while selecting the final feature subset. A feature is potentially redundant when the information contributed by the feature is more or less same as one or more other features. Among a group of potentially redundant features, a small number of features can be selected as a part of the final feature subset without causing any negative impact to learn model accuracy."
"None of the aboveFor bi-variate data exploration,         is an effective tool."
"All of the aboveFor categorical data,       cannot be used as a measure of central tendency."
"None of the aboveFor understanding relationship between two variables,       can be used."
What are the main activities involved when you are preparing to start with modelling in machine learning?What are the basic data types in machine learning? Give an example of each one of them.
Why do we need to explore data? Is there a difference in the way of exploring qualitative data vis-a-vis quantitative data?
"What are the different measures of central tendency? Why do mean, in certain data sets, differ widely from median?"
Explain how bivariate relationships can be explored using scatter plot. Can outliers be detected using scatter plot?Explain how cross-tabs can be used to understand relationship between two variables.
Explain qualitative and quantitative data in details. Differentiate between the two.Prepare a simple data set along with some sample records in it. Have at least one attribute of the different data types used in machine learning.
"What are the different causes of data issues in machine learning? What are the fallouts?Explain, with proper example, different ways of exploring categorical data."
"Explain, in details, the different strategies of addressing missing data values.What are the different techniques for data pre-processing? Explain, in brief, dimensionality reduction and feature selection."
"2. Explain, in details, the different components of a box plot? When will the lower whisker be longer than the upper whisker? How can outliers be detected using box plot?"
"The previous chapter gives a comprehensive understanding of the basic data types in the context of machine learning. It also enables a beginner in the field of machine learning to acquire an understanding about the nature and quality of the data by effective exploration of the data set. In this chapter, the objective is to introduce the basic concepts of learning. In this regard, the information shared concerns the aspects of model selection and application. It also imparts knowledge regarding how to judge the effectiveness of the model in doing a specific learning task, supervised or unsupervised, and how to boost the model performance using different tuning parameters."
‌The learning process of machines may seem quite magical to somebody who is new to machine learning. The thought that a machine is able to think and take intelligent action may be mesmerizing – much like a science fiction or a fantasy story.
"However, delving a bit deeper helps them realize that it is not as magical as it may seem to be. In fact, it tries to emulate human learning by applying mathematical and statistical formulations. In that sense, both human and machine learning strives to build formulations or mapping based on a limited number of observations. As introduced in Chapter 1, the basic learning process, irrespective of the fact that the learner is a human or a machine, can be divided into three parts:"
"Though in Chapter 1 we have understood these aspects in details, let’s quickly refresh our memory with an example. It’s a fictitious situation. The detective department of New City Police has got a tip that in a campaign gathering for the upcoming election, a criminal is going to launch an attack on the main candidate. However, it is not known who the person is and quite obviously the person might use some disguise."
"The only thing that is for sure is the person is a history-sheeter or a criminal having a long record of serious crime. From the criminal database, a list of such criminals along with their photographs has been collected. Also, the photos taken by security cameras positioned at different places near the gathering are available with the detective department. They have to match the photos from the criminal database with the faces in the gathering to spot the potential attacker. So the main problem here is to spot the face of the criminal based on the match with the photos in the criminal database."
This can be done using human learning where a person from the detective department can scan through each shortlisted photo and try to match that photo with the faces in the gathering. A person having a strong memory can take a glance
"at the photos of all criminals in one shot and then try to find a face in the gathering which closely resembles one of the criminal photos that she has viewed. Easy, isn’t it? But that is not possible in reality. The number of criminals in the database and hence the count of photos runs in hundreds, if not thousands. So taking a look at all the photos and memorizing them is not possible. Also, an exact match is out of the question as the criminal, in most probability, will come in disguise. The strategy to be taken here is to match the photos in smaller counts and also based on certain salient physical features like the shape of the jaw, the slope of the forehead, the size of the eyes, the structure of the ear, etc. So, the photos from the criminal database form the input data. Based on it, key features can be abstracted. Since human matching for each and every photo may soon lead to a visual as well as mental fatigue, a generalization of abstracted feature-based data is a good way to detect potential criminal faces in the gathering."
"For example, from the abstracted feature-based data, say it isThe same thing can be done using machine learning too."
"i.e. criminal database photos, apply computational techniques"
to abstract feature-based concept map from the input data and generalize the same in the form of a classification algorithm to decide whether a face in the gathering is potentially criminal or not.
"‌When we talk about the learning process, abstraction is a significant step as it represents raw input data in a summarized and structured format, such that a meaningful insight is obtained from the data. This structured representation of raw input data to the meaningful pattern is called a model. The model might have different forms. It might be a mathematical equation, it might be a graph or tree structure, it might be a computational block, etc. The decision regarding which model is to be selected for a specific data set is taken by the learning task, based on the problem to be solved and the type of data."
"For example, when the problem is related to prediction and the target field is numeric and continuous, the regression model is assigned. The process of assigning a model, and fitting a specific model to a data set is called model training. Once the model is trained, the raw input data is summarized into an abstracted form."
"However, with abstraction, the learner is able to only summarize the knowledge. This knowledge might be still very broad-based – consisting of a huge number of feature-based data and inter-relations. To generate actionable insight from such broad-based knowledge is very difficult. This is where generalization comes into play. Generalization searches through the huge set of abstracted knowledge to come up with a small and manageable set of key findings. It is not possible to do an exhaustive search by reviewing each of the abstracted findings one-by-one. A heuristic search is employed, an approach which is also used for human learning (often termed as ‘gut-feel’). It is quite obvious that the heuristics sometimesresult in erroneous result. If the outcome is systematically incorrect, the learning is said to have a bias."
"A machine learning algorithm creates its cognitive capability by building a mathematical formulation or function, known as target function, based on the features in the input data set."
"Just like a child learning things for the first time needs her parents guidance to decide whether she is right or wrong, in machine learning someone has to provide some non-learnable parameters, also called hyper-parameters. Without these human inputs, machine learning algorithms cannot be successful."
"‌Now that you are familiar with the basic learning process and have understood model abstraction and generalization in that context, let’s try to formalize it in context of a motivating example. Continuing the thread of the potential attack during the election campaign, New City Police department has succeeded in foiling the bid to attack the electoral candidate. However, this was a wake-up call for them and they want to take a proactive action to eliminate all criminal activities in the region. They want to find the pattern of criminal activities in the recent past, i.e. they want to see whether the number of criminal incidents per month has any relation with an average income of the local population, weapon sales, the inflow of immigrants, and other such factors. Therefore, an association between potential causes of disturbance and criminal incidents has to be determined. In other words, the goal or target is to develop a model to infer how the criminal incidents change based on the potential influencing factors mentioned above."
"In machine learning paradigm, the potential causes of disturbance, e.g. average income of the local population, weapon sales, the inflow of immigrants, etc. are input variables. They are also called predictors, attributes, features, independent variables, or simply variables. The number of criminal incidents is an output variable (also called response or dependent variable). Input variables can be denoted by X, while individual input variables are represented as X1, X2, X3,"
"…, Xn and output variable by symbol Y. The relationship between X and Y is represented in the general form: Y = f (X) + e, where ‘f ’ is the target function and ‘e’ is a random error term."
"Just like a target function with respect to a machine learning model, some other functions which are frequently tracked are"
"A cost function (also called error function) helps to measure the extent to which the model is going wrong in estimating the relationship between X and Y. In that sense, cost function can tell how bad the model is performing. For example, R-squared (to be discussed later in this chapter) is a cost function of regression model."
"Machine learning is an optimization problem. We try to define a model and tune the parameters to find the most suitable solution to a problem. However, we need to have a way to evaluate the quality or optimality of a solution. This is done using objective function. Objective means goal."
"Objective function takes in data and model (along with parameters) as input and returns a value. Target is to find values of model parameter to maximize or minimize the return value. When the objective is to minimize the value, it becomes synonymous to cost function. Examples:"
"For each of the cases, the model that has to be created/trained is different. Multiple factors play a role when we try to select the model for solving a machine learning problem. The most important factors are (i) the kind of problem we want to solve using machine learning and (ii) the nature of the underlying data. The problem may be related to the prediction of a class value like whether a tumour is malignant or benign, whether the next day will be snowy or rainy, etc. It may be related to prediction – but of some numerical value like what the price of a house should be in the next quarter, what is the expected growth of a certain IT stock in the next 7 days, etc. Certain problems are related to grouping of data like finding customer segments that are using a certain product, movie genres which have got more box office success in the last one year, etc. So, it is very difficult to give a generic guidance related to which machine learning has to be selected. In other words, there is no one model thatworks best for every machine learning problem. This is what ‘No Free Lunch’ theorem also states."
"Any learning model tries to simulate some real-world aspect. However, it is simplified to a large extent removing all intricate details. These simplifications are based on certain assumptions – which are quite dependent on situations. Based on the exact situation, i.e. the problem in hand and the data characteristics, assumptions may or may not hold. So the same model may yield remarkable results in a certain situation while it may completely fail in a different situation. That’s why, while doing the data exploration, which we covered in the previous chapter, we need to understand the data characteristics, combine this understanding with the problem we are trying to solve and then decide which model to be selected for solving the problem."
"Let’s try to understand the philosophy of model selection in a structured way. Machine learning algorithms are broadly of two types: models for supervised learning, which primarily focus on solving predictive problems and models for unsupervised learning, which solve descriptive problems."
"Models for supervised learning or predictive models, as is understandable from the name itself, try to predict certain value using the values in an input data set. The learning model attempts to establish a relation between the target feature, i.e. the feature being predicted, and the predictor features. The predictive models have a clear focus on what they want to learn and how they want to learn."
"‌The models which are used for prediction of target features of categorical value are known as classification models. The target feature is known as a class and the categories to which classes are divided into are called levels. Some of the popular classification models include k-Nearest Neighbor (kNN), Naïve Bayes, and Decision Tree."
Predictive models may also be used to predict numerical values of the target feature based on the predictor features. Below are some examples:
Prediction of potential flu patients and demand for flu shots next winter
The models which are used for prediction of the numerical value of the target feature of a data instance are known as regression models. Linear Regression and Logistic Regression models are popular regression models.
"Categorical values can be converted to numerical values and vice versa. For example, for stock price growth prediction, any growth percentage lying between certain ranges may be represented by a categorical value,"
"e.g. 0%–5% as ‘low’, 5%–10% as ‘moderate’, 10%–20% as ‘high’ and"
"20% as ‘booming’. In a similar way, a categorical value can be converted to numerical value, e.g. in the tumor malignancy detection"
"problem, replace ‘benign’ as 0 and ‘malignant’ as 1. This way, the models can be used interchangeably, though it may not work always. There are multiple factors to be considered while selecting a model. For example, while selecting the model for prediction, the training data size is an important factor to be considered. If the training data set is small, low variance models like Naïve Bayes are supposed to perform better because model overfitting needs to be avoided in this situation.Similarly, when the training data is large, low bias models like logistic regression should be preferred because they can represent complex relationships in a more effective way."
‌Descriptive models‌Models for unsupervised learning or descriptive models are used to describe a data set or gain insight from a data set.
"There is no target feature or single feature of interest in case of unsupervised learning. Based on the value of all features, interesting patterns or insights are derived about the data set."
"Descriptive models which group together similar data instances, i.e. data instances having a similar value of the different features are called clustering models. Examples of clustering include"
"Customer grouping or segmentation based on social, demographic, ethnic, etc. factorsGrouping of music based on different aspects like genre, language, time- period, etc."
Grouping of commodities in an inventory‌The most popular model for clustering is k-Means.
"Descriptive models related to pattern discovery is used for market basket analysis of transactional data. In market basket analysis, based on the purchase pattern available in the transactional data, the possibility of purchasing one product based on the purchase of another product is determined. For example, transactional data may reveal a pattern that generally a customer who purchases milk also purchases biscuit at the same time. This can be useful for targeted promotions or in- store set up. Promotions related to biscuits can be sent to customers of milk products or vice versa. Also, in the store products related to milk can be placed close to biscuits."
"In case of supervised learning, a model is trained using the labelled input data. However, how can we understand the performance of the model? The test data may not be available immediately. Also, the label value of the test data is not known. That is the reason why a part of the input data is held back (that is how the name holdout originates) for evaluation of the model. This subset of the input data is used as the test data for evaluating the performance of a trained model. In general 70%–80% of the input data (which is obviously labelled) is used for model training. The remaining 20%–30% is used as test data for validation of the performance of the model. However, a different proportion of dividing the input data into training and test data is also acceptable. To make sure that the data in both the buckets are similar in nature, the division is done randomly. Random numbers are used to assign data items to the partitions. This method of partitioning the input data into two parts – training and test data (depicted inFigure 3.1), which is by holding back a part of the input data for validating the trained model is known as holdout method."
"‌Once the model is trained using the training data, the labels of the test data are predicted using the model’s target function. Then the predicted value is compared with the actual value of the label. This is possible because the test data is a part of the input data with known labels. The performance of the model is in general measured by the accuracy of prediction of the label value."
"In certain cases, the input data is partitioned into three portions – a training and a test data, and a third validation data. The validation data is used in place of test data, for measuring the model performance. It is used in iterations and to refine the model in each iteration. The test data is used only for once, after the model is refined and finalized, to measure and report the final performance of the model as a reference for future learning efforts."
"Holdout method employing stratified random sampling approach still heads into issues in certain specific situations. Especially, the smaller data sets may have the challenge to divide the data of some of the classes proportionally amongst training and test data sets. A special variant of holdout method, called repeated holdout, is sometimes employed to ensure the randomness of the composed data sets. In repeated holdout, several random holdouts are used to measure the model performance. In the end, the average of all performances is taken. As multiple holdouts have been drawn, the training and test data (and also validation data, in case it is drawn) are more likely to contain representative data from all classes and resemble the original input data closely. This process of repeated holdout is the basis of k-fold cross- validation technique. In k-fold cross-validation, the data set is divided into k-completely distinct or non-overlapping random partitions called folds. Figure 3.2 depicts an overall approach for k-fold cross-validation."
Leave-one-out cross-validation (LOOCV)10-fold cross-validation is by far the most popular approach.
"Bootstrap sampling or simply bootstrapping is a popular way to identify training and test data sets from the input data set. It uses the technique of Simple Random Sampling with Replacement (SRSWR), which is a well-known technique in sampling theory for drawing random samples. We have seen earlier that k-fold cross-validation divides the data into separate partitions – say 10 partitions in case of 10-fold cross- validation. Then it uses data instances from partition as test data and the remaining partitions as training data. Unlike this approach adopted in case of k-fold cross- validation, bootstrapping randomly picks data instances from the input data set, with the possibility of the same data instance to be picked multiple times. This essentially means that from the input data set having ‘n’ data instances, bootstrapping can create one or more training data sets having ‘n’ data instances, some of the data instances being repeated multiple times."
"Figure 3.4 briefly presents the approach followed in bootstrapThis technique is particularly useful in case of input data sets of small size, i.e. having very less number of data instances."
"doesn’t need to refer back to the training data. Eager learners take more time in the learning phase than the lazy learners.Some of the algorithms which adopt eager learning approach include Decision Tree, Support Vector Machine, Neural Network, etc."
"Lazy learning, on the other hand, completely skips the abstraction and generalization processes, as explained in context of a typical machine learning process. In that respect, strictly speaking, lazy learner doesn’t ‘learn’ anything. It uses the training data in exact, and uses the knowledge to classify the unlabelled test data. Since lazy learning uses training data as-is, it is also known as rote learning (i.e. memorization technique based on repetition). Due to its heavy dependency on the given training data instance, it is also known as instance learning. They are also called non-parametric learning. Lazy learners take very little time in training because not much of training actually happens. However, it takes quite some time in classification as for each tuple of test data, a comparison-based assignment of label happens. One of the most popular algorithm for lazy learning is k-nearest neighbor."
"‌clubbed as parametric.On the other hand, in case of models such as k-Nearest Neighbor (kNN) and decision tree, number of parameters grows with the size of the training data. Hence, they are considered as non-parametric learning models."
"We have already seen that the goal of supervised machine learning is to learn or derive a target function which can best determine the target variable from the set of input variables. A key consideration in learning the target function from the training data is the extent of generalization. This is because the input data is just a limited, specific view and the new, unknown data in the test data set may be differing quite a bit from the training data.Fitness of a target function approximated by a learning algorithm determines how correctly it is able to classify a set of data it has never seen."
"If the target function is kept too simple, it may not be able to capture the essential nuances and represent the underlying data well. A typical case of underfitting may occur when trying to represent a non-linear data with a linear model as demonstrated by both cases of underfitting shown in figure"
3.5. Many times underfitting happens due to unavailability of sufficient training data. Underfitting results in both poor performance with training data as well as poor generalization to test data. Underfitting can be avoided by
"Overfitting refers to a situation where the model has been designed in such a way that it emulates the training data too closely. In such a case, any specific deviation in the training data, like noise or outliers, gets embedded in the model. It adversely impacts the performance of the model on the test data. Overfitting, in many cases, occur as a result of trying to fit an excessively complex model to closely match the training data. This is represented with a sample data set in figure 3.5 . The target function, in these cases, tries to make sure all training data points are correctly partitioned by the decision boundary. However, more often than not, this exact nature is not replicated in the unknown test data set. Hence, the target function results in wrong classification in the test data set."
"Overfitting results in good performance with training data set,"
but poor generalization and hence poor performance with test data set. Overfitting can be avoided by
remove the nodes which have little or no predictive power for the given machine learning problem.Both underfitting and overfitting result in poor classification quality which is reflected by low classification accuracy.
"In supervised learning, the class value assigned by the learning model built based on the training data may differ from the actual class value. This error in learning can be of two types – errors due to ‘bias’ and error due to ‘variance’. Let’s try to understand each of them in details."
"Errors due to bias arise from simplifying assumptions made by the model to make the target function less complex or easier to learn. In short, it is due to underfitting of the model."
"Parametric models generally have high bias making them easier to understand/interpret and faster to learn. These algorithms have a poor performance on data sets, which are complex in nature and do not align with the simplifying assumptions made by the algorithm. Underfitting results in high bias."
Errors due to variance occur from difference in training data sets used to train the model. Different training data sets (randomly sampled from the input data set) are used to train
"the model. Ideally the difference in the data sets should not be significant and the model trained using different training data sets should not be too different. However, in case of overfitting, since the model closely matches the training data, even a small difference in training data gets magnified inthe model."
"‌So, the problems in training a model can either happen because either (a) the model is too simple and hence fails to interpret the data grossly or (b) the model is extremelycomplex and magnifies even small differences in the training data."
"As can be observed in Figure 3.6 , the best solution is to have a model with low bias as well as low variance. However, that may not be possible in reality. Hence, the goal of supervised machine learning is to achieve a balance between bias and variance. The learning algorithm chosen and the user parameters which can be configured helps in striking a trade- off between bias and variance. For example, in a popular supervised algorithm k-Nearest Neighbors or kNN, the user configurable parameter ‘k’ can be used to do a trade-off between bias and variance. In one hand, when the value of ‘k’ is decreased, the model becomes simpler to fit and bias increases. On the other hand, when the value of ‘k’ is increased, the variance increases."
"In supervised learning, one major task is classification. The responsibility of the classification model is to assign class label to the target feature based on the value of the predictor features. For example, in the problem of predicting the"
"win/loss in a cricket match, the classifier will assign a class value win/loss to target feature based on the values of other features like whether the team won the toss, number of spinners in the team, number of wins the team had in the tournament, etc. To evaluate the performance of the model, the number of correct classifications or predictions made by the model has to be recorded. A classification is said to be correct if, say for example in the given problem, it has been predicted by the model that the team will win and it has actually won."
"Based on the number of correct and incorrect classifications or predictions made by a model, the accuracy of the model is calculated. If 99 out of 100 times the model has classified correctly, e.g. if in 99 out of 100 games what the model has predicted is same as what the outcome has been, then the model accuracy is said to be 99%. However, it is quite relative to say whether a model has performed well just by looking at the accuracy value. For example, 99% accuracy in case of a sports win predictor model may be reasonably good but the same number may not be acceptable as a good threshold when the learning problem deals with predicting a critical illness. In this case, even the 1% incorrect prediction may lead to loss of many lives. So the model performance needs to be evaluated in light of the learning problem in question. Also, in certain cases, erring on the side of caution may be preferred at the cost of overall accuracy. For that reason, we need to look more closely at the model accuracy and also at the same time look at other measures of performance of a model like sensitivity, specificity, precision, etc. So, let’s start with looking at model accuracy more closely. And let’s try to understand it with an example."
There are four possibilities with regards to the cricket match win/loss prediction:
"the model predicted loss and the team lostIn this problem, the obvious class of interest is ‘win’."
"The first case, i.e. the model predicted win and the team won is a case where the model has correctly classified data instances as the class of interest. These cases are referred as True Positive (TP) cases."
"The second case, i.e. the model predicted win and the team lost is a case where the model incorrectly classified data instances as the class of interest. These cases are referred as False Positive (FP) cases."
"The third case, i.e. the model predicted loss and the team won is a case where the model has incorrectly classified as not the class of interest. These cases are referred as False Negative (FN) cases."
"The fourth case, i.e. the model predicted loss and the team lost is a case where the model has correctly classified as not the class of interest. These cases are referred as True Negative (TN) cases. All these four cases are depicted in Figure 3.7 ."
"For any classification model, model accuracy is given by total number of correct classifications (either as the class of interest, i.e. True Positive or as not the class of interest, i.e. True Negative) divided by total number of classifications done."
"A matrix containing correct and incorrect predictions in the form of TPs, FPs, FNs and TNs is known as confusion matrix. The win/loss prediction of cricket match has two classes of interest – win and loss. For that reason it will generate a 2 × 2 confusion matrix. For a classification problem involving three classes, the confusion matrix would be 3 × 3, etc."
‌Let’s assume the confusion matrix of the win/loss prediction of cricket match problem to be as below:
"Sometimes, correct prediction, both TPs as well as TNs, may happen by mere coincidence. Since these occurrences boost model accuracy, ideally it should not happen. Kappa value of a model indicates the adjusted the model accuracy. It is calculated using the formula below:"
"‌Kappa value can be 1 at the maximum, which represents perfect agreement between model’s prediction and actual values."
"As discussed earlier, in certain learning problems it is critical to have extremely low number of FN cases, if needed, at the cost of a conservative classification model. Though it is a clear case of misclassification and will impact model accuracy adversely, it is still required as missing each class of interest may have serious consequence. This happens more in problems from medical domains like disease prediction problem. For example, if a tumor is malignant but wrongly classified as benign by the classifier, then the repercussion of such misclassification is fatal. It does not matter if higher number of tumours which are benign are wrongly classified as malignant. In these problems there are some measures of model performance which are more important than accuracy. Two such critical measurements are sensitivity and specificity of the model."
The sensitivity of a model measures the proportion of TP examples or positive cases which were correctly classified. It is measured as
"In the context of the above confusion matrix for the cricket match win prediction problem,"
"So, again taking the example of the malignancy prediction of tumours, class of interest is ‘malignant’. Sensitivity measure gives the proportion of tumours which are actually malignant and have been predicted as malignant. It is quite obvious that for such problems the most critical measure of the performance of a good model is sensitivity. A high value of sensitivity is more desirable than a high value of accuracy."
"‌A higher value of specificity will indicate a better model performance. However, it is quite understandable that a conservative approach to reduce False Negatives might actually push up the number of FPs. Reason for this is that the model, in order to reduce FNs, is going to classify more tumours as malignant. So the chance that benign tumours will be classified as malignant or FPs will increase."
"There are two other performance measures of a supervised learning model which are similar to sensitivity and specificity. These are precision and recall. While precision gives the proportion of positive predictions which are truly positive, recall gives the proportion of TP cases over all actually positive cases."
It is quite understandable that a model with higher precision is perceived to be more reliable.
"Recall indicates the proportion of correct prediction of positives to the total number of positives. In case of win/lossprediction of cricket, recall resembles what proportion of the total wins were predicted correctly."
"In the context of the above confusion matrix for the cricket match win prediction problem,"
"In context of the above confusion matrix for the cricket match win prediction problem,"
"As a combination of multiple measures into one, F-score gives the right measure using which performance of different models can be compared. However, one assumption the calculation is based on is that precision and recall have equal weight, which may not always be true in reality. In certain problems, the disease prediction problems, e.g., precision may"
"As we have seen till now, though accuracy is the most popular measure, there are quite a number of other measures to evaluate the performance of a supervised learning model.However, visualization is an easier and more effective way to understand the model performance. It also helps in comparing the efficiency of two models."
"Receiver Operating Characteristic (ROC) curve helps in visualizing the performance of a classification model. It shows the efficiency of a model in the detection of true positives while avoiding the occurrence of false positives. To refresh our memory, true positives are those cases where the model has correctly classified data instances as the class of interest. For example, the model has correctly classified the tumours as malignant, in case of a tumour malignancy prediction problem. On the other hand, FPs are those cases where the model incorrectly classified data instances as the class of interest.Using the same example, in this case, the model has incorrectly classified the tumours as malignant, i.e. tumours which are actually benign have been classified as malignant."
"In the ROC curve, the FP rate is plotted (in the horizontal axis) against true positive rate (in the vertical axis) at different classification thresholds. If we assume a lower value of classification threshold, the model classifies more items as positive. Hence, the values of both False Positives and True Positives increase. The area under curve (AUC) value, as shown in figure 3.8a , is the area of the two-dimensional space under the curve extending from (0, 0) to (1, 1), where each point on the curve gives a set of true and false positive values at a specific classification threshold. This curve gives an indication of the predictive quality of a model. AUC value ranges from 0 to 1, with an AUC of less than 0.5 indicating that the classifier has no predictive ability. Figure 3.8b shows the curves of two classifiers – classifier 1 and classifier 2."
"Quite obviously, the AUC of classifier 1 is more than the AUC"
"A well-fitted regression model churns out predicted values close to actual values. Hence, a regression model which ensures that the difference between predicted and actual values is low can be considered as a good model. Figure 3.9 represents a very simple problem of real estate value prediction solved using linear regression model. If ‘area’ is the predictor variable (say x) and ‘value’ is the target variable (say y), the linear regression model can be represented in the form:"
"‌For a certain value of x, say x̂, the value of y is predicted as ŷ whereas the actual value of y is Y (say). The distance between the actual value and the fitted or predicted value, i.e. ŷ is known as residual. The regression model can be considered to be fitted well if the difference between actual and predicted value, i.e. the residual value is less."
Sum of Squares Total (SST) = squared differences of each observation from the overall mean = where y̅ is the
"‌Clustering algorithms try to reveal natural groupings amongst the data sets. However, it is quite tricky to evaluate the performance of a clustering algorithm. Clustering, by nature, is very subjective and whether the cluster is good or bad is open for interpretations. It was noted, ‘clustering is in the eye of the beholder’. This stems from the two inherent challenges which lie in the process of clustering:"
"It is generally not known how many clusters can be formulated from a particular data set. It is completely open-ended in most cases and provided as a user input to a clustering algorithm.Even if the number of clusters is given, the same number of clusters can be formed with different groups of data instances."
"In a more objective way, it can be said that a clustering algorithm is successful if the clusters identified using the algorithm is able to achieve the right results in the overall problem domain. For example, if clustering is applied for"
"identifying customer segments for a marketing campaign of a new product launch, the clustering can be considered successful only if the marketing campaign ends with a success,i.e. it is able to create the right brand recognition resulting in steady revenue from new product sales. However, there are couple of popular approaches which are adopted for cluster quality evaluation."
"In this approach, the cluster is assessed based on the underlying data that was clustered. The internal evaluation methods generally measure cluster quality based on homogeneity of data belonging to the same cluster and heterogeneity of data belonging to different clusters. The homogeneity/heterogeneity is decided by some similarity measure. For example, silhouette coefficient, which is one of the most popular internal evaluation methods, uses distance (Euclidean or Manhattan distances most commonly used) between data elements as a similarity measure. The value of silhouette width ranges between –1 and +1, with a high value indicating high intra-cluster homogeneity and inter-cluster heterogeneity."
"For a data set clustered into ‘k’ clusters, silhouette width is calculated as:"
"the i th data element in cluster 1, assuming there are n1 data elements in"
"‌In the same way, let’s calculate the distance of an arbitrary data element ‘i’ in cluster 1 with the different data elements from another cluster, say cluster 4 and take an average of all those distances. Hence,"
"where n4 is the total number of elements in cluster 4. In the same way, we"
"In this approach, class label is known for the data set subjected to clustering. However, quite obviously, the known class labels are not a part of the data used in clustering. The cluster algorithm is assessed based on how close the"
"results are compared to those known class labels. For example, purity is one of the most popular measures of cluster algorithms – evaluates the extent to which clusters contain a single class."
"For a data set having ‘n’ data instances and ‘c’ known class labels which generates ‘k’ clusters, purity is measured as:"
"Now we have almost reached the end of the journey of building learning models. We have got some idea about what modelling is, how to approach about it to solve a learning problem and how to measure the success of our model. Now comes a million dollar question. Can we improve the performance of our model? If so, then what are the levers for improving the performance? In fact, even before that comes the question of model selection – which model should be selected for which machine learning task? We have already discussed earlier that the model selection is done one several aspects:"
"Type of learning the task in hand, i.e. supervised or unsupervised"
"Above all, experience in working with different models to solve problems of diverse domains"
"So, assuming that the model selection is done, what are the different avenues to improve the performance of models?"
"One effective way to improve model performance is by tuning model parameter. Model parameter tuning is the process of adjusting the model fitting options. For example, in the popular classification model k-Nearest Neighbour (kNN), using different values of ‘k’ or the number of nearest"
"neighbours to be considered, the model can be tuned. In the same way, a number of hidden layers can be adjusted to tune the performance in neural networks model. Most machine learning models have at least one parameter which can be tuned."
"As an alternate approach of increasing the performance of one model, several models may be combined together. The models in such combination are complimentary to each other,"
i.e. one model may learn one type data sets well while struggle with another type of data set. Another model may perform well with the data set which the first one struggled with. This approach of combining different models with diverse strengths is known as ensemble (depicted in Figure 3.11 ). Ensemble helps in averaging out biases of the different underlying models and also reducing the variance. Ensemble methods combine weaker learners to create stronger ones. A performance boost can be expected even if models are built as usual and then ensembled. Following are the typical steps in ensemble process:
"For diversifying the models generated, the training data subset can be varied using the allocation function. Sampling techniques like bootstrapping may be used to generate unique training data sets.Alternatively, the same training data may be used but the models combined are quite varying, e.g, SVM, neural network, kNN, etc."
"The outputs from the different models are combined using a combination function. A very simple strategy of combining, say in case of a prediction task using ensemble, can be majority voting of the different models combined. For example, 3 out of 5 classes predict ‘win’ and 2 predict ‘loss’ – then the final outcome of the ensemble using majority vote would be a ‘win’."
‌One of the earliest and most popular ensemble models is bootstrap aggregating or bagging. Bagging uses bootstrap sampling method (refer section 3.3.3) to generate multiple training data sets. These training data sets are used to generate (or train) a set of models using the same learning algorithm.
"Then the outcomes of the models are combined by majority voting (classification) or by average (regression). Bagging is a very simple ensemble technique which can perform really well for unstable learners like a decision tree, in which a slight change in data can impact the outcome of a model significantly."
"Just like bagging, boosting is another key ensemble-based technique. In this type of ensemble, weaker learning models are trained on resampled data and the outcomes are combined using a weighted voting approach based on the performance of different models. Adaptive boosting or AdaBoost is a special"
variant of boosting algorithm. It is based on the idea of generating weak learners and slowly learning
"In this chapter, you have been introduced to the crux of machine learning, i.e. modelling. Thorough understanding of the technical aspects elaborated in this chapter is extremely crucial for the success of any machine learning project."
"For example, the first dilemma comes about which model to select. Again, in case of supervised learning, how can we deal with the unavailability of sufficient training data. In the same way, once the model is trained in case of supervised learning or the grouping is done in case of clustering, how we can understand whether the model training (for supervised) or grouping done (for unsupervised) is good or bad. All these and more have been addressed as a part of this chapter."
‘k’ folds. The value of ‘k’ in k-fold cross-validation can be set to any number. Two extremely popular approaches are:
"Bootstrap sampling or simply bootstrapping is a popular way to identify training and test data sets from the input data set. It uses the technique of Simple Random Sampling with Replacement (SRSWR). Bootstrapping randomly picks data instances from the input data set, with the possibility of the same data instance to be picked multiple times."
"Fitness of a target function approximated by a learning algorithm determines how correctly it is able to predict the value or class for a set of data it has never seen.If the target function is kept too simple, it may not be able to capture the essential nuances and represent the underlying data well. This known as underfitting."
"Overfitting refers to a situation where the model has been designed in such a way that it emulates the training data too closely. In such a case, any specific nuance in the training data, like noise or outliers, gets embedded in the model. It adversely impacts the performance of the model on the test data."
"In supervised learning, the value predicted by the learning model built based on the training data may differ from the actual class value. This error in learning can be of two types – errors due to ‘bias’ and error due to ‘variance’. Errors due to bias arise from simplifying assumptions made by the model whereas errors due to variance occur from over-aligning the model with the training data sets."
"For any classification model, model accuracy is the primary indicator of the goodness of the model. It is given by a total number of correct classifications (either as the class of interest, or as not the class of interest) divided by total number of classifications done. There are other indicators like error rate, sensitivity, specificity, precision and recall."
"For unsupervised learning (clustering), silhouette coefficient (or width) is one of the most popular internal evaluation methods. A high value of silhouette width indicates high intra-cluster homogeneity and inter-cluster heterogeneity. In case, class label is known for the data set, purity is another popular measure which evaluates the extent to which clusters contain a single class."
"Model parameter tuning is the process of adjusting the model fitting options. For example, in the popular classification model k-Nearest Neighbour (kNN), using different values of ‘k’ or the number of nearest neighbours to be considered, the model can be tuned.The approach of combining different models with diverse strengths is known as ensemble. Ensemble methods combine weaker learners to create stronger ones."
One of the earliest and most popular ensemble models is bootstrap aggregating or bagging. Bagging uses bootstrapping to generate multiple training data sets. These training data sets are used to generate a set of models using the same learning algorithm.
"Just like bagging, boosting is another key ensemble-based technique. In boosting, weaker learning models are trained on resampled data and the outcomes are combined using a weighted voting approach based on the performance of different models.Adaptive boosting or AdaBoost is a special variant of boosting algorithm."
MULTIPLE-CHOICE QUESTIONS (1 MARK QUESTIONS):Structured representation of raw input data to meaningful       is called a model.
none of the aboveFor supervised learning we have       model.
Which of the following measure is not used for a classification model?
Which of the following is a performance measure for regression?
"Out of 200 emails, a classification model correctly predicted 150 spam emails and 30 ham emails. What is the accuracy of the model?"
"Out of 200 emails, a classification model correctly predicted 150 spam emails and 30 ham emails. What is the error rate of the model?"
There is no one model that works best for every machine learning problem. This is stated as
"Explain “No Free Lunch” theorem in context of machine learning.Explain, in details, the process of K-fold cross-validation."
"Explain the bootstrap sampling. Why is it needed?Why do we need to calculate Kappa value for a classification model? Show, with a sample set of data, how to calculate Kappa value of a classification model."
What is the main purpose of a descriptive model? State some real-world problems solved using descriptive models.Explain the process of evaluating a linear regression model.
"What are predictive models? What are descriptive models? Give examples of both types of models. Explain the difference between these types of models.Explain, in details, the process of evaluating the performance of a classification model. Explain the different parameters of measurement."
1. What is underfitting in context of machine learning models? What is the major cause of underfitting?
"Explain bias-variance trade-off in context of model fitting.Can the performance of a learning model be improved? If yes, explain how."
How would you evaluate the success of an unsupervised learning model? What are the most popular measures of performance for an unsupervised learning model?Is there a way to use a classification model for a numerical data or a regression model on a categorical data? Explain your answer.
Describe the process of predictive modelling for numerical values. How is it different from predictive modelling for categorical values?
"While predicting malignancy of tumour of a set of patients using a classification model, following are the data recorded:"
"Calculate the error rate, Kappa value, sensitivity, precision, and"
"In the last three chapters, you have been introduced to the basic concepts of machine learning. Also, the process to start modelling a problem has also been discussed in details. With this context in mind, in this chapter, we will introduce you to another very important aspect of machine learning, that is feature engineering. Though not a core part of the machine learning processes, feature engineering is a critical allied task that we need to perform to make learning more effective. It has three key components – feature construction, feature selection, and feature transformation, each of which will be covered in details in this chapter."
"In the last three chapters, we had a jumpstart to the machine learning process. We first started with what human learning is and how the different types of machine learning emulate the aspects of human learning. We had a detailed view of the"
"different types of problem that can be solved using machine learning techniques. Before applying machine learning to solve the problems, there are certain preparatory steps. These preparatory steps have been covered in details. After that, we have done a step-by-step navigation of the different activities of modelling a problem using machine learning. Modelling alone doesn’t help us to realize the effectiveness of"
"machine learning as a problem- solving tool. So we also learnt how to measure the effectiveness of machine learning models in solving problems. In case a specific model is not effective, we can use different levers to boost the effectiveness. Those levers of boosting the model performance were also covered."
"A feature is an attribute of a data set that is used in a machine learning process. There is a view amongst certain machine learning practitioners that only those attributes which are meaningful to a machine learning problem are to be called as features, but this view has to be taken with a pinch of salt. In fact, selection of the subset of features which are meaningful for machine learning is a sub-area of feature engineering which draws a lot of research interest. The features in a data set are also called its dimensions. So a data set having ‘n’ features is called an n-dimensional data set."
"Let’s take the example of a famous machine learning data set, Iris, introduced by the British statistician and biologist Ronald Fisher, partly shown in Figure 4.1. It has five attributes or features namely Sepal.Length, Sepal.Width, Petal.Length, Petal. Width and Species. Out of these, the feature ‘Species’ represent the class variable and the remaining features are the predictor variables. It is a five-dimensional data set."
‌Feature engineering refers to the process of translating a data set into features such that these features are able to represent the data set more effectively and result in a better learning performance.
"As we know already, feature engineering is an important pre-processing step for machine learning. It has two major elements:"
feature extractionBoth are sometimes known as feature discovery.
"Feature construction process discovers missing information about the relationships between features and augments the feature space by creating additional features. Hence, if there are ‘n’ features or dimensions in a data set, after feature construction ‘m’ more features or dimensions may get added. So at the end, the data set will become ‘n + m’ dimensional."
"Unlike feature transformation, in case of feature subset selection (or simply feature selection) no new feature is generated. The objective of feature selection is to derive a subset of features from the full feature set which is most meaningful in the context of a specific machine learning problem. So, essentially the job of feature selection is to derive a subset Fj (F1, F2, …, Fm) of Fi (F1, F2, …, Fn), where m < n, such that Fj is most meaningful and gets the best result for a machine learning problem. We will discuss these concepts in detail in the next section."
"It is quite evident that feature construction expands the feature space, while feature extraction and feature selection reduces the feature space."
"‌Engineering a good feature space is a crucial prerequisite for the success of any machine learning model. However, often it is not clear which feature is more important. For that reason, all available attributes of the data set are used as features and the problem of identifying the important features is left to the learning model. This is definitely not a feasible approach, particularly for certain domains e.g. medical image classification, text categorization, etc. In case a model has to be trained to classify a document as spam or non-spam, we can represent a document as a bag of words. Then the feature space will contain all unique words occurring across all documents. This will easily be a feature space of a few hundred thousand features. If we start including bigrams or trigrams along with words, the count of features will run in millions. To deal with this problem, feature transformation comes into play. Feature transformation is used as an effective tool for dimensionality reduction and hence for boosting learning model performance. Broadly, there are two distinct goals of feature transformation:"
Achieving best reconstruction of the original features in the data set Achieving highest efficiency in the learning task
"In the field of natural language processing, ‘n-gram’ is a contiguous set of n items for example words in a text block or document. Using numerical prefixes, n-gram of size 1 is called unigram (i.e. a single word), size 2 is called bigram (i.e. a two-word phrase), size 3 is called trigram (i.e. a three-word phrase) etc."
"Feature construction involves transforming a given set of input features to generate a new set of more powerful features. To understand more clearly, let’s take the example of a real estate data set having details of all apartments sold in a specific region."
"The data set has three features – apartment length, apartment breadth, and price of the apartment. If it is used as an input to a regression problem, such data can be training data for the regression model. So given the training data, the model should be able to predict the price of an apartment whose price is not known or which has just come up for sale. However, instead of using length and breadth of the apartment as a predictor, it is much convenient and makes more sense to use the area of the apartment, which is not an existing feature of the data set. So such a feature, namely apartment area, can be added to the data set. In other words, we transform the three- dimensional data set to a four-dimensional data set, with the newly ‘discovered’ feature apartment area being added to the original data set. This is depicted in Figure 4.2."
There are certain situations where feature construction is an essential activity before we can start with the machine learning task. These situations are
"Let’s take the example of another data set on athletes, as presented in Figure 4.3a. Say the data set has features age, city of origin, parents athlete (i.e. indicate whether any one of the parents was an athlete) and Chance of Win. The feature chance of a win is a class variable while the others are predictor variables. We know that any machine learning algorithm, whether it’s a classification algorithm (like kNN) or a regression algorithm, requires numerical figures to learn from. So there are three features – City of origin, Parents athlete, andChance of win, which are categorical in nature and cannot be used by any machine learning task."
"In this case, feature construction can be used to create new dummy features which are usable by machine learning algorithms. Since the feature ‘City of origin’ has three unique values namely City A, City B, and City C, three dummy features namely origin_ city_A, origin_city_B, and origin_city_C is created. In the same way, dummy features parents_athlete_Y and parents_athlete_N are created for feature ‘Parents athlete’ and win_chance_Y and win_chance_N are created for feature ‘Chance of win’. The dummy features have value 0 or 1 based on the categorical value for the original feature in that row. For example, the second row had a categorical value ‘City B’ for the feature ‘City of origin’. So, the newly created features in place of ‘City of origin’, i.e. origin_city_A, origin_city_B and origin_city_C will have values 0, 1 and 0, respectively. In the same way, parents_athlete_Y and parents_athlete_N will have values 0 and 1, respectively in row 2 as the original feature ‘Parents athlete’ had a categorical value ‘No’ in row 2. The entire set of transformation for athletes’ data set is shown in Figure 4.3b."
"However, examining closely, we see that the features ‘Parents athlete’ and ‘Chance of win’ in the original data set can have two values only. So creating two features from them is a kind of duplication, since the value of one feature can be decided from the value of the other. To avoid this duplication, we can just leave one feature and eliminate the other, as shown in Figure 4.3c."
transformed to a categorical variable price-grade either as shown in Figure 4.5b or as shown in Figure 4.5c.
"‌In the current world, text is arguably the most predominant medium of communication. Whether we think about social networks like Facebook or micro-blogging channels like Twitter or emails or short messaging services such as Whatsapp, text plays a major role in the flow of information. Hence, text mining is an important area of research – not only for technology practitioners but also for industry practitioners. However, making sense of text data, due to the inherent unstructured nature of the data, is not so straightforward. In the first place, the text data chunks that we can think about do not have readily available features, like structured data sets, on which machine learning tasks can be executed. All machine learning models need numerical data as input. So the text data in the data sets need to be transformed into numerical features."
"Text data, or corpus which is the more popular keyword, is converted to a numerical representation following a process is known as vectorization. In this process, word occurrences in all documents belonging to the corpus are consolidated in the form of bag-of-words. There are three major steps that are followed:"
"In order to tokenize a corpus, the blank spaces and punctuations are used as delimiters to separate out the words, or tokens. Then the number of occurrences of each token is counted, for each document. Lastly, tokens are weighted with"
reducing importance when they occur in the majority of the documents. A matrix is then formed with each token representing a column and a specific document of the corpus representing each row. Each cell contains the count of occurrence of the token in a specific document. This matrix is known as a document-term matrix (also known as a term- document matrix). Figure 4.6 represents a typical document- term matrix which forms an input to a machine learning model.
"For nominal features: Cartesian product, M of N, etc.For numerical features: Min, Max, Addition, Subtraction, Multiplication, Division, Average, Equivalence, Inequality, etc."
"Let’s take an example and try to understand. Say, we have a data set with a feature set Fi (F1, F2, …, Fn). After feature extraction using a mapping function f (F1, F2, …, Fn) say, weand m < n. For example, . This is depicted in Figure 4.7."
‌Let’s discuss the most popular feature extraction algorithms used in machine learning:
success of machine learning lies in the fact that the features are less in number as well as the similarity between each other is very less. This is the main guiding philosophy of principal component analysis (PCA) technique of feature extraction.
"In PCA, a new set of features are extracted from the original features which are quite dissimilar in nature. So an n- dimensional feature space gets transformed to an m- dimensional feature space, where the dimensions are orthogonal to each other, i.e. completely independent of each other. To understand the concept of orthogonality, we have to step back and do a bit of dip dive into vector space concept in linear algebra."
"where, ai represents ‘n’ scalars and ui represents the basis vectors. Basis vectors are orthogonal to each other."
Orthogonality of vectors in n-dimensional vector space can be thought of an extension of the vectors being perpendicular in a two-dimensional vector space. Two orthogonal vectors are completely unrelated or independent of each other. So the transformation of a set of vectors to the corresponding set of
basis vectors such that each vector in the original set can be expressed as a linear combination of basis vectors helps in decomposing the vectors to a number of independent components.
"Now, let’s extend this notion to the feature space of a data set. The feature vector can be transformed to a vector space of the basis vectors which are termed as principal components."
"These principal components, just like the basis vectors, are orthogonal to each other. So a set of feature vectors which may have similarity with each other is transformed to a set of principal components which are completely unrelated."
"However, the principal components capture the variability of the original feature space. Also, the number of principal component derived, much like the basis vectors, is much smaller than the original set of features."
"‌The objective of PCA is to make the transformation in such a way thatThe new features are distinct, i.e. the covariance between the new features, i.e. the principal components is 0."
"The principal components are generated in order of the variability in the data that it captures. Hence, the first principal component should capture the maximum variability, the second principal component should capture the next highest variability etc.The sum of variance of the new features or the principal components should be equal to the sum of variance of the original features."
"First, calculate the covariance matrix of a data set.Then, calculate the eigenvalues of the covariance matrix."
The eigenvector having highest eigenvalue represents the direction in which there is the highest variance. So this will help in identifying thefirst principal component.
"The eigenvector having the next highest eigenvalue represents the direction in which data has the highest remaining variance and also orthogonal to the first direction. So this helps in identifying the second principal component.Like this, identify the top ‘k’ eigenvectors having top ‘k’ eigenvalues so as to get the ‘k’ principal components."
"where, U and V are orthonormal matrices, U is an m × m unitary matrix, V is an n × n unitary matrix and Σ is an m × n rectangular diagonal matrix. The diagonal entries of Σ are known as singular values of matrix A. The columns of U and V are called the left-singular and right-singular vectors of matrix A, respectively."
"SVD is generally used in PCA, once the mean of each variable has been removed. Since it is not always advisable to remove the mean of a data attribute, especially when the data set is sparse (as in case of text data), SVD is a good choice for dimensionality reduction in those situations."
"Patterns among the instances are captured by the left-singular, i.e. the columns of U.‌Larger a singular value, larger is the part of the matrix A that it accounts for and its associated vectors."
New data matrix with ‘k’ attributes is obtained using the equation
"Thus, the dimensionality gets reduced to kSVD is often used in the context of text data."
"Linear discriminant analysis (LDA) is another commonly used feature extraction technique like PCA or SVD. The objective of LDA is similar to the sense that it intends to transform a data set into a lower dimensional feature space. However, unlike PCA, the focus of LDA is not to capture the data set variability. Instead, LDA focuses on class separability, i.e. separating the features based on class separability so as to avoid over-fitting of the machine learning model."
Calculate the mean vectors for the individual classes.Calculate intra-class and inter-class scatter matrices.
the intra-class scatter matrix and SB is the inter-class scatter matrix
"where, mi is the sample mean for each class, m is the overall mean of the data set, Ni is the sample size of each class"
"‌Feature selection is arguably the most critical pre-processing activity in any machine learning project. It intends to select a subset of system attributes or features which makes a most meaningful contribution in a machine learning activity. Let’s quickly discuss a practical example to understand the philosophy behind feature selection. Say we are trying to predict the weight of students based on past information about similar students, which is captured in a ‘student weight’ data set. The student weight data set has features such as Roll Number, Age, Height, and Weight. We can well understand that roll number can have no bearing, whatsoever, in predicting student weight. So we can eliminate the feature roll number and build a feature subset to be considered in this machine learning problem. The subset of features is expected to give better results than the full set. The same has been depicted in Figure 4.8."
"But before we go forward with more detailed discussion on feature selection, let’s try to understand the issues which have"
"‘High-dimensional’ refers to the high number of variables or attributes or features present in certain data sets, more so in the domains like DNA analysis, geographic information systems (GIS), social networking, etc. The high-dimensional spaces often have hundreds or thousands of dimensions or attributes, e.g. DNA microarray data can have up to 450,000 variables (gene probes)."
"Alongside, two new application domains have seen drastic development. One is that of biomedical research, which includes gene selection from microarray data. The other one is text categorization which deals with huge volumes of text data from social networking sites, emails, etc. The first domain, i.e. biomedical research generates data sets having a number of features in the range of a few tens of thousands. The text data generated from different sources also have extremely high dimensions. In a large document corpus having few thousand documents embedded, the number of unique word tokens"
"‌which represent the feature of the text data set, can also be in the range of a few tens of thousands. To get insight from such high-dimensional data may be a big challenge for any machine learning algorithm. On one hand, very high quantity of computational resources and high amount of time will be required. On the other hand the performance of the model – both for supervised and unsupervised machine learning task, also degrades sharply due to unnecessary noise in the data."
"Also, a model built on an extremely high number of features may be very difficult to understand. For this reason, it is necessary to take a subset of the features instead of the full set."
‌Key drivers of feature selection – feature relevance and redundancy‌
"In supervised learning, the input data set which is the training data set, has a class label attached. A model is inducted based on the training data set – so that the inducted model can assign class labels to new, unlabelled data. Each of the predictor variables, is expected to contribute information to decide the value of the class label. In case a variable is not contributing any information, it is said to be irrelevant. In case the information contribution for prediction is very little, the variable is said to be weakly relevant. Remaining variables, which make a significant contribution to the prediction task are said to be strongly relevant variables."
"To get a perspective, we can think of the simple example of the student data set that we discussed at the beginning of this section. Roll number of a student doesn’t contribute any significant information in predicting what the Weight of a student would be. Similarly, if we are trying to group together students with similar academic capabilities, Roll number can really not contribute any information whatsoever. So, in context of the supervised task of predicting student Weight or the unsupervised task of grouping students with similar academic merit, the variable Roll number is quite irrelevant."
Any feature which is irrelevant in the context of a machine learning task is a candidate for rejection when we are selecting a subset of features. We can consider whether the weakly relevant features are to be rejected or not on a case-to-case basis.
"‌A feature may contribute information which is similar to the information contributed by one or more other features. For example, in the weight prediction problem referred earlier in the section, both the features Age and Height contribute similar information. This is because with an increase in Age,"
"Weight is expected to increase. Similarly, with the increase of Height also Weight is expected to increase. Also, Age and Height increase with each other. So, in context of the Weight prediction problem, Age and Height contribute similar information. In other words, irrespective of whether the feature height is present as a part of the feature subset, the learning model will give almost same results. In the same way, without age being part of the predictor variables, the outcome of the learning model will be more or less same. In this kind of a situation when one feature is similar to another feature, the feature is said to be potentially redundant in the context of the learning problem."
All features having potential redundancy are candidates for rejection in the final feature subset. Only a small number of representative features out of a set of potentially redundant features are considered for being a part of the final feature subset.
"So, in a nutshell, the main objective of feature selection is to remove all features which are irrelevant and take a representative subset of the features which are potentially redundant. This leads to a meaningful feature subset in context of a specific learning task."
"Now, the question is how to find out which of the features are irrelevant or which features have potential redundancy. For that multiple measures are being used, some of which have been covered in the next sub-section."
"As mentioned earlier, feature relevance is to be gauged by the amount of information contributed by a feature. For supervised learning, mutual information is considered as a good measure of information contribution of a feature to decide the value of the class label. That’s why it is a good indicator of the relevance of a feature with respect to the class variable. Higher the value of mutual information of a feature, more relevant is that feature. Mutual information can be calculated as follows:"
"and K = number of classes, C = class variable, f = feature set that take discrete values."
"In case of unsupervised learning, there is no class variable. Hence, feature-to-class mutual information cannot be used to measure the information contribution of the features. In case of unsupervised learning, the entropy of the set of features without one feature at a time is calculated for all the features."
‌information gain from a feature and top ‘β’ percentage (value of ‘β’ is a design parameter of the algorithm) of features are selected as relevant features. The entropy of a feature f is calculated using Shannon’s formula below:
"is used only for features that take discrete values. Forcontinuous features, it should be replaced by discretization performed first to estimate probabilities p(f = x)."
"Feature redundancy, as we have already discussed, is based on similar information contribution by multiple features. There are multiple measures of similarity of information contribution, salient ones being"
"Correlation values range between +1 and –1. A correlation of 1 (+ / –) indicates perfect correlation, i.e. the two features having a perfect linear relationship. In case the correlation is 0, then the features seem to have no linear relationship.Generally, for all feature selection problems, a threshold value is adopted to decide whether two features have adequate similarity or not."
"‌where F1 and F2 are features of an n-dimensional data set. Refer to the Figure 4.9. The data set has two features, aptitude (F1) and communication (F2) under consideration. The Euclidean distance between the features has been calculated using the formula provided above."
Minkowski distance takes the form of Euclidean distance (also called L2 norm) when r = 2.
"At r = 1, it takes the form of Manhattan distance (also called L1 norm), as shown below:"
"A specific example of Manhattan distance, used more frequently to calculate the distance between binary vectors is the Hamming distance. For example, the Hamming distancebetween two vectors 01101011 and 11001001 is 3, as illustrated in Figure 4.10a."
"For two features having binary values, Jaccard index is measured as"
n01 = number of cases where the feature 1 has value 0 and feature 2 has value 1
n10 = number of cases where the feature 1 has value 1 and feature 2 has value 0
"Let’s consider two features F1 and F2 having values (0, 1, 1, 0, 1, 0, 1, 0) and (1, 1, 0, 0, 1, 0, 0, 0). Figure 4.10b shows the identification of the values of n11, n01 and n10. As shown, the cases where both the values are 0 have been left out without border – as an indication of the fact that they will be excluded in the calculation of Jaccard coefficient."
"where, n11 = number of cases where both the features have value 1"
n01 = number of cases where the feature 1 has value 0 and feature 2 has value 1
n10 = number of cases where the feature 1 has value 1 and feature 2 has value 0
"One more measure of similarity using similarity coefficient calculation is Cosine Similarity. Let’s take the example of a typical text classification problem. The text corpus needs to be first transformed into features with a word token being a feature and the number of times the word occurs in a document comes as a value in each row. There are thousands of features in such a text data set. However, the data set is sparse in nature as only a few words do appear in a document, and hence in a row of the data set. So each row has very few non-zero values. However, the non-zero values can be anything integer value as the same word may occur any number of times. Also, considering the sparsity of the data set, the 0-0 matches (which obviously is going to be pretty high) need to be ignored. Cosine similarity which is one of the most popular measures in text classification is calculated as:"
"Let’s calculate the cosine similarity of x and y, where x = (2, 4, 0, 0, 2, 1, 3, 0, 0) and y = (2, 1, 0, 0, 3, 2, 1, 0, 1)."
"Feature selection is the process of selecting a subset of features in a data set. As depicted in Figure 4.12, a typical feature selection process consists of four steps:"
"Subset generation, which is the first step of any feature selection algorithm, is a search procedure which ideally should produce all possible candidate subsets. However, for an n- dimensional data set, 2n subsets can be generated. So, as the value of ‘n’ becomes high, finding an optimal subset from all the 2n candidate subsets becomes intractable. For that reason,"
"different approximate search strategies are employed to find candidate subsets for evaluation. On one hand, the search may start with an empty set and keep adding features. This search strategy is termed as a sequential forward selection. On the other hand, a search may start with a full set and successively remove features. This strategy is termed as sequential backward elimination. In certain cases, search start with both ends and add and remove features simultaneously. This strategy is termed as a bi-directional selection."
"Each candidate subset is then evaluated and compared with the previous best performing subset based on certain evaluation criterion. If the new subset performs better, it replaces the previous one."
‌This cycle of subset generation and evaluation continues till a pre-defined stopping criterion is fulfilled. Some commonly used stopping criteria are
some given bound (e.g. a specified number of iterations) is reached
subsequent addition (or deletion) of the feature is not producing a better subset
a sufficiently good subset (e.g. a subset having better classification accuracy than the existing benchmark) is selected
"Then the selected best subset is validated either against prior benchmarks or by experiments using real-life or synthetic but authentic data sets. In case of supervised learning, the accuracy of the learning model may be the performance parameter considered for validation. The accuracy of the model using the subset derived is compared against the model accuracy of the subset derived using some other benchmark algorithm. In case of unsupervised, the cluster quality may be the parameter for validation."
"In the filter approach (as depicted in Fig. 4.13), the feature subset is selected based on statistical measures done to assess the merits of the features from the data perspective. No learning algorithm is employed to evaluate the goodness of the feature selected. Some of the common statistical tests conducted on features as a part of filter approach are – Pearson’s correlation, information gain, Fisher score, analysis of variance (ANOVA), Chi-Square, etc."
"In the wrapper approach (as depicted in Fig. 4.14), identification of best feature subset is done using the induction algorithm as a black box. The feature selection algorithm searches for a good feature subset using the induction algorithm itself as a part of the evaluation function. Since for every candidate subset, the learning model is trained and the result is evaluated by running the learning algorithm, wrapper approach is computationally very expensive. However, the performance is generally superior compared to filter approach."
‌SUMMARY‌A feature is an attribute of a data set that is used in a machine learning process.
"Feature engineering is an important pre-processing step for machine learning, having two major elements:"
Feature transformation transforms data into a new set of features which can represent the underlying machine learning problem
feature extractionFeature construction process discovers missing information about the relationships between features and augments the feature space by creating additional features.
Feature extraction is the process of extracting or creating a new set of features from the original set of features using some functional mapping. Some popular feature extraction algorithms used in machine learning:
Linear Discriminant Analysis (LDA)Feature subset selection is intended to derive a subset of features from the full feature set. No new feature is generated.
Having faster and more cost-effective (i.e. less need for computational resources) learning model
Having a better understanding of the underlying model that generated the data
Feature selection intends to remove all features which are irrelevant and take a representative subset of the features which are potentially redundant. This leads to a meaningful feature subset in context of a specific learning task.Feature relevance is indicated by the information gain from a feature measured in terms of relative entropy.
Feature redundancy is based on similar information contributed by multiple features measured by feature-to-feature:
"Distance (Minkowski distances, e.g. Manhattan, Euclidean, etc. used as most popular measures)"
"Other coefficient-based (Jaccard, SMC, Cosine similarity, etc.)"
MULTIPLE-CHOICE QUESTIONS (1 MARK QUESTIONS)Engineering a good feature space is a crucial       for the success of any machine learning model.
None of the aboveFeature       involves transforming a given set of input features to generate a new set of more powerful features.
Conversion of a text corpus to a numerical representation is done usingprocess.
None of the aboveapproach uses induction algorithm for subset validation.
"EmbeddedIn feature extraction, some of the commonly used       are used for combining the original features."
"None of the aboveIn LDA, intra-class and inter-class       matrices are calculated."
None of the aboveThis approach is quite similar to wrapper approach as it also uses and inductive algorithm to evaluate the generated feature subsets.
"Hybrid approachIn       approach, identification of best feature subset is done using the induction algorithm as a black box."
SHORT-ANSWER TYPE QUESTIONS (5 MARKS QUESTIONS)What is a feature? Explain with an example.
Explain the process of encoding nominal variables.Explain the process of transforming numeric features to categorical features.
Explain the wrapper approach of feature selection. What are the merits and de-merits of this approach?
When can a feature be termed as irrelevant? How can it be measured?
When can a feature be termed as redundant? What are the measures to determine the potentially redundant features?
What are the different distance measures that can be used to determine similarity of features?
"What is feature engineering? Explain, in details, the different aspects of feature engineering?"
What is feature selection? Why is it needed? What are the different approaches of feature selection?
Explain the filter and wrapper approaches of feature selection. What are the merits and demerits of these approaches?
"Explain, with an example, the main underlying concept of feature extraction. What are the most popular algorithms for feature extraction?Explain the process of feature engineering in context of a text categorization problem."
"Why is cosine similarity a suitable measure in context of text categorization? Two rows in a document-term matrix have values - (2,"
similarity.1. How can we calculate Hamming distance? Find the Hamming distance between 10001011 and 11001111.
"Compare the Jaccard index and similarity matching coefficient of two features having values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 0, 0, 1, 1,0, 0, 1)."
What do you understand by a high-dimensional data set? Give a few practical examples? What is the challenge while applying machine learning technique on a high-dimensional data set? How can that be addressed?
Sequential forward selection vs. sequential backward elimination
"The principles of machine learning are largely dependent on effectively handling the uncertainty in data and predicting the outcome based on data in hand. All the learning processes we will be discussing in later chapters of this book expects the readers to understand the foundational rules of probability, the concept of random and continuous variables, distribution, and sampling principles and few basic principles such as central limit theorem, hypothesis testing, and Monte Carlo approximations. As these rules, theorems, and principles form the basis of learning principles, we will be discussing those in this chapter with examples and illustrations."
"‌As we discussed in previous chapters, machine learning provides us a set of methods that can automatically detect patterns in data, and then can be used to uncover patterns to predict future data, or to perform other kinds of decision"
"making under uncertainty. The best way to perform such activities on top of huge data set known as big data is to use the tools of probability theory because probability theory can be applied to any situation involving uncertainty. In machine learning there may be uncertainties in different forms like arriving at the best prediction of future given the past data, arriving at the best model based on certain data, arriving at the confidence level while predicting the future outcome based on past data, etc. The probabilistic approach used in machine learning is closely related to the field of statistics, but the emphasis is in a different direction as we see in this chapter. In this chapter we will discuss the tools, equations, and models of probability that are useful for machine learning domain."
"In machine learning, we train the system by using a limited data set called ‘training data’ and based on the confidence level of the training data we expect the machine learning algorithm to depict the behaviour of the larger set of actual data. If we have observation on a subset of events, called ‘sample’, then there will be some uncertainty in attributing the sample results to the whole set or population. So, the question was how a limited knowledge of a sample set can be used to predict the behaviour of a real set with some confidence. It was realized by mathematicians that even if some knowledge is based on a sample, if we know the amount of uncertainty related to it, then it can be used in an optimum way without causing loss of knowledge. Refer Figure 5.1"
"Probability theory provides a mathematical foundation for quantifying this uncertainty of the knowledge. As the knowledge about the training data comes in the form of interdependent feature sets, the conditional probability theories (especially the Bayes theorem), discussed later in this chapter form the basis for deriving required confidence level of the training data."
"Different distribution principles discussed in this chapter create the view of how the data set that we will be dealing with in machine learning can behave, in terms of their feature distributions. It is important to understand the mathematical function behind each of these distributions so that we can understand how the data is spread out from its average value – denoted by the mean and the variance. While choosing the samples from these distribution sets we should be able to calculate to what extent the sample is representing the actual behaviour of the full data set. These along with the test of hypothesis principles build the basis for finding out the uncertainty in the training data set to represent the actual data set which is the fundamental principle of machine learning."
‌CONCEPT OF PROBABILITY – FREQUENTIST AND BAYESIAN INTERPRETATION‌
rather than repeated trials. This is called the Bayesian interpretation of probability. Taking the same example of coin flipping is interpreted as – the coin is equally likely to land heads or tails when we flip the coin next.
"The reason the Bayesian interpretation can be used to model the uncertainty of events is that it does not expect the long run frequencies of the events to happen. For example, if we have to compute the probability of Brazil winning 2018 football world cup final, that event can happen only once and can’t be repeated over and over again to calculate its probability. But still, we should be able to quantify the uncertainty about the event and which is only possible if we interpret probability the Bayesian way. To give some more machine learning oriented examples, we are starting a new software implementation project for a large customer and want to compute the probability of this project getting into customer escalation based on data from similar projects in the past, or we want to compute the probability of a tumor to be malignant or not based on the probability distribution of such cases among the patients of similar profile. In all these cases it is not possible to do a repeated trial of the event and the Bayesian concept is valid for computing the uncertainty. So, in this book, we will focus on the Bayesian interpretation to develop our machine learning models. The basic formulae of probability remain the same anyway irrespective of whatever interpretation we adopt."
"We will briefly touch upon the basic probability theory in this section just as a refresher so that we can move to the building blocks of machine learning way of use of probability. As discussed in previous chapters, the basic concept of machine learning is that we want to have a limited set of ‘Training’ data"
that we use as a representative of a large set of Actual data and through probability distribution we try to find out how an event which is matching with the training data can represent the outcome with some confidence.Let us introduce few notations that will be used throughout this book.
"p(A) denotes the probability that the event A is true. For example, A might be the logical statement ‘Brazil is going to win the next football world cup final’. The expression 0 ≤ p(A)"
"≤ 1 denotes that the probability of this event happening lies between 0 and 1, where p(A) = 0 means the event will definitely not happen, and p(A) = 1 means the event will"
definitely happen. The notation p(A̅) denotes the probability of
"‌The probability of selecting an event A, from a sample size of X is defined asp(A) = , where n is the number of times the instance of event A is present in the sample of size X."
"Two events A and B are called mutually exclusive if they can’t happen together. For example, England winning the Football World Cup 2018 and Brazil winning the Football World Cup 2018 are two mutually exclusive events and can’t happen together. For any two events, A and B, we define the probability of A or B as"
"summing up the all probable states of B gives the total probability formulae, which is also called sum rule or the rule of total probability."
This formula can be extended for the countably infinite number of events in the set and chain rule of probability can be derived if the product rule is applied multiple times as
"We define the conditional probability of event A, given that event B is true, as follows:"
"where, p(A, B) is the joint probability of A and B and can also be denoted as p(A n B)"
which is the probability of both the parts being found defective.
"The Bayes rule, also known as Bayes Theorem, can be derived by combining the definition of conditional probability with the product and sum rules, as below:"
"Let’s take an example to identify the probability of an email to be really spam based on the name of the sender. We often receive email from mail id containing junk characters or words such as bulk, mass etc. which turn out to be a spam mail. So, we want our machine learning agent to flag the spam emails for us so that we can easily delete them."
"‌Let’s also assume that we have knowledge about the reliability of the assumption that emails with sender names ‘mass’ and ‘bulk’ are spam email is 80% meaning that if some email has sender name with ‘mass’ or ‘bulk’ then the email will be spam with probability 0.8. The probability of false alarm is 10% meaning that the agent will show the positive result as spam even if the email is not a spam with a probability 0.1. Also, we have a prior knowledge that only 0.4% of the total emails received are spam.So, p(x = 1|y = 1) = 0.8, which is the probability of the flag being positive if the email is spam."
"Many times this probability is wrongly interpreted as the probability of an email being spam if the flag is positive. But that is not true, because we will then ignore the prior knowledge that only 0.4% of the emails are actually spam and also there can be a false alarm in 10% of the cases."
"Combining these terms using the Bayes rule, we can compute the correct answer as follows:"
checks before someone starts deleting such emails based on only this flag.Details of the practice application of Bayes theorem in machine learning is discussed in Chapter 6.
"X(H) = 1, X(T) = 0, which means this variable is associated with the outcome of the coin facing head.Y(H) = 0, Y(T) = 1, which means this variable is associated with the outcome of the coin facing tails."
"‌Here in the sample space S which is the outcome related to tossing the coin once, random variables represent the single- valued real function [X(()] that assigns a real number, called its value to each sample point of S. A random variable is not a variable but is a function. The sample space S is called the domain of random variable X and the collection of all the numbers, i.e. values of X((), is termed the range of the random variable."
We can define the event (X = x) where x is a fixed real number as
"Accordingly, we can define the following events for fixed numbers x, x1, and x2:"
"We will use the term cdf in this book to denote the distribution function or cumulative distribution function, which takes the form of random variable X as"
"Let us extend the concept of binary events by defining a discrete random variable X. Let X be a random variable with cdf Fx(x) and it changes values only in jumps (a countable number of them) and remains constant between the jumps thenit is called a discrete random variable. So, the definition of a discrete random variable is that its range or the set X contains a finite or countably infinite number of points."
"Let us consider that the jumps of Fx(x) of the discrete random variable X is occurring at points x1,x2,x3…., where this sequence represents a finite or countably infinite set and xi <xj if i < j. See Figure 5.2."
"The probability of the event that X = x is denoted as p(X = x), or just p(x) for short. p is called a probability mass function (pmf)."
"‌FIG. 5.2 A discrete random variable X with cdf Fx(x) = p(X ≤ x) for x = 0, 1, 2, 3, 4, and Fx(x) has jumps at x = 0, 1, 2, 3, 4‌This satisfies the properties 0 ≤ p(x) ≤ 1 and Ʃx∈X p(x) = 1."
The cumulative distribution function (cdf) Fx(x) of a discrete random variable X can be denoted as
"Refer to Figure 5.3, the pmf is defined on a finite state space X = {1, 2, 3, 4} that shows a uniform distribution with"
"‌FIG. 5.C A uniform distribution within the set {1, 2, 3, 4} where p(x = k) ="
‌Discuss few examples of discrete random variables in practical life. How do you think the behaviour is different from the continuous random variables?
We will now see how to extend probability to reason about uncertain continuous quantities. The probability that x lies in any interval a ≤ x≤ b can be computed as follows.
Define the function F(q) = p(X ≤ q). The cumulative distribution function (cdf) of random variable X can be obtained by
"which is a monotonically increasing function. Using this notation, we have"
"using the pdf of x, we can compute the probability of the continuous variable being in a finite interval as follows:"
"As the size of the interval gets smaller, it is possible to write"
mean is one of the important parameters for a probability distribution.
"Variance of a random variable X measures the spread or dispersion of X. If E(X) is the mean of the random variable X, then the variance is given by"
The difference between Probability Density Function (pdf) and the Probability Mass Function (pmf) is that the latter is associated with the continuous random variable and the former is associated with the discrete random variable.
"While pmf represents the probability that a discrete random variable is exactly equal to some value, the pdf does not represent a probability by itself. The integration of pdf over a continuous interval yields the probability."
"‌SOME COMMON DISCRETE DISTRIBUTIONS‌In this section, we will discuss some commonly used parametric distributions defined on discrete state spaces, both finite and countably infinite."
"When we have a situation where the outcome of a trial is withered ‘success’ or ‘failure’, then the behaviour of the random variable X can be represented by Bernoulli distribution. Such trials or experiments are termed as Bernoulli trials."
"So we can derive that, a random variable X is called Bernoulli random variable with parameter p when its pmf takes the form of"
"Where 0 ≤ p ≤ 1. So, using the cdf Fx(x) of Bernoulli random variable is expressed as"
"here, the probability of success is p and probability of failure is 1 – p. This is obviously just a special case of a Binomial distribution with n = 1 as we will discuss below."
"If n independent Bernoulli trials are performed and X represents the number of success in those n trials, then X is called a binomial random variable. That’s the reason a Bernoulli random variable is a special case of binomial random variable with parameters (1, p)."
is also called the binomial coefficientwhich is the number of ways to choose k items from n.
"For example, if we toss a coin n times. Let X ∈ {0, …, n} be the number of heads. If the probability of heads is p, then we say X has a binomial distribution, written as X ~Bin(n, p)."
Figure 5.4 shows the binomial distribution of n = 6 and p = 0.6
"The binomial distribution can be used to model the outcomes of coin tosses, or for experiments where the outcome can be either success or failure. But to model the outcomes of tossing a K-sided die, or for experiments where the outcome can be multiple, we can use the multinomial distribution. This is defined as: let x = (x1, …, xK) be a random vector, where xj is the number of times side j of the die occurs. Then x has the following pmf:"
"and the summation is over the set of all non-negative integers x1, x2, … . xk whose sum is n."
"‌Now consider a special case of n = 1, which is like rolling a K-sided dice once, so x will be a vector of 0s and 1s (a bit vector), in which only one bit can be turned on. This means, if the dice shows up face k, then the k’th bit will be on. We can consider x as being a scalar categorical random variable with K states or values, and x is its dummy encoding with x = [Π(x = 1), …,Π(x = K)]."
"For example, if K = 3, this states that 1, 2, and 3 can be encoded as (1, 0, 0), (0, 1, 0), and (0, 0, 1). This is also called a one-hot encoding, as we interpret that only one of the K"
"‘wires’ is ‘hot’ or on. This very common special case is known as a categorical or discrete distribution and because of the analogy with the Binomial/ Bernoulli distinction, Gustavo Lacerda suggested that this is called the multinoulli distribution."
"Poisson random variable has a wide range of application as it may be used as an approximation for binomial with parameter (n,p) when n is large and p is small and thus np is of moderate size. An example application area is, if a fax machine has a faulty transmission line, then the probability of receiving an erroneous digit within a certain page transmitted can be calculated using a Poisson random variable."
"So, a random variable X is called a Poisson random variable with parameter λ (>0) when the pmf looks like"
‌SOME COMMON CONTINUOUS DISTRIBUTIONS‌In this section we present some commonly used univariate (one-dimensional) continuous probability distributions.
"‌This is also corroborated by the fact that because X is uniformly distributed over the interval [a, b] we can expect the"
This is a useful distribution when we don’t have any prior knowledge of the actual pdf and all continuous values in the same range seems to be equally likely.
The most widely used distribution in statistics and machine learning is the Gaussian or normal distribution. Its pdf is given
Which will help us evaluate the value of Fx(x) which can be written as
"Figure 5.7 shows the normal distribution graph with mean = 4, s.d. = 12."
the notation N(μ; σ2) is used to denote that p(X = x) = N(x|μ;
"A standard normal random variable is defined as the one whose mean is 0 and variance is 1 which means Z = N(0;1). The plot for this is sometimes called the bell curve, (see Fig 5.8)It has two parameters that are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance."
"The Gaussian distribution makes the least number of assumptions, subject to the constraint of having a specified mean and variance and thus is a good default choice in many cases.Its simple mathematical form is easy to implement, but often highly effective."
Here μ is a location parameter and b >0 is a scale parameter.
"Till now we were dealing with one random variable in a sample space, but in most of the practical purposes there are two or more random variables on the same sample space. We will discuss their associated distribution and interdependence."
"‌(X, Y) is called a discrete bivariate random variable if the random variables X and Y both by themselves are discrete. Similarly, (X, Y) is called a continuous bivariate random variable if the random variables X and Y both are continuous and is called a mixed bivariate random variable if one of X and Y is discrete and the other is continuous."
The joint cumulative distribution function (or joint cdf) of X and Y is defined as:
"For certain values of x and y, if A and B are independent events of S, then"
Few important properties of joint cdf of two random variables which are similar to that of the cdf of single random variable are
"Few important properties of pXY (xi, yj) areP[(X, Y) ∈ A] = Ʃ (xi, yj) ∈ RA Ʃ pXY(xi, yj), where the summation is done over the points (xi, yj ) in the range space RA."
"‌The joint cdf of a discrete bivariate random variable (X, Y) is given by"
"is called the joint probability density function (joint pdf) of (X, Y). Thus, integrating, we get"
"In the same way, when (X,Y) is a continuous bivariate random variable and has joint pdf fXY (x, y), then the conditional cdf of Y in case X = x is defined as"
"So, if the variance is the measure of how a random variable varies with itself, then the covariance is the measure of how two random variables vary with each other."
"Covariance can be between 0 and infinity. Sometimes, it is more convenient to work with a normalized measure, because covariance alone may not have enough information about the relationship among the random variables. For example, let’s define 3 different random variables based on flipping of a coin:"
Just by looking into these random variables we can understand that they are essentially the same just a constant multiplied at their output. But the covariance of them will be very different when calculating with the equation 5.55:
"So, we should remembercorr [X, Y] = 1 if and only if Y = aX + b for some parameters a and b, i.e., if there is a linear relationship between X and Y."
"From property 2 it may seem that the correlation coefficient is related to the slope of the regression line, i.e., the coefficient a in the expression Y= aX + b. However, the regression coefficient is in fact given by a = cov[X, Y] /var [X]. A better way to interpret the correlation coefficient is as a degree of linearity."
As n → ∞ (meaning for a very large but finite set) then Zn tends to the standard normal.
"So, according to the central limit theorem, irrespective of the distribution of the individual Xi’s the distribution of the sum Sn – X1 + …Xn is approximately normal for large n. This is the very important result as whenever we need to deal with a random variable which is the sum of a large number of random variables than using central limit theorem we can assume that this sum is normally distributed."
"‌As we discussed earlier in this chapter, an important application of statistics in machine learning is how to draw a conclusion about a set or population based on the probability model of random samples of the set. For example, based on the malignancy sample test results of some random tumour cases we want to estimate the proportion of all tumours which are malignant and thus advise the doctors on the requirement or non-requirement of biopsy on each tumour case. As we can"
"understand, different random samples may give different estimates, if we can get some knowledge about the variability of all possible estimates derived from the random samples, then we should be able to arrive at reasonable conclusions."
Some of the terminologies used in this section are defined below:
"While choosing the samples from the population if each object chosen is returned to the population before the next object is chosen, then it is called the sampling with replacement. In this case, repetitions are allowed. That means, if the sample size n is chosen from the population size of N, then the number of such samples isN × N × cc. × N = Nn, because each object can be repeated."
"(A, A), (A, B), (A, C), (B, A), (B, B), (B, C), (C, A), (C, B), (C, C)"
That means the number of random samples of 2 from the population of 3 is
Nn = 32 = 9and each of the random sample has probability  of being chosen.
"In case, we don’t return the object being chosen to the population before choosing the next object, then the random sample of size n is defined as the unordered subset of n objects from the population and called sampling without replacement. The number of such samples that can be drawn from the population size of N is"
"Also, each of these 3 samples of size 2 has the probability of getting chosen as"
"Let us consider X as a random variable with mean μ and standard deviation σ from a population N. A random sample of size n, drawn without replacement will generate n values x1, x2…..,xn for X. When samples are drawn with replacement, these values are independent of each other and can be considered as values of n independent random variables X1, X2,…., Xn, each having mean µ and variance σ2. The sample mean is a random variable Ẋ as"
"for a large number of n, if X is approximately normally distributed, then X̅ will also be normally distributed."
"where, N is the size of the population and n < N. Also if X is normally distributed then X̅ also has a normal distribution."
"Now, based on the central limit theorem, if the sample size of a random variable based on a finite population is large then the sample mean is approximately normally distributed irrespective of the distribution of the population. For most of the practical applications of sampling, when the sample size is large enough (as a rule of thumb ≥ 30) the sample mean is approximately normally distributed. Also, when a random sample is drawn from a large population, it can be assumed that the values x1, x2…..,xn are independent. This assumption of independence is one of the key to application of probability theory in statistical inference. We use the terms like ‘population is much larger than the sample size’ or ‘population is large compared to its sample size’, etc. to denote that the population is large enough to make the samples independent ofeach other. In practice, if , then independence may be assumed."
"‌While dealing with random variables a common situation is when we have to make certain decisions or choices based on the observations or data which are random in nature. The solutions for dealing with these situations is called decision theory or hypothesis testing and it is a widely used process in real life situations. As we discussed earlier, the key component of machine learning is to use a sample- based training data which can be used to represent the larger set of actual data and it is important to estimate how confidently an outcome can be related to the behaviour of the training data so that the decisions on the actual data can be made. So, hypothesis testing is an integral part of machine learning."
"In terms of statistics, a hypothesis is an assumption about the probability law of the random variables. Take, e.g. a random sample (X1,….. Xn) of a random variable whose pdf on parameter m is given by f(x, m) = f(x1, x2…., xn; m). We want to test the assumption m = m0 against the assumption m = m1. In this case, the assumption m = m0 is called null hypothesis and is denoted by H0. Assumption m = m1 is called alternate hypothesis and is denoted by H1."
"A simple hypothesis is the one where all the parameters are specified with an exact value, like H0 or H1 in this case. But if the parameters don’t have an exact value, like H1:m ’ m1 then H1 is composite."
"Concept of hypothesis testing is the decision process used for validating a hypothesis. We can interpret a decision process by dividing an observation space, say R into two regions – R0 and R1. If x = (x1,…..xn) are the set of observation, then if x ∈ R0 the decision is in favor of H0and if x ∈ R1 then the decision is in favor of H1. The region R0is called acceptance region as the null hypothesis is accepted and R1 is the rejection region. There are 4 possible decisions based on the two regions in observation space:"
H0 is true; reject H0 (which means accept H1) ➔ this is an incorrect decision
H1 is true; reject H1 (which means accept H0) ➔ this is an incorrect decision
"So, we can see there is the possibility of 2 correct and 2 incorrect decisions and the corresponding actions. The erroneous decisions can be termed as:"
"Type I error: reject H0 (or accept H1) when H0 is true. The example of this situation is in a malignancy test of a tumour, a benign tumour is accepted as malignant tumour and corresponding treatment is started. This is also called Alpha error where good is interpreted as bad."
"Type II error: reject H1 (or accept H0) when H1 is true. The example of this situation is in a malignancy test of a tumour, a malignant tumour is accepted as a benign tumour and notreatment for malignancy is started. This is also called Beta error where bad is interpreted as good and can have a more devastating impact."
"‌where, Di (I = 0, 1) denotes the event that the decision is for accepting Hi. PI is also denoted by a and known as level of significance whereas PII is denoted by β and is known as the power of the test. Here, a and β are not independent of each other as they represent the probabilities of the event from same decision problem. So, normally a decrease in one type of error leads to an increase in another type of when the sample size is fixed. Though it is desirable to reduce both types of errors it is only possible by increasing the sample size. In all practical applications of hypothesis testing, each of the four possible outcomes and courses of actions are associated with relative importance or certain cost and thus the final goal can be to reduce the overall cost."
"Though we discussed the distribution functions of the random variables, in practical situations it is difficult to compute them using the change of variables formula. Monte Carlo approximation provides a simple but powerful alternative to this. Let’s first generate S samples from the distribution, as x1,"
s = 1of f(X) by using the empirical distribution of {f(xs)}S .
"In principle, Monte Carlo methods can be used to solve any problem which has a probabilistic interpretation. We know that by the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (or the sample mean) of independent samples of the random variable. A widely used sampler is Markov chain Monte Carlo (MCMC) sampler for parametrizing the probability distribution of a random variable. The main idea is to design a judicious Markov chain model with a prescribed stationary probability distribution."
"Monte Carlo techniques are now widely used in statistics and machine learning as well. Using the Monte Carlo technique, we can approximate the expected value of any function of a random variable by simply drawing samples from the population of the random variable, and then computing the arithmetic mean of the function applied to the samples, as follows:"
"This is also called Monte Carlo integration, and is easier to evaluate than the numerical integration which attempts to evaluate the function at a fixed grid of points. Monte Carloevaluates the function only in places where there is a non- negligible probability."
"For different functions f(), the approximation of some important quantities can be obtained:"
A discrete random variable is expressed with the probability mass function (pmf) p(x) = p(X = x).A continuous random variable is expressed with the probability density function (pdf) p(x) = p(X = x).
The cumulative distribution function (cdf) of random variable X can be
The equation to find out the mean for random variable X is:when X is continuous.
"If n independent Bernoulli trials are performed and X represents the number of success in those n trials, then X is called a binomial random variable.To model the outcomes of tossing a K-sided die, or for experiments where the outcome can be multiple, we can use the multinomial distribution."
"If there are two random variables X and Y in the sample space of S of a random experiment, then the pair (X, Y) is called as bivariate random variable.The covariance between two random variables X and Y measures the degree to which X and Y are (linearly) related."
In hypothesis testing:Type I error: reject H0 (or accept H1) when H0 is true. Type II error: reject H1 (or accept H0) when H1 is true.
"Using the Monte Carlo technique, we can approximate the expected value of any function of a random variable by simply drawing samples from the population of the random variable, and then computing the arithmetic mean of the function applied to the samples."
The probabilistic approach used in machine learning is closely related to:
PsychologyThis type of interpretation of probability tries to quantify the uncertainty of some event and thus focuses on information rather than repeated trials.
Bayesian interpretation of probabilityThe reason the Bayesian interpretation can be used to model the uncertainty of events is that it does not expect the long run frequencies of the events to happen.
"Based on this joint distribution on two events p(A,B), we can define the this distribution as follows:"
"We can define this probability as p(A|B) = p(A,B)/p(B) if p(B) > 0"
"Normal probabilityIn statistical terms, this represents the weighted average score."
"The covariance between two random variables X and Y measures the degree to which X and Y are (linearly) related, which means how X varies with Y and vice versa. What is the formula for Cov (X,Y)?"
"Cov(X,Y) = E(X)E(Y)/ E(XY)The binomial distribution can be used to model the outcomes of coin tosses."
FalseTwo events A and B are called mutually exclusive if they can happen together.
Define the Bayesian interpretation of probability.Define probability of a union of two events with equation.
What is conditional probability means? What is the formula of it?
What is Poisson distribution? What is the formula?Define covariance.
What is sampling without replacement? Give example.What is hypothesis? Give example.
If Z = X + Y. Find the range and PMF of Z.Find P(X = 4|Z = 8).
"In an exam, there were 20 multiple-choice questions. Each question had 44 possible options. A student knew the answer to 10 questions, but the other 10 questions were unknown to him and he chose answers randomly. If the score of the student X is equal to the total number of correct answers, then find out the PMF of X. What is P(X>15)?"
If we have two independent random variables X and Y such that X~Poisson(a) and Y~Poisson(β). Define a new random variable as Z = X + Y. Find out the PMF of Z.There is a discrete random variable X with the pmf.
"There is a box containing 40 white shirts and 60 black shirts. If we choose 10 shirts (without replacement) at random, then find the joint PMF of X and Y where X is the number of white shirts and Y is the number of black shirts."
"Find E[X|Y = y], for 0 ≤ y ≤ 1.Find Var(X|Y = y), for 0 ≤ y ≤ 1."
"‌Principles of probability for classification are an important area of machine learning algorithms. In our practical life, our decisions are affected by our prior knowledge or belief about an event. Thus, an event that is otherwise very unlikely to occur may be considered by us seriously to occur in certain situations if we know that in the past, the event had certainly occurred when other events were observed. The same concept is applied in machine learning using Bayes’ theorem and the related algorithms discussed in this chapter. The concepts of probabilities discussed in the previous chapters are used extensively in this chapter."
"In the last chapter, we discussed the rules of probability and possible uses of probability, distribution functions, and hypothesis testing principles in the machine learning domain. In this chapter, we will discuss the details of the Bayesian theorem and how it provides the basis for machine learning"
"concepts. The technique was derived from the work of the 18th century mathematician Thomas Bayes. He developed the foundational mathematical principles, known as Bayesian methods, which describe the probability of events, and more importantly, how probabilities should be revised when there is additional information available."
"‌Bayesian learning algorithms, like the naive Bayes classifier, are highly practical approaches to certain types of learning problems as they can calculate explicit probabilities for hypotheses. In many cases, they are equally competitive or even outperform the other learning algorithms, including decision tree and neural network algorithms."
"Bayesian classifiers use a simple idea that the training data are utilized to calculate an observed probability of each class based on feature values. When the same classifier is used later for unclassified data, it uses the observed probabilities to predict the most likely class for the new features. The application of the observations from the training data can also be thought of as applying our prior knowledge or prior belief to the probability of an outcome, so that it has higher probability of meeting the actual or real-life outcome. This simple concept is used in Bayes’ rule and applied for training a machine in machine learning terms. Some of the real-life uses of Bayesian classifiers are as follows:"
"Text-based classification such as spam or junk mail filtering, author identification, or topic categorization"
"Medical diagnosis such as given the presence of a set of observed symptoms during a disease, identifying the probability of new patients having the disease"
Network security such as detecting illegal intrusion or anomaly in computer networks
"One of the strengths of Bayesian classifiers is that they utilize all available parameters to subtly change the predictions, while many other algorithms tend to ignore the features that have weak effects. Bayesian classifiers assume that even if few individual parameters have small effect on the outcome, the collective effect of those parameters could be quite large. For such learning tasks, the naive Bayes classifier is most effective."
"Some of the features of Bayesian learning methods that have made them popular are as follows:to apply this method. Moreover, it normally involves high computational cost to arrive at the optimal Bayes hypothesis."
"Before we discuss Bayes’ theorem and its application in concept learning, we should be clear about what is concept learning. Let us take an example of how a child starts to learn meaning of new words, e.g. ‘ball’. The child is provided with positive examples of ‘objects’ which are ‘ball’. At first, the child may be confused with many different colours, shapes and sizes of the balls and may also get confused with some objects which look similar to ball, like a balloon or a globe. The child’s parent continuously feeds her positive examples like ‘that is a ball’, ‘this is a green ball’, ‘bring me that small ball’, etc. Seldom there are negative examples used for such concept teaching, like ‘this is a non-ball’, but the parent may clear the confusion of the child when it points to a balloon and says it is a ball by saying ‘that is not a ball’. But it is observed that the learning is most influenced through positive examples rather than through negative examples, and the expectation is that the child will be able to identify the object ‘ball’ from a wide variety of objects and different types of balls kept together once the concept of a ball is clear to her. We can extend"
"To relate the above-mentioned learning concept with the mathematical model of Bayes, we can correlate the learning process of ‘meaning of a word’ as equivalent to learning, a concept using binary classification. Let us define a concept set C and a corresponding function f(k). We also define f(k) = 1, when k is within the set C and f(k) = 0 otherwise. Our aim is to learn the indicator function f that defines which elements are"
"within the set C. So, by using the function f, we will be able to classify the element either inside or outside our concept set. In Bayes’ theorem, we will learn how to use standard probability calculus to determine the uncertainty about the function f, and we can validate the classification by feeding positive examples."
where A and B are conditionally related events and p(A|B) denotes the probability of event A occurring when event B has already occurred.Let us assume that we have a training data set D where we have noted some observed data. Our task is to determine the best hypothesis in space H by using the knowledge of D.
"The prior knowledge or belief about the probabilities of various hypotheses in H is called Prior in context of Bayes’ theorem. For example, if we have to determine whether a particular type of tumour is malignant for a patient, the prior knowledge of such tumours becoming malignant can be used to validate our current hypothesis and is a prior probability or simply called Prior."
"The probability that a particular hypothesis holds for a data set based on the Prior is called the posterior probability or simply Posterior. In the above example, the probability of the hypothesis that the patient has a malignant tumour considering the Prior of correctness of the malignancy test is a posterior probability. In our notation, we will say that we are interested in finding out P(h|T), which means whether the hypothesis holds true given the observed training data T. This is called the posterior probability or simply Posterior in machine learning language. So, the prior probability P(h), which represents the probability of the hypothesis independent of the training data (Prior), now gets refined with the introduction of influence of the training data as P(h|T)."
"‌From the above equation, we can deduce that P(h|T) increases as P(h) and P(T|h) increases and also as P(T) decreases. The simple explanation is that when there is more probability that T can occur independently of h then it is less probable that h can get support from T in its occurrence."
"It is a common question in machine learning problems to find out the maximum probable hypothesis h from a set of hypotheses H (h∈H) given the observed training data T. This maximally probable hypothesis is called the maximum a posteriori (MAP) hypothesis. By using Bayes’ theorem, we can identify the MAP hypothesis from the posterior probability of each candidate hypothesis:"
"and as P(T) is a constant independent of h, in this case, we can write"
"In certain machine learning problems, we can further simplify equation 6.1 if every hypothesis in H has equal probable priori as P(hi) = P(hj), and then, we can determine P(h|T) from the probability P(T|h) only. Thus, P(T|h) is called the likelihood of data T given h, and any hypothesis that maximizes P(T|h) is"
"called the maximum likelihood (ML) hypothesis, hML. See figure 6.1 and 6.2 for the conceptual and mathematical representation of Bayes theorem and the relationship of Prior, Posterior and Likelihood."
Arriving at the refined probability of an event in the light of probability of a related event is a powerful concept and relates very closely with our day-to-day handling of events and using our knowledge to influence the decisions.
h1 = the particular tumour is of malignant type = MT in our example
h2 = the particular tumour is not malignant type = !MT in our example
"As P(h2|PT) is higher than P(h1|PT), it is clear that the hypothesis h2 has more probability of being true. So, hMAP = h2 = !MT."
"‌This indicates that even if the posterior probability of malignancy is significantly higher than that of non- malignancy, the probability of this patient not having malignancy is still higher on the basis of the prior knowledge. Also, it should be noted that through Bayes’ theorem, we identified the probability of one hypothesis being higher than the other hypothesis, and we did not completely accept or reject the hypothesis by this theorem. Furthermore, there is very high dependency on the availability of the prior data for successful application of Bayes’ theorem."
"One simplistic view of concept learning can be that if we feed the machine with the training data, then it can calculate the posterior probability of the hypotheses and outputs the most probable hypothesis. This is also called brute-force Bayesian learning algorithm, and it is also observed that consistency in providing the right probable hypothesis by this algorithm is very comparable to the other algorithms."
"For calculating the highest posterior probability, we can use Bayes’ theorem as discussed earlier in this chapter:"
"Please note that calculating the posterior probability for each hypothesis requires a very high volume of computation, and for a large volume of hypothesis space, this may be difficult to achieve."
"Let us try to connect the concept learning problem with the problem of identifying the hMAP. On the basis of the probability distribution of P(h) and P(T|h), we can derive the prior knowledge of the learning task. There are few important assumptions to be made as follows:"
"‌The training data or target sequence T is noise free, which means that it is a direct function of X only (i.e. ti = c(xi))"
Each hypothesis is equally probable and independent of each other
"On the basis of assumption 3, we can say that each hypothesis h within the space H has equal prior probability, and also because of assumption 2, we can say that these prior probabilities sum up to 1. So, we can write"
"P(T|h) is the probability of observing the target values ti in the fixed set of instances {xi,…, xm) in the space where h holds true and describes the concept c correctly. Using assumption 1 mentioned above, we can say that if T is consistent with h,"
then the probability of data T given the hypothesis h is 1 and is 0 otherwise:
"For the cases when h is inconsistent with the training data T, using 6.5 we get"
"Now, if we define a subset of the hypothesis H which is consistent with T as HD, then by using the total probability equation, we get"
"So, with our set of assumptions about P(h) and P(T|h), we get the posterior probability P(h|T) as"
"‌where HD is the number of hypotheses from the space H which are consistent with target data set T. The interpretation of this evaluation is that initially, each hypothesis has equal probability and, as we introduce the training data, the posterior probability of inconsistent hypotheses becomes zero and the total probability that sums up to 1 is distributed equally among the consistent hypotheses in the set. So, under this condition,"
"From the above discussion, we understand the behaviour of the general class of learner whom we call as consistent learners. So, the group of learners who commit zero error over the training data and output the hypothesis are called consistent learners. If the training data is noise free and deterministic (i.e. P(D|h) = 1 if D and h are consistent and 0 otherwise) and if there is uniform prior probability distribution over H (so, P(hm) = P(hn) for all m, n), then every consistent learner outputs the MAP hypothesis. An important application of this conclusion is that Bayes’ theorem can characterize the behaviour of learning algorithms even when the algorithm does not explicitly manipulate the probability. As it can help to identify the optimal distributions of P(h) and P(T|h) under which the algorithm outputs the MAP hypothesis, the knowledge can be used to characterize the assumptions under which the algorithms behave optimally."
"Though we discussed in this section a special case of Bayesian output which corresponds to the noise-free training data and deterministic predictions of hypotheses where P(T|h) takes on value of either 1 or 0, the theorem can be used with the same effectiveness for noisy training data and additional assumptions about the probability distribution governing the noise."
"‌In this section, we will discuss the use of the MAP hypothesis to answer the question what is the most probable classification of the new instance given the training data. To illustrate the concept, let us assume three hypotheses h1, h2, and h3 in the hypothesis space H. Let the posterior probability of these hypotheses be 0.4, 0.3, and 0.3, respectively. There is a new instance x, which is classified as true by h1, but false by h2 and h3."
"the basis of the combined predictions of all alternative hypotheses, weighted by their posterior probabilities."
"The set of possible outcomes for the new instance x is within the set C = {True, False} and"
"This method maximizes the probability that the new instance is classified correctly when the available training data, hypothesis space and the prior probabilities of the hypotheses are known. This is thus also called Bayes optimal classifier."
‌Naïve Bayes is a simple technique for building classifiers: models that assign class labels to problem instances. The basic idea of Bayes rule is that the outcome of a hypothesis can be predicted on the basis of some evidence (E) that can be observed.
A prior probability of hypothesis h or P(h): This is the probability of an event or hypothesis before the evidence is observed.A posterior probability of h or P(h|D): This is the probability of an event after the evidence is observed within the population D.
"For example, a person has height and weight of 182 cm and 68 kg, respectively. What is the probability that this person belongs to the class ‘basketball player’? This can be predicted using the Naïve Bayes classifier. This is known as probabilistic classifications."
"In machine learning, a probabilistic classifier is a classifier that can be foreseen, given a perception or information (input), a likelihood calculation over a set of classes, instead of just yielding (outputting) the most likely class that the perception (observation) should belong to. Parameter estimation for Naïve Bayes models uses the method of ML."
"Bayes’ theorem is used when new information can be used to revise previously determined probabilities. Depending on the particular nature of the probability model, Naïve Bayes classifiers can be trained very professionally in a supervised learning setting."
"Let us see the basis of deriving the principles of Naïve Bayes classifiers. We take a learning task where each instance x has some attributes and the target function (f(x)) can take any value from the finite set of classification values C. We also have a set of training examples for target function, and the set of attributes {a1, a2,…, an} for the new instance are known to us. Our task is to predict the classification of the new instance."
"According to the approach in Bayes’ theorem, the classification of the new instance is performed by assigning the most probable target classification CMAP on the basis of the attribute values of the new instance {a1, a2,…, an}. So,"
As combined probability of the attributes defining the new instance fully is always 1
"‌So, to get the most probable classifier, we have to evaluate the two terms P(a1, a2, c, an|ci) and P(ci). In a practical scenario, it is possible to calculate P(ci) by calculating the frequency of each target value ci in the training data set. But the P(a1, a2, c, an|ci) cannot be estimated easily and needs a very high effort of calculation. The reason is that the number of these terms is equal to the product of number of possible instances and the number of possible target values, and thus, each instance in the instance space needs to be visited many times to arrive at the estimate of the occurrence. Thus, the Naïve Bayes classifier makes a simple assumption that the attribute values are conditionally independent of each other for the target value. So, applying this simplification, we can now say that for a target value of an instance, the probability of observing the combination a1,a2,…, an is the product of probabilities of individual attributes P(ai|cj)."
"Then, from equation 6.7, we get the approach for the Naïve Bayes classifier as"
"Here, we will be able to compute P(ai|cj) as we have to calculate this only for the number of distinct attributes values (ai) times the number of distinct target values (cj), which is much smaller set than the product of both the sets. The most"
"important reason for the popularity of the Naïve Bayes classifier approach is that it is not required to search the whole hypothesis space for this algorithm, but rather we can arrive at the target classifier by simply counting the frequencies of various data combinations within the training example."
"To summarize, a Naïve Bayes classifier is a primary probabilistic classifier based on a view of applying Bayes’ theorem (from Bayesian inference with strong naive) independence assumptions. The prior probabilities in Bayes’ theorem that are changed with the help of newly available information are classified as posterior probabilities."
"A key benefit of the naive Bayes classifier is that it requires only a little bit of training information (data) to gauge the parameters (mean and differences of the variables) essential for the classification (arrangement). In the Naïve Bayes classifier, independent variables are always assumed, and only the changes (variances) of the factors/variables for each class should be determined and not the whole covariance matrix.Because of the rather naïve assumption that all features of the dataset are equally important and independent, this is called Naïve Bayes classifier."
P(Yes) will give the overall probability of favourable condition in the given scenario.P(No) will give the overall probability of non-favourable condition in the given scenario.
"To predict whether the team will win for given weather conditions (a1) = Rainy, Wins in last three matches (a2) = 2 wins, Humidity (a3) = Normal and Win toss (a4) = True, we need to choose ‘Yes’ from the above table for the given conditions."
"‌This equation becomes much easier to resolve if we recall that Naïve Bayes classifier assumes independence among events. This is specifically true for class-conditional independence, which means that the events are independent so long as they are conditioned on the same class value. Also, we know that if"
"the events are independent, then the probability rule says, P(A n B) = P(A) P(B), which helps in simplifying the above equation significantly as"
"‌Conclusion: This shows that there is 58% probability that the team will win if the above conditions become true for that particular day. Thus, Naïve Bayes classifier provides a simple yet powerful way to consider the influence of multiple attributes on the target outcome and refine the uncertainty of the event on the basis of the prior knowledge because it is able to simplify the calculation through independence assumption."
"classifier with a dynamical model. The Naïve Bayes classifier employs ‘single words’ and ‘word pairs’ like features and determines the sentiments of the users. It allocates user utterances into nice, nasty, and neutral classes, labelled as +1, –1, and 0, respectively. This binary output drives a simplefirst-order dynamical system, whose emotional state represents the simulated emotional state of the experiment’s personification."
‌Handling Continuous Numeric Features in Naïve Bayes Classifier‌
"The workaround that is applied in these cases is discretizing the continuous data on the basis of some data range. This is also called binning as the individual categories are termed as bins. For example, let us assume we want to market a certain credit card to all the customers who are visiting a particular bank. We have to classify the persons who are visiting a bank as either interested candidate for taking a new card or non- interested candidate for a new card, and on the basis of this classification, the representative will approach the customer for sale. In this case, the customers visit the bank continuously during banking hours and have different values for the attributes we want to evaluate before classifying them into the interested/non-interested categories."
"If we plot the number of customers visiting the bank during the 8 hours of banking time, the distribution graph will be a continuous graph. But if we introduce a logic to categorize the customers according to their time of entering the bank, then we will be able to put the customers in ‘bins’ or buckets for our analysis. We can then try to assess what time range is best suited for targeting the customers who will have interest in the new credit card. The bins created by categorizing the customers by their time of entry looks like Figure 6.5.This creates eight natural bins for us (or we may change the number of bins by changing our categorizing criteria), which can now be used for Bayes analysis."
"We must have noted that a significant assumption in the Naïve Bayes classifier was that the attribute values a1, a2,…, an are conditionally independent for a target value. The Naïve Bayes classifier generates optimal output when this condition is met. Though this assumption significantly reduces the complexity of computation, in many practical scenarios, this requirement of conditional independence becomes a difficult constraint for the application of this algorithm. So, in this section, we will discuss the approach of Bayesian Belief network, which assumes that within the set of attributes, the probability distribution can have conditional probability relationship as well as conditional independence assumptions. This is different from the Naïve Bayes assumption of conditional independence of all the attributes as the belief network provides the flexibility of declaring a subset of the attributes as conditionally dependent while leaving rest of the attributes to hold the assumptions of conditional independence. The prior knowledge or belief about the influence of one attribute over the other is handled through joint probabilities as discussed later in this section."
"Let us refresh our mind on the concept of conditional probability. If an uncertain event A is conditional on a knowledge or belief K, then the degree of belief in A with the assumption that K is known is expressed as P(A|K)."
"Traditionally, conditional probability is expressed by joint probability as follows:"
"For a set of n attributes, the generalized form of the product rule becomes"
"P(A1, A2, …, An) = P(A1|A2, …, An)P(A2|A3, …, An)P(An - 1|An)P(An) (6.10)This generalized version of the product rule is called the Chain Rule."
"Let us understand the chain rule by using the diagram in Figure 6.6. From the joint probability formula 6.10, we can write"
"But from Figure 6.6, it is evident that E is not related to C and D, which means that the probabilities of variables C and D are not influenced by E and vice versa. Similarly, A is directly influenced only by B. By applying this knowledge of independence, we can simplify the above equation asLet us discuss this concept of independence and conditional independence in detail in the next section."
"We represent the conditional probability of A with knowledge of K as P(A|K). The variables A and K are said to be independent if P(A|K) = P(A), which means that there is no influence of K on the uncertainty of A. Similarly, the joint probability can be written as P(A,K) = P(A)P(K).Extending this concept, the variables A and K are said to be conditionally independent given C if P(A|C) = P(A|K, C)."
"This concept of conditional independence can also be extended to a set of attributes. We can say that the set of variables A1, A2,…, An is conditionally independent of the set of variables B1, B2,…, Bm given the set of variables C1, C2,…, Cl if"
"If we compare this definition with our assumption in the Naïve Bayes classifier, we see that the Naïve Bayes classifier assumes that the instance attribute A1 is conditionally independent of the instance attribute A2, given the target value V, which can be written using the general product rule and application of conditional independence formula as"
"A Bayesian Belief network describes the joint probability distribution of a set of attributes in their joint space. In Figure 6.7, a Bayesian Belief network is presented. The diagram consists of nodes and arcs. The nodes represent the discrete or continuous variables for which we are interested to calculate the conditional probabilities. The arc represents the causal relationship of the variables."
"The two important information points we get from this network graph are used for the determining the joint probability of the variables. First, the arcs assert that the node variables are conditionally independent of its non-descendants in the network given its immediate predecessors in the network. If two variables A and B are connected through a directed path, then B is called the descendent of A. Second, the conditional probability table for each variable provides the probability distribution of that variable given the values of its immediate predecessors. We can use Bayesian probability to calculate different behaviours of the variables in Figure 6.7."
= P(Tim late|Rain Today)P(Rain Today) + P(Tim late|No Rain Today)*P(No Rain Today)
"From this unconditional probability, the most important use of the Bayesian Belief network is to find out the revised probability on the basis of the prior knowledge. If we assume that there was rain today, then the probability table can quickly provide us the information about the probability of Paul being late to class or the probability of Tim being late to class from the probability distribution table itself. But if we do not know whether there was rain today or not, but we only know that Tim is late to class today, then we can arrive at the following probabilities –"
The revised probability that Paul will be late to class today –
= P(Paul late|Rain today)P(Rain today) + P(Paul late|No rain today)P(No rain today)
"Here, we used the concept of hard evidence and soft evidence. Hard evidence (instantiation) of a node is evidence that the state of the variable is definitely as a particular value. In our above example, we had hard evidence that ‘Tim is late to class’. If a particular node is instantiated, then it will block propagation of evidence further down to its child nodes. Soft evidence for a node is the evidence that provides the prior probability values for the node. The node ‘Paul is late to class’ is soft evidenced with the prior knowledge that ‘Tim is late to class’."
"‌There can be two main scenarios faced in the Bayesian Belief network learning problem. First, the network structure might be available in advance or can be inferred from the training data. Second, all the network variables either are directly observable in each training example or"
"some of the variables may be unobservable. Learning the conditional probability tables is a straightforward problem when the network structure is given in advance and the variables are fully observable in the training examples. But in the case the network structure is available and only some of the variable values are observable in the training data, then the learning problem is more difficult. This is topic of much research, and some of the advanced topics for identifying the node values include algorithms such as Gradient Ascent Training and the EM algorithm."
"As discussed above, by using the Bayesian network, we would like to infer the value of a target variable on the basis of the observed values of some other variables. Please note that it will not be possible to infer a single value in the case of random variables we are dealing with, but our intention is to"
"infer the probability distribution of the target variable given the observed values of other variables. In general, the Bayesian network can be used to compute the probability distribution of any subset of node variables given the values or distribution of the remaining variables."
"‌hypotheses and the probability of likelihood or observing various training data given the hypothesis. It then assigns a posterior probability to each candidate hypothesis on the basis of the assumed priors and the observed data.The MAP hypothesis is the most probable hypothesis given the data. As no other hypothesis is more likely, this is also the optimal hypothesis."
"The Bayes optimal classifier calculates the most probable classification of each new instance by combining the predictions of all alternative hypotheses, weighted by their posterior probabilities."
"The Naïve Bayes classifier has been found to be useful in many practical applications and is considered as one of the powerful learning methods. Even when the assumption of conditional independence is not met, the Naïve Bayes classifier is quite effective in providing a standard for the other learning methods."
"Three companies X, Y, and Z supply 40%, 45%, and 15% of the uniforms to a school. Past experience shows that 2%, 3%, and 4% of the uniforms supplied by these companies are defective. If a uniform was found to be defective, what is the probability that the uniform was supplied by Company X?"
"A box of apples contains 10 apples, of which 6 are defective. If 3 of the apples are removed from the box in succession without replacement,"
Two boxes containing chocolates are placed on a table. The boxes are labelled B1 and B2. Box B1 contains 6 Cadbury chocolates and 5 Amul chocolates. Box B2 contains 3 Cadbury chocolates and 8 Nestle chocolates. The boxes are arranged so that the probability of selecting
box B1 is 1/3 and the probability of selecting box B2 is 2/3. Sneha is
"blindfolded and asked to select a chocolate. She will win Rs. 10,000 if she selects a Cadbury chocolate. If she win Rs 10,000, what is the probability that she selected a Cadbury chocolate from the first box?"
"In a certain basketball club, there are 4% of male players who are over 6 feet tall and 1% of female players who are over 6 feet tall. The ratio of male to female players in the total player population is male:female ="
2:3. A player is selected at random from among all those who are over 6 feet tall. What is the probability that the player is a female?
The probability that a particular hypothesis holds for a data set based on the Prior is called
Dependent probabilitiesOne main disadvantage of Bayesian classifiers is that they utilize all available parameters to subtly change the predictions.
"In a bolt factory, machines A1, A2, and A3 manufacture respectively 25%, 35%, and 40% of the total output. Of these 5%, 4%, and 2% are defective bolts. A bolt is drawn at random from the product and is found to be defective. What is the probability that it was manufactured by machine A2?"
4. 0.125Bayesian methods can perform better than the other methods while validating the hypotheses that make probabilistic predictions.
FalseNaïve Bayes classifier makes the naïve assumption that the attribute values are conditionally dependent given the classification of the instance.
What is posterior probability? Give an example.What is likelihood probability? Give an example.
Write any two strengths of Bayes classifier.Write any two weaknesses of Bayes classifier.
"LONG ANSWER-TYPE QUESTIONS (10 MARKS EACH)Explain the concept of Prior, Posterior, and Likelihood with an example."
How Bayes’ theorem supports the concept learning principle?Explain Naïve Bayes classifier with an example of its use in practical life.
"Is it possible to use Naïve Bayes classifier for continuous numeric data? If so, how?"
What are Bayesian Belief networks? Where are they used? Can they solve all types of problems?
"In an airport security checking system, the passengers are checked to find out any intruder. Let I with i∈ {0, 1} be the random variable which indicates whether somebody is an intruder (i = 1) or not (i = 0) and A with a ∈ {0, 1} be the variable indicating alarm. An alarm will be raised if an intruder is identified with probability P(A = 1|I = 1) = 0.98 and a non-intruder with probability P(A = 1|I = 0) = 0.001, which implies the error factor. In the population of passengers, the probability of someone is intruder is P(I = 1) = 0.00001. What is the probability that an alarm is raised when a person actually is an intruder?"
An antibiotic resistance test (random variable T) has 1% false positives (i.e. 1% of those not resistance to an antibiotic show positive result in the test) and 5% false negatives (i.e. 5% of those actually resistant to an antibiotic test negative). Let us assume that 2% of those tested are resistant to antibiotics. Determine the probability that somebody who tests positive is actually resistant (random variable D).
"For preparation of the exam, a student knows that one question is to be solved in the exam which is either of types A, B, or C. The probabilities of A, B, or C appearing in the exam are 30%, 20%, and 50% respectively. During the preparation, the student solved 9 of 10 problems of type A, 2 of 10 problems of type B, and 6 of 10 problems of type C."
What is the probability that the student will solve the problem of the exam?
"Given that the student solved the problem, what is the probability that it was of type A?"
"A CCTV is installed in a bank to monitor the incoming customers and take a photograph. Though there are continuous flows of customers, we"
"create bins of timeframe of 5 min each. In each time frame of 5 min, there may be a customer moving into the bank with 5% probability or there is no customer (again, for simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If there is a customer, it will be detected by the CCTV with a probability of 99%. If there is no customer, the camera will take a false photograph by detecting other thing’s movement with a probability of 10%."
How many customers enter the bank on average per day (10 hours)?
How many false photographs (there is a photograph taken even though there is no customer) and how many missed photographs (there is no photograph even though there is a customer) are there on average per day?
"If there is a photograph, what is the probability that there is indeed a customer?"
Draw the Bayesian Belief network to represent the conditional independence assumptions of the Naïve Bayes classifier for the match winning prediction problem of Section 6.4.4. Construct the conditional probability table associated with the node Won Toss.
"In the last chapter on Bayesian Concept Learning, you were introduced to an important supervised learning algorithm – the Naïve Bayes algorithm. As we have seen, it is a very simple but powerful classifier based on Bayes’ theorem of conditional probability. However, other than the Naïve Bayes classifier, there are more algorithms for classification. This chapter will focus on other classification algorithms."
"The first algorithm we will delve into in this chapter is k-Nearest Neighbour (kNN), which tries to classify unlabelled data instances based on the similarity with the labelled instances in the training data."
"Finally, a very powerful and popular classifier named Support Vector Machine (SVM) will be explored.So, by the end of this chapter, you will gain enough knowledge to start solving a classification problem by using some standard classifiers."
"In supervised learning, the labelled training data provide the basis for learning. According to the definition of machine learning, this labelled training data is the experience or prior knowledge or belief. It is called supervised learning because the process of learning from the training data by a machine can be related to a teacher supervising the learning process of a student who is new to the subject. Here, the teacher is the training data."
"Training data is the past information with known value of class field or ‘label’. Hence, we say that the ‘training data is labelled’ in the case of supervised learning (refer Fig. 7.1)."
"Contrary to this, there is no labelled training data for unsupervised learning. Semi-supervised learning, as depicted in Figure 7.1, uses a small amount of labelled data along with unlabelled data for training."
"In a hospital, many patients are treated in the general wards. In comparison, the number of beds in the Intensive Care Unit (ICU) is much less. So, it is always a cause of worry for the hospital management that if the health condition of a number of patients in the general ward suddenly aggravates and they would have to be moved to ICU. Without previous planning and preparations, such a spike in demand becomes difficult for the hospital to manage. This problem can be addressed in a much better way if it is possible to predict which of the patients in the normal wards have a possibility of their health condition deteriorating and thus need to be moved to ICU."
"This kind of prediction problem comes under the purview of supervised learning or, more specifically, under classification. The hospital already has all past patient records. The records of the patients whose health condition aggravated in the past and had to be moved to ICU can form the training data for this prediction problem. Test results of newly admitted patients are used to classify them as high-risk or low-risk patients."
Prediction of results of a game based on the past analysis of results
"Predicting whether a tumour is malignant or benign on the basis of the analysis of dataPrice prediction in domains such as real estate, stocks, etc."
"‘IBM Watson Health’ developed by IBM software provides evidence-backed cancer care to each patient by understanding millions of data points. ‘Watson for Oncology’ helps physicians quickly identify vital information in a patient’s medical record, surface relevant evidence, and explore various treatment options for patients(Source: https://www.ibm.com/watson/health/oncology- and-genomics/oncology)."
"Let us consider two examples, say ‘predicting whether a tumour is malignant or benign’ and ‘price prediction in the domain of real estate’. Are these two problems same in nature?"
"The answer is ‘no’. It is true that both of them are problems related to prediction. However, for tumour prediction, we are trying to predict which category or class, i.e. ‘malignant’ or ‘benign’, an unknown input data related to tumour belongs to. In the other case, that is, for price prediction, we are trying to predict an absolute value and not a class."
"Whereas when we are trying to predict a numerical variable such as ‘price’, ‘weight’, etc. the problem falls under the category of regression.Supervised machine learning is as good as the data used to train it. If the training data is poor in quality, the prediction will also be far from being precise."
"We can observe that in classification, the whole problem centres around assigning a label or category or class to a test data on the basis of the label or category or class information that is imparted by the training data. Because the target objective is to assign a class label, we call this type of problem as a classification problem. Figure 7.2 depicts the typical process of classification where a classification model is obtained from the labelled training data by a classifier algorithm. On the basis of the model, a class label (e.g. ‘Intel’ as in the case of the test data referred in Fig. 7.2) is assigned to the test data."
"‌A critical classification problem in the context of the banking domain is identifying potentially fraudulent transactions. Because there are millions of transactions which have to be scrutinized to identify whether a particular transaction might be a fraud transaction, it is not possible for any human being to carry out this task. Machine learning is"
"leveraged efficiently to do this task, and this is a classic case of classification. On the basis of the past transaction data, especially the ones labelled as fraudulent, all new incoming transactions are marked or labelled as usual or suspicious. The suspicious transactions are subsequently segregated for a closer review."
"In summary, classification is a type of supervised learning where a target feature, which is of categorical type, is predicted for test data on the basis of the information imparted by the training data. The target categorical feature is known as class ."
"Prediction of natural calamity such as earthquake, flood, etc. Handwriting recognition :"
Machine learning saves lives – it can spot 52% of breast cancer cells at least a year before patients are diagnosed
US Postal Service uses machine learning for handwriting recognition Facebook’s news feed uses machine learning to personalize each member’s feed
"First, there is a problem which is to be solved, and then, the required data (related to the problem, which is already stored in the system) is evaluated and pre-processed based on the algorithm. Algorithm selection is a critical point in supervised learning. The result after iterative training rounds is a classifier for the problem in hand (refer Fig. 7.3)."
"Because the data is gathered from different sources, it is usually collected in a raw format and is not ready for immediate analysis. This step ensures that the data is ready to be fed into the machine learning algorithm."
"Definition of Training Data Set: Before starting the analysis, the user should decide what kind of data set is to be used as a training set. In the case of signature analysis, for example, the training data set might be a single handwritten alphabet, an entire handwritten word (i.e. a group of the alphabets) or an entire line of handwriting (i.e. sentences or a group of words). Thus, a set of ‘input meta-objects’ and corresponding ‘output meta-objects’ are also gathered. The training set needs to be actively representative of the real-world use of the given scenario. Thus, a set of data input (X) and corresponding outputs (Y) is gathered either from human experts or experiments.algorithm) may also be adjusted by optimizing performance on a subset (called as validation set) of the training set."
"Let us now delve into some common classification algorithms. Following are the most common classification algorithms, out of which we have already learnt about the Naïve Bayes classifier in Chapter 6. We will cover details of the other algorithms in this chapter."
"The kNN algorithm is a simple but extremely powerful classification algorithm. The name of the algorithm originates from the underlying philosophy of kNN – i.e. people having similar background or mindset tend to stay close to each other. In other words, neighbours in a locality have a similar background. In the same way, as a part of the kNN algorithm, the unknown and unlabelled data which comes for a prediction problem is judged on the basis of the training data set elements which are similar to the unknown element. So, the class label of the unknown element is assigned on the basis of the class labels of the similar training data set elements (metaphoricallycan be considered as neighbours of the unknown element). Let us try to understand the algorithm with a simple data set."
"Let us consider a very simple Student data set as depicted in Figure 7.4. It consists of 15 students studying in a class. Each of the students has been assigned a score on a scale of 10 on two performance parameters – ‘Aptitude’ and ‘Communication’. Also, a class value is assigned to each student based on the following criteria:"
Students having good communication skills as well as a good level of aptitude have been classified as ‘Leader’
Students having good communication skills but not so good level of aptitude have been classified as ‘Speaker’
Students having not so good communication skill but a good level of aptitude have been classified as ‘Intel’
"As we have already discussed, in the kNN algorithm, the class label of the test data elements is decided by the class label of the training data elements which are neighbouring, i.e. similar in nature. But there are two challenges:"
What is the basis of this similarity or when can we say that two data elements are similar?
How many similar elements should be considered for deciding the class label of each test data element?
"‌To answer the first question, though there are many measures of similarity, the most common approach adopted by kNN to measure similarity between two data elements is Euclidean distance. Considering a very simple data set having"
"two features (say f1 and f2), Euclidean distance between two data elements d1 and d2 can be measured by"
where f11 = value of feature f1 for data element d1 f12 = value of feature f1 for data element d2
"So, as depicted in Figure 7.6, the training data points of the Student data set considering only the features ‘Aptitude’ and ‘Communication’ can be represented as dots in a two-"
"dimensional feature space. As shown in the figure, the training data points having the same class value are coming close to each other. The reason for considering two-dimensional data space is that we are considering just the two features of the Student data set, i.e. ‘Aptitude’ and ‘Communication’, for doing the classification. The feature ‘Name’ is ignored because, as we can understand, it has no role to play in deciding the class value. The test data point for student Josh is represented as an asterisk in the same space. To find out the closest or nearest neighbours of the test data point, Euclidean distance of the different dots need to be calculated from the asterisk. Then, the class value of the closest neighbours helps in assigning the class value of the test data element."
"Now, let us try to find the answer to the second question, i.e. how many similar elements should be considered. The answer lies in the value of ‘k’ which is a user-defined parameter given"
"as an input to the algorithm. In the kNN algorithm, the value of ‘k’ indicates the number of neighbours that need to be considered. For example, if the value of k is 3, only three nearest neighbours or three training data elements closest to the test data element are considered. Out of the three data elements, the class which is predominant is considered as the class label to be assigned to the test data. In case the value of k is 1, only the closest training data element is considered. The class label of that data element is directly assigned to the test data element. This is depicted in Figure 7.7."
"Let us now try to find out the outcome of the algorithm for the Student data set we have. In other words, we want to see what class value kNN will assign for the test data for student Josh. Again, let us refer back to Figure 7.7. As is evident, when the value of k is taken as 1, only one training data point needs to be considered. The training record for student Gouri"
"comes as the closest one to test record of Josh, with a distance value of 1.118. Gouri has class value ‘Intel’. So, the test data point is also assigned a class label value ‘Intel’. When the value of k is assumed as 3, the closest neighbours of Josh in the training data set are Gouri, Susant, and Bobby with distances being 1.118, 1.414, and 1.5, respectively. Gouri and Bobby have class value ‘Intel’, while Susant has class value ‘Leader’. In this case, the class value of Josh is decided by majority voting. Because the class value of ‘Intel’ is formed by the majority of the neighbours, the class value of Josh is assigned as ‘Intel’. This same process can be extended for any value of k."
One common practice is to set k equal to the square root of the number of training records.An alternative approach is to test several k values on a variety of test data sets and choose the one that delivers the best performance.
"Another interesting approach is to choose a larger value of k, but apply a weighted voting process in which the vote of close neighbours is considered more influential than the vote of distant neighbours."
"Calculate the distance (usually Euclidean distance) of the test data point from the different training data points.Find the closest ‘k’ training data points, i.e. training data points whose distances are least from the test data point."
"Whichever class label is predominantly present in the training data points, assign that class label to the test data point"
"We have already discussed in Chapter 3 that eager learners follow the general steps of machine learning, i.e. perform an abstraction of the information obtained from the input data and then follow it through by a generalization step. However, as we have seen in the case of the kNN algorithm, these steps are completely skipped. It stores the training data and directly applies the philosophy of nearest neighbourhood finding to arrive at the classification. So, for kNN, there is no learning happening in the real sense. Therefore, kNN falls under the category of lazy learner."
"Very effective in certain situations, e.g. for recommender system design Very fast or almost no time required for the training phase"
"One of the most popular areas in machine learning where the kNN algorithm is widely adopted is recommender systems. As we know, recommender systems recommend users different items which are similar to a particular item that the user seems to like. The liking pattern may be revealed from past purchases or browsing history and the similar items are identified using the kNN algorithm."
Another area where there is widespread adoption of kNN is searching documents/ contents similar to a given document/content. This is a core area under information retrieval and is known as concept search.
"Decision tree learning is one of the most widely adopted algorithms for classification. As the name indicates, it builds a model in the form of a tree structure. Its grouping exactness is focused with different strategies, and it is exceptionally productive."
‌A decision tree is used for multi-dimensional analysis with multiple classes. It is characterized by fast execution time and
ease in the interpretation of the rules. The goal of decision tree learning is to create a model (based on the past data called past vector) that predicts the value of the output variable based on the input variables in the feature vector.
"Each node (or decision node) of a decision tree corresponds to one of the feature vector. From every node, there are edges to children, wherein there is an edge for each of the possible values (or range of values) of the feature associated with the node. The tree terminates at different leaf nodes (or terminal nodes) where each leaf node represents a possible value for the output variable. The output variable is determined by following a path that starts at the root and is guided by the values of the input variables.A decision tree is usually represented in the format depicted in Figure 7.8."
Each internal node (represented by boxes) tests an attribute (represented as ‘A’/‘B’ within the boxes). Each branch corresponds to an attribute value (T/F) in the above case. Each leaf node assigns a classification. The first node is called as ‘Root’ Node. Branches from the root node are called as ‘Leaf’Nodes where ‘A’ is the Root Node (first node). ‘B’ is the Branch Node. ‘T’ & ‘F’ are Leaf Nodes.
"Figure 7.9 shows an example decision tree for a car driving – the decision to be taken is whether to ‘Keep Going’ or to ‘Stop’, which depends on various situations as depicted in the figure. If the signal is RED in colour, then the car should be stopped. If there is not enough gas (petrol) in the car, the car should be stopped at the next available gas station."
Decision trees are built corresponding to the training data following an approach called recursive partitioning. The
"approach splits the data into multiple subsets on the basis of the feature values. It starts from the root node, which is nothing but the entire data set. It first selects the feature which predicts the target class in the strongest way. The decision tree splits the data set into multiple partitions, with data in each partition having a distinct value for the feature based on which the partitioning has happened. This is the first set of branches. Likewise, the algorithm continues splitting the nodes on the basis of the feature which helps in the best partition. This continues till a stopping criterion is reached. The usual stopping criteria are –"
All or most of the examples at a particular node have the same class
"Let us try to understand this in the context of an example. Global Technology Solutions (GTS), a leading provider of IT solutions, is coming to College of Engineering and Management (CEM) for hiring B.Tech. students. Last year during campus recruitment, they had shortlisted 18 students for the final interview. Being a company of international repute, they follow a stringent interview process to select only the best of the students. The information related to the interview evaluation results of shortlisted students (hiding the names) on the basis of different evaluation parameters is available for reference in Figure 7.10. Chandra, a student of CEM, wants to find out if he may be offered a job in GTS. His CGPA is quite high. His self-evaluation on the other parameters is as follows:"
"Let us try to solve this problem, i.e. predicting whether Chandra will get a job offer, by using the decision tree model. First, we need to draw the decision tree corresponding to the training data given in Figure 7.10. According to the table, job offer condition (i.e. the outcome) is FALSE for all the cases where Aptitude = Low, irrespective of other conditions. So, the feature Aptitude can be taken up as the first node of the decision tree.Figure 7.11 depicts the complete decision tree diagram for the table given in Figure 7.10."
"Repeat the above steps until the solution is reached.Exhaustive search travels through the decision tree exhaustively, but it will take much time when the decision tree is big with multiple leaves and multiple attribute values."
"Branch and bound uses an existing best solution to sidestep searching of the entire decision tree in full. When the algorithm starts, the best solution is well defined to have the worst possible value; thus, any solution it finds out is an improvement. This makes the algorithm initially run down to the left-most branch of the tree, even though that is unlikely to produce a realistic result. In the partitioning problem, that solution corresponds to putting every item in one group, and it is an unacceptable solution. A programme can speed up the process by using a fast heuristic to find an initial solution. This can be used as an input for branch and bound. If the heuristic is right, the savings can be substantial."
"There are many implementations of decision tree, the most prominent ones being C5.0, CART (Classification and Regression Tree), CHAID (Chi-square Automatic Interaction Detector) and ID3 (Iterative Dichotomiser 3) algorithms. The biggest challenge of a decision tree algorithm is to find out which feature to split upon. The main driver for identifying the feature is that the data should be split in such a way that the partitions created by the split should contain examples belonging to a single class. If that happens, the partitions are considered to be pure. Entropy is a measure of impurity of an attribute or feature adopted by many algorithms such as ID3 and C5.0. The information gain is calculated on the basis of the decrease in entropy (S) after a data set is split according to a particular attribute (A). Constructing a decision tree is all about finding an attribute that returns the highest information gain (i.e. the most homogeneous branches)."
"Like information gain, there are other measures like Gini index or chi-square for individual nodes to decide the feature on the basis of which the split has to be applied. The CART algorithm uses Gini index, while the CHAID algorithm uses chi-square for deciding the feature for applying split."
‌where c is the number of different class labels and p refers to the proportion of values falling into the i-th class label.
"For example, with respect to the training data in Figure 7.10, we have two values for the target class ‘Job Offered?’ – Yes and No. The value of pi for class value ‘Yes’ is 0.44 (i.e. 8/18) and that for class value ‘No’ is 0.56 (i.e. 10/18). So, we can calculate the entropy asEntropy(S) = -0.44 log 2(0.44) - 0.56 log 2(0.56) = 0.99."
"The information gain is created on the basis of the decrease in entropy (S) after a data set is split according to a particular attribute (A). Constructing a decision tree is all about finding an attribute that returns the highest information gain (i.e. the most homogeneous branches). If the information gain is 0, it means that there is no reduction in entropy due to split of the data set according to that particular feature. On the other hand, the maximum amount of information gain which may happen is the entropy of the data set before the split."
"For calculating the entropy after split, entropy for all partitions needs to be considered. Then, the weighted summation of the entropy for each partition can be taken as the total entropy after split. For performing weighted summation, the proportion of examples falling into each partition is used as weight."
Let us examine the value of information gain for the training data set shown in Figure 7.10. We will find the value of entropy at the beginning before any split happens and then again after the split happens. We will compare the values for all the cases –
"Figure 7.13a gives the entropy values for the first level split for each of the cases mentioned above.As calculated, entropy of the data set before split (i.e."
"Therefore, the information gain from the feature ‘CGPA’ =0.47 and 0.04, respectively."
"Hence, it is quite evident that among all the features, ‘Aptitude’ results in the best information gain when adopted for the split. So, at the first level, a split will be applied according to the value of ‘Aptitude’ or in other words, ‘Aptitude’ will be the first node of the decision tree formed. One important point to be noted here is that for Aptitude = Low, entropy is 0, which indicates that always the result will be the same irrespective of the values of the other features.Hence, the branch towards Aptitude = Low will not continue any further."
"As a part of level 2, we will thus have only one branch to navigate in this case – the one for Aptitude = High. Figure"
"7.13b presents calculations for level 2. As can be seen from the figure, the entropy value is as follows:"
"‌As a part of level 3, we will thus have only one branch to navigate in this case – the one for Communication = Bad. Figure 7.13c presents calculations for level 3. As can be seen from the figure, the entropy value is as follows:"
"Because the maximum information gain is already achieved, the tree will not continue any further."
Draw a decision tree node containing the attribute Fmin and split the data set into subsetsRepeat the above steps until the full tree is drawn covering all the attributes of the original table.
"The decision tree algorithm, unless a stopping criterion is applied, may keep growing indefinitely – splitting for every feature and dividing into smaller partitions till the point that the data is perfectly classified. This, as is quite evident, results in overfitting problem. To prevent a decision tree getting overfitted to the training data, pruning of the decision tree is essential. Pruning a decision tree reduces the size of the treesuch that the model is more generalized and can classify unknown and unlabelled data in a better way."
It can handle both numerical and categorical variables. Can work well both with small and large training data sets.Decision trees provide a definite clue of which features are more useful for classification.
"Decision tree can be applied in a data set in which there is a finite list of attributes and each data instance stores a value for that attribute (e.g. ‘High’ for the attribute CGPA). When each attribute has a small number of distinct values (e.g. ‘High’, ‘Medium’, ‘Low’), it is easier/quicker for the decision tree to suggest (or choose) an effective solution. This algorithm can be extended to handle real-value attributes (e.g. a floating point temperature)."
The most straightforward case exists when there are only two possible values for an attribute (Boolean classification). Example: Communication has only two values as ‘Good’ or ‘Bad’. It is also easy to extend the decision tree to create a target function with more than two possible output values.
"Example: CGPA can take one of the values from ‘High’, ‘Medium’, and ‘Low’. Irrespective of whether it is a binary value/ multiple values, it is discrete in nature. For example, Aptitude can take the value of either ‘High’ or ‘Low’. It is not possible to assign the value of both ‘High’ and ‘Low’ to the attribute Aptitude to draw a decision tree."
"There should be no infinite loops on taking a decision. As we move from the root node to the next level node, it should move step-by-step towards the decision node. Otherwise, the algorithm may not give the final result for a given data. If a setof code goes in a loop, it would repeat itself forever, unless the system crashes."
"Balancing overfitting and underfitting in decision tree is a very tricky topic, involving more of an art than science.The way to master this art is experience in working with more number of data sets with a lot of diversity."
"Random forest is an ensemble classifier, i.e. a combining classifier that uses and combines many decision tree classifiers. Ensembling is usually done using the concept of bagging with different feature sets. The reason for using large number of trees in random forest is to train the trees enough such that contribution from each feature comes in a number of models. After the random forest is generated by combining the trees, majority vote is applied to combine the output of the different trees. A simplified random forest model is depicted in Figure 7.14. The result from the ensemble model is usually better than that from the individual decision tree models."
Select a different subset of the training data ‘with replacement’ to train another decision tree following steps (1) to (3). Repeat this to build and train ‘n’ decision trees.Final class assignment is done on the basis of the majority votes from the ‘n’ trees.
"In the random forest classifier, if the number of trees is assumed to be excessively large, the model may get overfitted. In an extreme case of overfitting, themodel may mimic the training data, and training error might be almost 0. However, when the model is run on an unseen sample, it may result in a very high validation error."
"Generated forests can be saved for future use on other data.Lastly, the random forest algorithm can be used to solve both classification and regression problems."
"Random forest is a very powerful classifier which combines the versatility of many decision tree models into a single model. Because of the superior results, this ensemble model is gaining wide adoption and popularity amongst the machine learning practitioners to solve a wide range of classification problems."
"SVM is a model, which can do linear classification as well as regression. SVM is based on the concept of a surface, called a hyperplane, which draws a boundary between data instances plotted in the multi-dimensional feature space. The output prediction of an SVM is one of two conceivable classes which are already defined in the training data. In summary, the SVM algorithm builds an N-dimensional hyperplane model that assigns future instances into one of the two possible output classes."
"In SVM, a model is built to discriminate the data instances belonging to different classes. Let us assume for the sake of simplicity that the data instances are linearly separable. In this case, when mapped in a two-dimensional space, the data instances belonging to different classes fall in different sides of a straight line drawn in the two-dimensional space as depicted in Figure 7.15a. If the same concept is extended to a multi- dimensional feature space, the straight line dividing data instances belonging to different classes transforms to a hyperplane as depicted in Figure 7.15b."
"Thus, an SVM model is a representation of the input instances as points in the feature space, which are mapped so that an apparent gap between them divides the instances of the separate classes. In other words, the goal of the SVM analysis is to find a plane, or rather a hyperplane, which separates the instances on the basis of their classes. New examples (i.e. new instances) are then mapped into that same space and predicted to belong to a class on the basis of which side of the gap the new instance will fall on. In summary, in the overall training process, the SVM algorithm analyses input data and identifies a surface in the multi-dimensional feature space called the hyperplane. There may be many possible hyperplanes, and one of the challenges with the SVM model is to find the optimal hyperplane."
"‌Training data sets which have a substantial grouping periphery will function well with SVM. Generalization error in terms of SVM is the measure of how accurately and precisely this SVM model can predict values for previously unseen data (new data). A hard margin in terms of SVM means that an SVM model is inflexible in classification and tries to work exceptionally fit in the training set, thereby causing overfitting."
"Hyperplane and Margin: For an N-dimensional feature space, hyperplane is a flat subspace of dimension (N–1) that separates and classifies a set of data. For example, if we consider a two-dimensional feature space (which is nothing but a data set having two features and a class variable), a hyperplane will be a one-dimensional subspace or a straight"
"line. In the same way, for a three-dimensional feature space (data set having three features and a class variable), hyperplane is a two-dimensional subspace or a simple plane. However, quite understandably, it is difficult to visualize a feature space greater than three dimensions, much like for a subspace or hyperplane having more than three dimensions."
"Mathematically, in a two-dimensional space, hyperplane can be defined by the equation:c0 + c1X1 + c2X2 = 0, which is nothing but an equation of a straight line."
"Extending this concept to an N-dimensional space, hyperplane can be defined by the equation:"
"c0 + c1X1 + c2X2 + … + cNXN = 0 which, in short, can be represented as follows:"
"‌Spontaneously, the further (or more distance) from the hyperplane the data points lie, the more confident we can be about correct categorization. So, when a new testing data point/data set is added, the side of the hyperplane it lands on will decide the class that we assign to it. The distance between hyperplane and data points is known as margin.arriving to that conclusion. For the sake of simplicity of visualization, the hyperplanes have been shown as straight lines in most of the diagrams."
"Scenario 2Here, maximizing the distances between the nearest data points of both the classes and hyperplane will help us decide the correct hyperplane. This distance is called as margin."
"One triangle at the other end is like an outlier for the triangle class. SVM has a feature to ignore outliers and find the hyperplane that has the maximum margin (hyperplane A, as shown in Fig. 7.19b). Hence, we can say that SVM is robust to outliers."
"If there is a need to prioritize between higher margin and lesser misclassification, the hyperplane should try to reduce misclassifications.Our next focus is to find out a way to identify a hyperplane which maximizes the margin."
"Using this equation, the objective is to find a set of values for the vector  such that two hyperplanes, represented by the equations below, can be specified."
This is to ensure that all the data instances that belong to one class falls above one hyperplane and all the data instances belonging to the other class falls below another hyperplane.
should be . It is quite obvious that in order to maximize the
separable data to a linearly separable data. These functions are called kernels.
‌Some of the common kernel functions for transforming from a lower dimension ‘i’ to a higher dimension ‘j’ used by different SVM implementations are as follows:
Polynomial kernel: It is in the form  Sigmoid kernel: It is in the form
"When data instances of the classes are closer to each other, this method can be used. The effectiveness of SVM depends both on the"
"SVM can be used for both classification and regression.It is robust, i.e. not much impacted by data with noise or outliers. The prediction results using this model are very promising."
"The objective of classification is to predict the class of unknown objects on the basis of prior class-related information of similar objects. This learning could be used when how to classify a given data is known or, in other words, class values of data instances are available."
"a particular transaction might be a fraud transaction, it is not possible for any human being to carry out this task.Common classification algorithms are kNN, decision tree, random forest, SVM, and Naïve Bayes."
"The kNN algorithm is among the best and the simplest of machine learning algorithms. A new instance is classified by a majority vote of its neighbours, with the instance being allocated the class that is predominant among its kNN.Decision tree learning is the most broadly utilized classifier. It is characterized by fast execution time and ease in the interpretation of the rules."
where c is the number of different class labels and p refers to the proportion of values falling into the i-th class label.
Random forest is an ensemble classifier (combining classifier) which uses and combines many decision tree models. The result from an ensemble model is usually better than that from one of the individual models.
An SVM is a binary linear classifier. The output prediction of an SVM is one of the two conceivable classes which are already defined in the training data. SVM is also an example of a linear classifier and a maximum margin classifier.
"SVM has a new technique called the kernel trick. These are functions, which take a lower dimensional input space and transform it to a higher dimensional space and in the process converts a non-linearly separable problem to a linearly separable problem. These functions are called kernels. When all the classes are closer to each other, this method can be used."
Predicting whether a tumour is malignant or benign is an example of?
Price prediction in the domain of real estate is an example of?
"Categorical AttributeLet us consider two examples, say ‘predicting whether a tumour is malignant or benign’ and ‘price prediction in the domain of real estate’. These two problems are same in nature."
FALSESupervised machine learning is as good as the data used to train it.
"Which is a type of machine learning where a target feature, which is of categorical type, is predicted for the test data on the basis of the information imparted by the training data?"
"Classification is a type of supervised learning where a target feature, which is of categorical type, is predicted for the test data on the basis of the information imparted by the training data. The target categorical feature is known as?"
ClassThis is the first step in the supervised learning model.
Definition of Training Data SetThis is the cleaning/transforming the data set in the supervised learning model.
Definition of Training Data SetThis refers to the transformations applied to the identified data before feeding the same into the algorithm.
Definition of Training Data SetThis step of supervised learning determines ‘the type of training data set’.
Definition of Training Data SetEntire design of the programme is done over here in supervised learning.
"15 are the data points (representing classes), the importantcomponent in a data set, which are near the identified set of lines (hyperplane)."
Support Vectors16 is a line that linearly separates and classifies a set of data.
The distance between hyperplane and data points is called as:
What sizes of training data sets are not best suited for SVM?
Training data set size does not matterSupport Vectors are near the hyperplane.
"FalseIn SVM, these functions take a lower dimensional input space and transform it to a higher dimensional space."
Which of the following options is true about the kNN algorithm?
It is not possible to use for both classification and regression
"Which of the following will be Euclidean distance between the two data points A(4,3) and B(2,3)?"
"Which of the following will be Manhattan distance between the two data points A(8,3) and B(4,3)?"
"When you find many noises in data, which of the following options would you consider in kNN?"
"What would be the relationship between the training time taken by 1- NN, 2-NN, and 3-NN?"
Which of the following algorithms is an example of the ensemble learning algorithm?
Which of the following is not an inductive bias in a decision tree?
Trees that place nodes near the root with high information gain are preferred
Give an example of supervised learning in a hospital industry.Give any three examples of supervised learning.
"Give some examples of common classification algorithms.Explain, in brief, the SVM model."
Define Support Vectors in the SVM model.Define kernel in the SVM model.
"What are the disadvantages of the kNN algorithm?Explain, in brief, the decision tree algorithm."
Write any three strengths of the decision tree method.Write any three weaknesses of the decision tree method.
Explain classification steps in detail.Discuss the SVM model in detail with different scenarios.
Discuss how to calculate the distance between the test data and the training data for kNN.Write the algorithm for kNN.
Explain various options of searching a decision tree.Discuss the decision tree algorithm in detail.
What is inductive bias in a decision tree? How to avoid overfitting?
What are the strengths and weaknesses of the decision tree method?Discuss appropriate problems for decision tree learning in detail.
Discuss the random forest model in detail. What are the features of random forest?Discuss OOB error and variable importance in random forest.
"In the last two chapters, you have got quite a good conceptual overview of supervised learning algorithm for categorical data prediction. You got a detailed understanding of all the popular models of classification that are used by machine learning practitioners to solve a wide array of prediction problems where the target variable is a categorical variable."
"In this chapter, we will build concepts on prediction of numerical variables – which is another key area of supervised learning. This area, known as regression, focuses on solving problems such as predicting value of real estate, demand forecast in retail, weather forecast, etc."
"First, you will be introduced to the most popular and simplest algorithm, namely simple linear regression. This model roots from the statistical concept of fitting a straight line and the least squares method. We will explore thisalgorithm in detail. In this same context, we will also explore the concept of multiple linear regression."
"We will then briefly touch upon the other important algorithms in regression, namely multivariate adaptive regression splines, logistic regression, and maximum likelihood estimation.By the end of this chapter, you will gain sufficient knowledge in all the aspects of supervised learning and become ready to start solving problems on your own."
"‌We have mentioned many times that real estate price prediction is a problem that can be solved by supervised learning or, more specifically, by regression. So, what this problem really is? Let us delve a little deeper into the problem."
"New City is the primary hub of the commercial activities in the country. In the last couple of decades, with increasing globalization, commercial activities have intensified in New City. Together with that, a large number of people have come and settled in the city with a dream to achieve professional growth in their lives. As an obvious fall-out, a large number of housing projects have started in every nook and corner of the city. But the demand for apartments has still outgrown the supply. To get benefit from this boom in real estate business, Karen has started a digital market agency for buying and selling real estates (including apartments, independent houses, town houses, etc.). Initially, when the business was small, she used to interact with buyers and sellers personally and help them arrive at a price quote – either for selling a property (for"
"a seller) or for buying a property (for a buyer). Her long experience in real estate business helped her develop an intuition on what the correct price quote of a property could be – given the value of certain standard parameters such as area (sq. m.) of the property, location, floor, number of years since purchase, amenities available, etc. However, with the huge surge in the business, she is facing a big challenge. She is not able to manage personal interactions as well as setting the correct price quote for the properties all alone. She hired an assistant for managing customer interactions. But the assistant, being new in the real estate business, is struggling with price quotations. How can Karen solve this problem?"
"Fortunately, Karen has a friend, Frank, who is a data scientist with in-depth knowledge in machine learning models. Frank comes up with a solution to Karen’s problem. He builds a model which can predict the correct value of a real estate if it has certain standard inputs such as area (sq. m.) of the property, location, floor, number of years since purchase, amenities available, etc. Wow, that sounds to be like Karen herself doing the job! Curious to know what model Frank has used? Yes, you guessed it right. He used a regression model to solve Karen’s real estate price prediction problem."
"So, we just discussed about one problem which can be solved using regression. In the same way, a bunch of other problems related to prediction of numerical value can be solved using the regression model. In the context of regression, dependent variable (Y) is the one whose value is to be predicted, e.g. the price quote of the real estate in the context of Karen’s problem. This variable is presumed to be functionally related to one (say, X) or more independent variables called predictors. In the context of Karen’s problem, Frank used area of the property, location, floor, etc. as"
"predictors of the model that he built. In other words, the dependent variable depends on independent variable(s) or predictor(s). Regression is essentially finding a relationship (or) association between the dependent variable (Y) and the independent variable(s) (X), i.e. to find the function ‘f ’ for the association Y = f (X)."
"As the name indicates, simple linear regression is the simplest regression model which involves only one predictor. This model assumes a linear relationship between the dependent variable and the predictor variable as shown in Figure 8.1."
"In the context of Karen’s problem, if we take Price of a Property as the dependent variable and the Area of the Property (in sq. m.) as the predictor variable, we can build a model using simple linear regression."
"‌Assuming a linear association, we can reformulate the model aswhere ‘a’ and ‘b’ are intercept and slope of the straight line, respectively."
"Just to recall, straight lines can be defined in a slope– intercept form Y = (a + bX), where a = intercept and b = slope of the straight line. The value of intercept indicates the value of Y when X = 0. It is known as ‘the intercept or Y intercept’ because it specifies where the straight line crosses the vertical or Y-axis (refer to Fig. 8.1)."
Slope of a straight line represents how much the line in a graph changes in the vertical direction (Y-axis) over a change in the horizontal direction (X-axis) as shown in Figure 8.2.
"Let us find the slope of the graph where the lower point on the line is represented as (–3, –2) and the higher point on the line is represented as (2, 2)."
‌There can be two types of slopes in a linear regression model: positive slope and negative slope. Different types of regression lines based on the type of slope includeA positive slope always moves upward on a graph from left to right (refer to Fig. 8.3).
"Slope for a variable (X) may vary between two graphs, but it will always be positive; hence, the above graphs are called as graphs with curve linear positive slope.A negative slope always moves downward on a graph from left to right. As X value (on X-axis) increases, Y value decreases (refer to Fig. 8.5)."
"Slope for a variable (X) may vary between two graphs, but it will always be negative; hence, the above graphs are called as graphs with curve linear negative slope.Scatter graph shown in Figure 8.7 indicates ‘no relationship’ curve as it is very difficult to conclude whether the relationship between X and Y is positive or negative."
"The regression equation model in machine learning uses the above slope–intercept format in algorithms. X and Y values are provided to the machine, and it identifies the values of a (intercept) and b (slope) by relating the values of X and Y. However, identifying the exact match of values for a and b is not always possible. There will be some error value (ɛ) associated with it. This error is called marginal or residual error."
"Now that we have some context of the simple regression model, let us try to explore an example to understand clearly how to decide the parameters of the model (i.e. values of a and b) for a given problem."
"A college professor believes that if the grade for internal examination is high in a class, the grade for external examination will also be high. A random sample of 15 students in that class was selected, and the data is given below:"
‌A scatter plot was drawn to explore the relationship between the independent variable (internal marks) mapped to X-axis and dependent variable (external marks) mapped to Y-axis as depicted in Figure 8.8.
"‌As we know, in simple linear regression, the line is drawn using the regression formula."
"If we know the values of ‘a’ and ‘b’, then it is easy to predict the value of Y for any given X by using the above formula. But the question is how to calculate the values of ‘a’ and ‘b’ for a given set of X and Y values?"
"So, let us calculate the value of a and b for the given example. For detailed calculation, refer to Figure 8.10."
"Hence, for the above example, the estimated regression equation is constructed on the basis of the estimated values of a and b:"
Marks in external exam = 19.04 + 1.89 × (Marks in internal exam)
"As we have already seen, the simple linear regression model built on the data in the example is"
"The value of the intercept from the above equation is 19.05. However, none of the internal mark is 0. So, intercept = 19.05 indicates that 19.05 is the portion of the external examination marks not explained by the internal examination marks."
"increases by 1.89 for each additional 1 mark in the internal examination.Now that we have a complete understanding of how to build a simple linear regression model for a given problem, it is time to summarize the algorithm."
Step 7: Divide output of step 4 by output of step 6 to calculate ‘b’ Step 8: Calculate ‘a’ using the value of ‘b’
Point 63 is at the maximum point for this curve (refer to Fig.
Point 40 (marked with an arrow in Fig. 8.14) is the minimum point for this curve. Point 40 is at the lowest point on this curve. It has a lesser y-coordinate value than any other point on the curve and has a slope of zero.
"‌In a multiple regression model, two or more independent variables, i.e. predictors are involved in the model. If we think in the context of Karen’s problem, in the last section, we came up with a simple linear regression by considering Price of a Property as the dependent variable and the Area of the Property (in sq. m.) as the predictor variable. However, location, floor, number of years since purchase, amenities available, etc. are also important predictors which should not be ignored. Thus, if we consider Price of a Property (in $) as the dependent variable and Area of the Property (in sq. m.), location, floor, number of years since purchase and amenities"
"available as the independent variables, we can form a multiple regression equation as shown below:"
"The following expression describes the equation involving the relationship with two predictor variables, namely X1 and X2.‌The model describes a plane in the three-dimensional space of Ŷ, X1, and X2. Parameter ‘a’ is the intercept of this plane."
"Parameters ‘b1’ and ‘b2’ are referred to as partial regression coefficients. Parameter b1 represents the change in the mean response corresponding to a unit change in X1 when X2 is held constant. Parameter b2 represents the change in the mean response corresponding to a unit change in X2 when X1 is held constant.Consider the following example of a multiple linear regression model with two predictor variables, namely X1 and X2 (refer to Fig. 8.15)."
"Multiple regression for estimating equation when there are ‘n’ predictor variables is as follows:While finding the best fit line, we can fit either a polynomial or curvilinear regression. These are known as polynomial or curvilinear regression, respectively."
"Relationships determined by regression are only relationships of association based on the data set and not necessarily of cause and effect of the defined class.‌Regression line can be valid only over a limited range of data. If the line is extended (outside the range of extrapolation), it may only lead to wrong predictions."
Variance is the same for all values of X (homoskedasticity).The error term (c) is normally distributed. This also means that the mean of the error (c) has an expected value of 0.
"The values of the error (c) are independent and are not related to any values of X. This means that there are no relationships between a particular X, Y that are related to another specific value of X, Y.Given the above assumptions, the OLS estimator is the Best Linear Unbiased Estimator (BLUE), and this is called as Gauss-Markov Theorem."
"‌Main Problems in Regression Analysis‌In multiple regressions, there are two primary problems: multicollinearity and heteroskedasticity."
"Two variables are perfectly collinear if there is an exact linear relationship between them. Multicollinearity is the situation in which the degree of correlation is not only between the dependent variable and the independent variable, but there is also a strong correlation within (among) the independent"
"variables themselves. A multiple regression equation can make good predictions when there is multicollinearity, but it is difficult for us to determine how the dependent variable will change if each independent variable is changed one at a time."
"When multicollinearity is present, it increases the standard errors of the coefficients. By overinflating the standard errors, multicollinearity tries to make some variables statistically insignificant when they actually should be significant (with lower standard errors). One way to gauge multicollinearity is to calculate the Variance Inflation Factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if the predictors are correlated. If no factors are correlated, the VIFs will be equal to 1."
"The assumption of no perfect collinearity states that there is no exact linear relationship among the independent variables. This assumption implies two aspects of the data on the independent variables. First, none of the independent variables, other than the variable associated with the intercept term, can be a constant. Second, variation in the X’s is necessary. In general, the more variation in the independent variables, the better will be the OLS estimates in terms of identifying the impacts of the different independent variables on the dependent variable."
"where ‘var’ represents the variance, ‘cov’ represents the covariance, ‘u’ represents the error terms, and ‘X’ represents the independent variables."
"‌Improving Accuracy of the Linear Regression Model‌Accuracy refers to how close the estimation is near the actual value, whereas prediction refers to continuous estimation of the value."
High bias = low accuracy (not close to real value) High variance = low prediction (values are scattered) Low bias = high accuracy (close to real value)
Low variance = high prediction (values are close to each other)
"In the linear regression model, it is assumed that the number of observations (n) is greater than the number of parameters(k) to be estimated, i.e. n > k, and in that case, the least squares estimates tend to have low variance and hence will perform well on test observations."
"However, if observations (n) is not much larger than parameters (k), then there can be high variability in the least squares fit, resulting in overfitting and leading to poor predictions.If k > n, then linear regression is not usable. This also indicates infinite variance, and so, the method cannot be used at all."
Accuracy of linear regression can be improved using the following three methods:
"‌By limiting (shrinking) the estimated coefficients, we can try to reduce the variance at the cost of a negligible increase in bias. This can in turn lead to substantial improvements in the accuracy of the model."
Few variables used in the multiple regression model are in fact not associated with the overall response and are called as irrelevant variables; this may lead to unnecessary complexity in the regression model.
"This approach involves fitting a model involving all predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing the overall variance. Some of the coefficients may also be estimated to be exactly zero, thereby indirectly performing variable selection. The two best-known techniques for shrinking the regression coefficients towards zero are"
"Ridge regression performs L2 regularization, i.e. it adds penalty equivalent to square of the magnitude of coefficients"
Minimization objective of ridge = LS Obj + a × (sum of square of coefficients)
"Ridge regression (include all k predictors in the final model) is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. If k > n, then the least squares estimates do not even have a unique"
"solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. Thus, ridge regression works best in situations where the least squares estimates have high variance. One disadvantage with ridge regression is that it will include all k predictors in the final model. This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables k is quite large. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size.Lasso regression performs L1 regularization, i.e. it adds penalty equivalent to the absolute value of the magnitude of coefficients."
Minimization objective of ridge = LS Obj + a × (absolute value of the magnitude of coefficients)
"The lasso overcomes this disadvantage by forcing some of the coefficients to zero value. We can say that the lasso yields sparse models (involving only subset) that are simpler as well as more interpretable. The lasso can be expected to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or equal to zero."
Identify a subset of the predictors that is assumed to be related to the response and then fit a model using OLS on the selected reduced subset of variables. There are two methods in which subset of the regression can be selected:
"‌In best subset selection, we fit a separate least squares regression for each possible subset of the k predictors. For computational reasons, best subset selection cannot be applied with very large value of predictors (k). The best subset selection procedure considers all the possible (2k) models containing subsets of the p predictors."
The stepwise subset selection method can be applied to choose the best subset. There are two stepwise subset selection:
"Forward stepwise selection is a computationally efficient alternative to best subset selection. Forward stepwise considers a much smaller set of models, that too step by step, compared to best set selection. Forward stepwise selection begins with a model containing no predictors, and then, predictors are added one by one to the model, until all the k predictors are included in the model. In particular, at each step, the variable (X) that gives the highest additional improvement to the fit is added.Backward stepwise selection begins with the least squares model which contains all k predictors and then iteratively removes the least useful predictor one by one."
"The earlier methods, namely subset selection and shrinkage, control variance either by using a subset of the original variables or by shrinking their coefficients towards zero. In dimensionality reduction, predictors (X) are transformed, and the model is set up using the transformed variables after dimensionality reduction. The number of variables is reduced using the dimensionality reduction method. Principal component analysis is one of the most important dimensionality (variable) reduction techniques."
"Polynomial regression model is the extension of the simple linear model by adding extra predictors obtained by raising (squaring) each of the original predictors to a power. For example, if there are three variables, X, X2, and X3 are used as predictors. This approach provides a simple way to yield a non-linear fit to data."
"In the above equation, c0, c1, c2, and c3 are the coefficients. Example: Let us use the below data set of (X, Y) for degree"
"‌Logistic regression is both classification and regression technique depending on the scenario used. Logistic regression (logit regression) is a type of regression analysis used for predicting the outcome of a categorical dependent variable similar to OLS regression. In logistic regression, dependent variable (Y) is binary (0,1) and independent variables (X) are continuous in nature. The probabilities describing the possible outcomes (probability that Y = 1) of a single trial are modelled as a logistic function of the predictor variables. In the logistic regression model, there is no R2 to gauge the fit of the overall model; however, a chi-square test is used to gauge how well the logistic regression model fits the data. The goal of logistic regression is to predict the likelihood that Y is equal to 1 (probability that Y = 1 rather than 0) given certain values of X. That is, if X and Y have a strong positive linear relationship, the probability that a person will have a score of Y = 1 will increase as values of X increase. So, we are predicting probabilities rather than the scores of the dependent variable.‌"
"For example, we might try to predict whether or not a small project will succeed or fail on the basis of the number of years of experience of the project manager handling the project. We presume that those project managers who have been managing projects for many years will be more likely to succeed. This means that as X (the number of years of experience of project manager) increases, the probability that Y will be equal to 1 (success of the new project) will tend to increase. If we take a hypothetical example in which 60 already executed projects were studied and the years of experience of project managers ranges from 0 to 20 years, we could represent this tendency to increase the probability that Y = 1 with a graph."
"When the graph is drawn for the above values of X and Y, it appears like the graph in Figure 8.18. As X increases, the probability that Y = 1 increases. In other words, when the project manager has more years of experience, a larger percentage of projects succeed. A perfect relationship represents a perfectly curved S rather than a straight line, as was the case in OLS regression. So, to model this relationship, we need some fancy algebra / mathematics that accounts for the bends in the curve."
"An explanation of logistic regression begins with an explanation of the logistic function, which always takes values between zero and one. The logistic formulae are stated in terms of the probability that Y = 1, which is referred to as P. The probability that Y is 0 is 1 – P."
"‘exp’ is the exponent function, which is sometimes also written as e."
We know that the coefficients of a = –100 and b = 0.6.= 150).
y = exp ( –100 + 0.6 × 150)/(1 + EXP( –100 + 0.6 × X) y = 0.000046or a probability of near zero that the person is a male.
The following assumptions must hold when building a logistic regression model:
"The error term follows a binomial distribution [n, p]  n = # of records in the data"
"Let us now discuss about the probability of getting only Head as an outcome; it is 5/10 = 0.5 in the above case.Whenever this number (P) is greater than 0.5, it is said to be in favour of Head. Whenever P is lesser than 0.5, it is said to be against the outcome of getting Head."
where #H is the number of flips that resulted in the expected outcome (heads in this case).
But the likelihood function is not a probability. The likelihood for some coins may be 0.25 or 0 or 1.MLE is about predicting the value for the parameters that maximizes the likelihood function.
"In supervised learning, when we are trying to predict a real-value variable such as ‘Price’, ‘Weight’, etc., the problem falls under the category of regression. A regression problem tries to forecast results as a continuous output."
Independent Variable (X) is called as predictor. The independent variable (X) is used in a regression model to estimate the value of the dependent variable (Y).Regression is essentially finding a relationship (or) association between the dependent variable (Y) and the independent variables (X).
"If the regression involves only one independent variable, it is called simple regression. Thus, if we take ‘Price of a used car’ as the dependent variable and the ‘Year of manufacturing of the car’ as the independent variable, we can build a simple regression."
Slope represents how much the line in a graph changes in the vertical direction (Y-axis) over a change in the horizontal direction (X-axis). Slope is also referred as the rate of change in a graph.Maximum and minimum points on a graph are found at points where the slope of the curve is zero. It becomes zero either from positive or from negative value.
"If two or more independent variables are involved, it is called multiple regression. Thus, if we take ‘Price of a used car’ as the dependent variable and year of manufacturing (Year), brand of the car (Brand), and mileage run"
"(Miles run) as the independent variables, we can form a multiple regression equation as given below:"
"Multicollinearity is the situation in which the degree of correlation is not only between the dependent variable and the independent variable, but there also exists a strong correlation within (among) the independent variables itself."
"Heteroskedasticity refers to the changing variance of the error term. If the variance of the error term is not constant across data sets, there will be erroneous predictions. In general, for a regression equation to make accurate predictions, the error term should be independent, normally (identically) distributed (iid). The error terms should not be related to each other."
Accuracy of linear regression can be improved using the following three methods:
"Polynomial regression model is the extension of the simple linear model by adding extra predictors, obtained by raising (squaring) each of the original predictors to a power. For example, if there are three variables, X, X2, and X3 are used as predictors."
"In logistic regression, the dependent variable (Y) is binary (0,1) and independent variables (X) are continuous in nature. The probabilities describing the possible outcomes (probability that Y = 1) of a single trial are modelled as a function of the explanatory (predictor) variables by using a logistic function."
"When we are trying to predict a real-value variable such as ‘$’, ‘Weight’, the problem falls under the category of"
Dependent variableThis is essentially finding a relationship (or) association between the dependent variable (Y) and the independent variables (X).
"If the regression involves only one independent variable, it is called as"
DY = Change in Y / Change in XThis slope always moves upward on a graph from left to right.
Positive slopeThis slope always moves downwards on a graph from left to right.
Maximum and minimum points on a graph are found at points where the slope of the curve is
"When new predictors (X) are added to the multiple linear regression model, how does R2 behave?"
Increasing or remains constantPredicting stochastic events precisely is not possible.
"Define slope in a linear regression.Find the slope of the graph where the lower point on the line is represented as (–3, –2) and the higher point on the line is represented as (2, 2)."
What are the conditions of a positive slope in linear regression?
What are the conditions of a negative slope in linear regression?
Define sum of squares due to error in multiple linear regression.Define sum of squares due to regression in multiple linear regression.
Explain the assumptions in regression analysis and BLUE concept.Explain two main problems in regression analysis.
Explain polynomial regression model in detail with an example.Explain logistic regression in detail.
What are the assumptions in logistic regression?Discuss maximum likelihood estimation in detail.
"‌We have discussed how to train our machines with past data on the basis of which they can learn, gain intelligence, and apply that intelligence on a new set of data. There are, however, situations when we do not have any prior knowledge of the data set we are working with, but we still want to discover interesting relationships among the attributes of the data or group the data in logical segments for easy analysis. The task of the machine is then to identify this knowledge without any prior training and that is the space of unsupervised learning. In this chapter, we will discuss how Clustering algorithms help in grouping data sets into logical segments and the association analysis which enables to identify a pattern or relationship of attributes within the data set. An interesting application of the association analysis is the Market Basket Analysis, which is used widely by retailers and advertisers across the globe."
"Unsupervised learning is a machine learning concept where the unlabelled and unclassified information is analysed to discover hidden knowledge. The algorithms work on the data without any prior training, but they are constructed in such a way that they can identify patterns, groupings, sorting order, and numerous other interesting knowledge from the set of data."
"‌Till now, we have discussed about supervised learning where the aim was to predict the outcome variable Y on the basis of the feature set X1:X2:… Xn, and we discussed methods such as regression and classification for the same. We will now introduce the concept of unsupervised learning where the objective is to observe only the features X1:X2:… Xn; we are not going to predict any outcome variable, but rather our intention is to find out the association between the features or their grouping to understand the nature of the data. This analysis may reveal an interesting correlation between the features or a common behaviour within the subgroup of the data, which provides better understanding of the data."
"In terms of statistics, a supervised learning algorithm will try to learn the probability of outcome Y for a particular input X, which is called the posterior probability. Unsupervised learning is closely related to density estimation in statistics."
"Here, every input and the corresponding targets are concatenated to create a new set of input such as {(X1, Y1), (X2, Y2),…, (Xn, Yn)}, which leads to a better understanding of the correlation of X and Y; this probability notation is called the joint probability."
Let us take an example of how unsupervised learning helps in pushing movie promotions to the correct group of people. In
"earlier days, movie promotions were blind push of the same data to all demography, such that everyone used to watch the same posters or trailers irrespective of their choice or preference. So, in most of the cases, the person watching the promotion or trailer would end up ignoring it, which leads to waste of effort and money on the promotion. But with the advent of smart devices and apps, there is now a huge database available to understand what type of movie is liked by what segment of the demography. Machine learning helps to find out the pattern or the repeated behaviour of the smaller groups/clusters within this database to provide the intelligence about liking or disliking of certain types of movies by different groups within the demography. So, by using this intelligence, the smart apps can push only the relevant movie promotions or trailers to the selected groups, which will significantly increase the chance of targeting the right interested person for the movie."
"We will discuss two methods in this chapter for explaining the principle underlying unsupervised learning – Clustering and Association Analysis. Clustering is a broad class of methods used for discovering unknown subgroups in data, which is the most important concept in unsupervised learning. Another technique is Association Analysis which identifies a low-dimensional representation of the observations that can explain the variance and identify the association rule for the explanation."
"Because of its flexibility that it can work on uncategorized and unlabelled data, there are many domains where unsupervised learning finds its application. Few examples of such applications are as follows:"
"So, in this chapter, we will cover two major aspects of unsupervised learning, namely Clustering which helps in segmentation of the set of objects into groups of similar objects and Association Analysis which is related to the identification of relationships among objects in a data set."
"Clustering refers to a broad set of techniques for finding subgroups, or clusters, in a data set on the basis of the characteristics of the objects within that data set in such a manner that the objects within the group are similar (or related to each other) but are different from (or unrelated to) the objects from the other groups. The effectiveness of clustering depends on how similar or related the objects within a group are or how different or unrelated the objects in different groups are from each other. It is often domain specific to define whatis meant by two objects to be similar or dissimilar and thus is an important aspect of the unsupervised machine learning task."
"As an example, suppose we want to run some advertisements of a new movie for a countrywide promotional activity. We have data for the age, location, financial condition, and political stability of the people in different parts of the country. We may want to run a different type of campaign for the different parts grouped according to the data we have. Any logical grouping obtained by analysing the characteristics of the people will help us in driving the campaigns in a more targeted way. Clustering analysis can help in this activity by analysing different ways to group the set of people and arriving at different types of clusters."
"There are many different fields where cluster analysis is used effectively, such as"
how clustering tasks differ from classification tasks and how clustering defines groups
"The primary driver of clustering knowledge is discovery rather than prediction, because we may not even know what we are looking for before starting the clustering analysis. So, clustering is defined as an unsupervised machine learning task that automatically divides the data into clusters or groups of similar items. The analysis achieves this without prior knowledge of the types of groups required and thus can provide an insight into the natural groupings within the data set. The primary guideline of clustering task is that the data inside a cluster should be very similar to each other but very different from those outside the cluster. We can assume that the definition of similarity might vary across applications, but the basic idea is always the same, that is, to create the group such that related elements are placed together. Using this principle, whenever a large set of diverse and varied data is presented for analysis, clustering enables to represent the data in a smaller number of groups. It helps to reduce the complexity and provides insight into patterns of relationships to generate meaningful and actionable structures within the data. The effectiveness of clustering is measured by the homogeneity within a group as well as the difference between distinct groups. See Figure 9.1 for reference."
"From the above discussion, it may seem that through clustering, we are trying to label the objects with class labels. But clustering is somewhat different from the classification and numeric prediction discussed in supervised learning chapters. In each of these cases, the goal was to create a model that relates features to an outcome or to other features and the"
"model identifies patterns within the data. In contrast, clustering creates new data. Unlabelled objects are given a cluster label which is inferred entirely from the relationship of attributes within the data."
"Let us take an example. You were invited to take a session on Machine Learning in a reputed university for induction of their professors on the subject. Before you create the material for the session, you want to know the level of acquaintance of the professors on the subject so that the session is successful. But you do not want to ask the inviting university, but rather do some analysis on your own on the basis of the data available freely. As Machine Learning is the intersection of Statistics and Computer Science, you focused on identifying the professors from these two areas also. So, you searched the list of research publications of these professors from the internet, and by using the machine learning algorithm, you now want to group the papers and thus infer the expertise of the professors into three buckets – Statistics, Computer Science, and Machine Learning.After plotting the number of publications of these professors in the two core areas, namely Statistics and Computer Science, you obtain a scatter plot as shown in Figure 9.2."
"Thus, in the above problem, we used visual indication of logical grouping of data to identify a pattern or cluster and labelled the data in three different clusters. The main driver for our clustering was the closeness of the points to each other to form a group. The clustering algorithm uses a very similar approach to measure how closely the data points are related and decides whether they can be labelled as a homogeneous group. In the next section, we will discuss few important algorithms for clustering."
‌We will discuss each of these methods and their related techniques in details in the following sections.
"Two of the most important algorithms for partitioning-based clustering are k-means and k-medoid. In the k-means algorithm, the centroid of the prototype is identified for clustering, which is normally the mean of a group of points. Similarly, the k-medoid algorithm identifies the medoid which is the most representative point for a group of points. We can also infer that in most cases, the centroid does not correspond to an actual data point, whereas medoid is always an actual data point. Let us discuss both these algorithms in detail."
This is one of the oldest and most popularly used algorithm for clustering. The basic principles used by this algorithm also
The principle of the k-means algorithm is to assign each of the ‘n’ data points to one of the K clusters where ‘K’ is a user- defined parameter as the number of clusters desired. The objective is to maximize the homogeneity within the clusters and also to maximize the differences between the clusters. The homogeneity and differences are measured in terms of the distance between the objects or points in the data set.
Step 1: Select K points in the data space and mark them as initial centroids
"Let us understand this algorithm with an example. In Figure 9.4, we have certain set of data points, and we will apply the k- means algorithm to find out the clusters generated from this data set. Let us fix K = 4, implying that we want to create four clusters out of this data set. As the first step, we assign four random points from the data set as the centroids, as represented by the * signs, and we assign the data points to the nearest centroid to create four clusters. In the second step, on the basis of the distance of the points from the corresponding centroids, the centroids are updated and points are reassigned to the updated centroids. After three iterations, we found that the centroids are not moving as there is no scope for refinement, and thus, the k-means algorithm will terminate.‌This provides us the most logical four groupings or cluster of the data sets where the homogeneity within the groups is highest and difference between the groups is maximum."
"One of the most important success factors in arriving at correct clustering is to start with the correct number of cluster assumptions. Different numbers of starting cluster lead to completely different types of data split. It will always help if we have some prior knowledge about the number of clusters and we start our k-means algorithm with that prior knowledge. For example, if we are clustering the data of the students of a university, it is always better to start with the number of departments in that university. Sometimes, the business needs or resource limitations drive the number of required clusters."
"For example, if a movie maker wants to cluster the movies on the basis of combination of two parameters – budget of the movie: high or low, and casting of the movie: star or non-star, then there are 4 possible combinations, and thus, there can be four clusters to split the data."
"For a small data set, sometimes a rule of thumb that is followed is"
"which means that K is set as the square root of n/2 for a data set of n examples. But unfortunately, this thumb rule does not work well for large data sets. There are several statistical methods to arrive at the suitable number of clusters."
"This method tries to measure the homogeneity or heterogeneity within the cluster and for various values of ‘K’ and helps in arriving at the optimal ‘K’. From Figure 9.5, we can see the homogeneity will increase or heterogeneity will decrease with increasing ‘K’ as the number of data points inside each cluster reduces with this increase. But these iterations take significant computation effort, and after a certain point, the increase in homogeneity benefit is no longer in accordance with the investment required to achieve it, as is evident from the figure. This point is known as the elbow point, and the ‘K’ value at this point produces the optimal clustering performance. There are a large number of algorithms to calculate the homogeneity and heterogeneity of the clusters, which are not discussed in this book."
"Another key step for the k-means algorithm is to choose the initial centroids properly. One common practice is to choose random points in the data space on the basis of the number of cluster requirement and refine the points as we move into the iterations. But this often leads to higher squared error in the final clustering, thus resulting in sub-optimal clustering solution. The assumption for selecting random centroids is that"
"‌multiple subsequent runs will minimize the SSE and identify the optimal clusters. But this is often not true on the basis of the spread of the data set and the number of clusters sought. So, one effective approach is to employ the hierarchical clustering technique on sample points from the data set and then arrive at sample K clusters. The centroids of these initial K clusters are used as the initial centroids. This approach is practical when the data set has small number of points and K is relatively small compared to the data points. There are procedures such as bisecting k-means and use of post- processing to fix initial clustering issues; these procedures can produce better quality initial centroids and thus better SSE for the final clusters."
We discussed in the k-means algorithm that the iterative step is to recalculate the centroids of the data set after each iteration. The proximities of the data points from each other within a cluster is measured to minimize the distances. The distance of the data point from its nearest centroid can also be calculated to minimize the distances to arrive at the refined centroid. The Euclidean distance between two data points is measured as follows:
"‌Using this function, the distance between the example data and its nearest centroid and the objective is calculated to minimize this distance. The measure of quality of clustering uses the SSE technique. The formula used is as follows:"
"where dist() calculates the Euclidean distance between the centroid ci of the cluster Ci and the data points x in the cluster. The summation of such distances over all the ‘K’ clusters gives the total sum of squared error. As you can understand, the lower the SSE for a clustering solution, the better is the representative position of the centroid. Thus, in our clustering algorithm in Algorithm 9.1, the recomputation of the"
"centroid involves calculating the SSE of each new centroid and arriving at the optimal centroid identification. After the centroids are repositioned, the data points nearest to the centroids are assigned to form the refined clusters. It is observed that the centroid that minimizes the SSE of the cluster is its mean. One limitation of the squared error method is that in the case of presence of outliers in the data set, the squared error can distort the mean value of the clusters."
"Let us use this understanding to identify the cluster step for the data set in Figure 9.6. Assume that the number of cluster requirement, K = 4. We will randomly select four cluster centroids as indicated by four different colours in Figure 9.7."
"The next step is to calculate the SSE of this clustering and update the position of the centroids. We can also proceed by our understanding that the new centroid should be the mean of the data points in the respective clusters. The distances of the data points currently marked as Cluster C3 from the centroid of cluster C3 are marked as ai1 , ai2 ,…, ain in the figure and those determine the homogeneity within cluster C3. On the other hand, the distances of data points of cluster C4 from the centroid of cluster C3 determine the heterogeneity among these two different clusters. Our aim is to minimize the homogeneity within the clusters and maximize the heterogeneity among the different clusters. So, the revised centroids are as shown in Figure 9.9."
"‌The k-means algorithm continues with the update of the centroid according to the new cluster and reassignment of the points, until no more data points are changed due to the centroid shift. At this point, the algorithm stops. Figure 9.10 shows the final clustering of the data set we used. The complexity of the k-means algorithm is O ( nKt ), where ‘ n ’ is the total number of data points or objects in the data set, K is the number of clusters, and ‘ t ’ is the number of iterations.Normally, ‘ K ’ and ‘ t ’ are kept much smaller than ‘ n ’, and thus, the k-means method is relatively scalable and efficient in processing large data sets."
"Because of the distance-based approach from the centroid to all points in the data set, the k-means method may not always converge to the global optimum and often terminates at a local optimum. The result of the clustering largely depends on the initial random selection of cluster centres."
"centroids. Let us take an example of eight data points, and for simplicity, we can consider them to be 1-D data with values 1, 2, 3, 5, 9, 10, 11, and 25. Point 25 is the outlier, and it affects the cluster formation negatively when the mean of the points is considered as centroids."
"With K = 2, the initial clusters we arrived at are {1, 2, 3, 6}"
"‌and the mean of the cluster  So, the SSE within the clusters is"
"Because the SSE of the second clustering is lower, k-means tend to put point 9 in the same cluster with 1, 2, 3, and 6 though the point is logically nearer to points 10 and 11. This skewedness is introduced due to the outlier point 25, which shifts the mean away from the centre of the cluster."
where oi is the representative point or object of cluster Ci.
"Thus, the k-medoids method groups n objects in k clusters by minimizing the SSE. Because of the use of medoids from the actual representative data points, k-medoids is less influenced by the outliers in the data. One of the practical implementation of the k-medoids principle is the Partitioning"
Step 1: Randomly choose k points in the data set as the initial representative points
Step C: Randomly select a non-representative point or in each cluster
Step 4: Swap the representative point oj with or and compute the new SSE after swapping
"Step 5: If SSEnew < SSEold, then swap oj with or to form the new set of k"
representative objects;Step 6: Refine the k clusters on the basis of the nearest representative point.
"‌In this algorithm, we replaced the current representative object with a non-representative object and checked if it improves the quality of clustering. In the iterative process, all possible replacements are attempted until the quality of clusters no longer improves."
"If o1,…, ok are the current set of representative objects or medoids and there is a non-representative object or, then to determine whether or is a good replacement of oj (1 ≤ j ≤ k), the distance of each object x is calculated from its nearest medoid from the set {o1, o2,…, oj–1, or, oj+1,…, ok} and the SSE is calculated. If the SSE after replacing oj with or decreases, it means that or represents the cluster better than oj, and the data points in the set are reassigned according to the nearest medoids now."
"As shown in Figure 9.11, point pi was belonging to the cluster with medoid oj+1 in the first iteration, but after oj was replaced by or, it was found that pi is nearest to the new random medoid and thus gets assigned to it. In this way, the clusters get refined after each medoid is replaced with a new non-representative medoid. Each time a reassignment is done, the SSE based on the new medoid is calculated. The difference between the SSE before and after the swap indicates whether or not the replacement is improving the quality of the clustering by bringing the most similar points together."
"‌Though the k-medoids algorithm provides an effective way to eliminate the noise or outliers in the data set, which was the problem in the k-means algorithm, it is expensive in terms of calculations. The complexity of each iteration in the k-medoids algorithm is O(k(n - k)2). For large value of ‘n’ and ‘k’, this calculation becomes much costlier than that of the k-means algorithm."
"Till now, we have discussed the various methods for partitioning the data into different clusters. But there are situations when the data needs to be partitioned into groups at different levels such as in a hierarchy. The hierarchical clustering methods are used to group the data into hierarchy or tree-like structure. For example, in a machine learning problem of organizing employees of a university in different departments, first the employees are grouped under the different departments in the university, and then within each department, the employees can be grouped according to their roles such as professors, assistant professors, supervisors, lab assistants, etc. This creates a hierarchical structure of the employee data and eases visualization and analysis. Similarly, there may be a data set which has an underlying hierarchy structure that we want to discover and we can use the hierarchical clustering methods to achieve that.There are two main hierarchical clustering methods: agglomerative clustering and divisive clustering."
"Agglomerative clustering is a bottom-up technique which starts with individual objects as clusters and then iteratively merges them to form larger clusters. On the other hand, the divisive method starts with one cluster with all given objectsand then splits it iteratively to form smaller clusters. See Figure 9.12."
The agglomerative hierarchical clustering method uses the bottom-up strategy. It starts with each object forming its own cluster and then iteratively merges the clusters according to their similarity to form larger clusters. It terminates either when a certain clustering condition imposed by the user is achieved or all the clusters merge into a single cluster.
"The divisive hierarchical clustering method uses a top-down strategy. The starting point is the largest cluster with all the objects in it, and then, it is split recursively to form smaller and smaller clusters, thus forming the hierarchy. The end of iterations is achieved when the objects in the final clusters are sufficiently homogeneous to each other or the final clusters contain only one object or the user-defined clustering condition is achieved."
"In both these cases, it is important to select the split and merger points carefully, because the subsequent splits or mergers will use the result of the previous ones and there is no option to perform any object swapping between the clusters or rectify the decisions made in previous steps, which may result in poor clustering quality at the end."
‌A dendrogram is a commonly used tree structure representation of step-by-step creation of hierarchical clustering. It shows how the clusters are merged iteratively (in the case of agglomerative clustering) or split iteratively (in the case of divisive clustering) to arrive at the optimal clustering solution. Figure 9.13 shows a dendro-gram with four levels and how the objects are merged or split at each level to arrive at the hierarchical clustering.
Refer to Figure 9.14 for understanding the concept of these distances.
"‌Often the distance measure is used to decide when to terminate the clustering algorithm. For example, in an agglomerative clustering, the merging iterations may be"
"stopped once the MIN distance between two neighbouring clusters becomes less than the user-defined threshold. So, when an algorithm uses the minimum distance Dmin to measure the distance between the clusters, then it is referred to as nearest neighbour clustering algorithm, and if the decision to stop the algorithm is based on a user-defined limit on Dmin, then it is called single linkage algorithm."
"On the other hand, when an algorithm uses the maximum distance Dmax to measure the distance between the clusters, then it is referred to as furthest neighbour clustering algorithm, and if the decision to stop the algorithm is based on a user- defined limit on Dmax then it is called complete linkage algorithm."
"As minimum and maximum measures provide two extreme options to measure distance between the clusters, they are prone to the outliers and noisy data. Instead, the use of mean and average distance helps in avoiding such problem and provides more consistent results."
"‌You might have noticed that when we used the partitioning and hierarchical clustering methods, the resulting clusters are spherical or nearly spherical in nature. In the case of the other shaped clusters such as S-shaped or uneven shaped clusters, the above two types of method do not provide accurate results. The density-based clustering approach provides a solution to identify clusters of arbitrary shapes. The principle is based on identifying the dense area and sparse area within the data set and then run the clustering algorithm. DBSCAN is one of the popular density-based algorithm which creates clusters by using connected regions with high density."
"Association rule presents a methodology that is useful for identifying interesting relationships hidden in large data sets. It is also known as association analysis, and the discovered relationships can be represented in the form of association rules comprising a set of frequent items. A common application of this analysis is the Market Basket Analysis that retailers use for cross-selling of their products. For example, every large grocery store accumulates a large volume of data about the buying pattern of the customers. On the basis of the items purchased together, the retailers can push some cross-selling either by placing the items bought together in adjacent areas or creating some combo offer with those different product types. The below association rule signifies that people who have bought bread and milk have often bought egg also; so, for the retailer, it makes sense that these items are placed together for new opportunities for cross-selling."
"The application of association analysis is also widespread in other domains such as bioinformatics, medical diagnosis, scientific data analysis, and web data mining. For example, by discovering the interesting relationship between food habit and patients developing breast cancer, a new cancer prevention mechanism can be found which will benefit thousands of people in the world. In this book, we will mainly illustrate the analysis techniques by using the market basket example, but it can be used more widely across domains to identify association among items in transactional data. The huge pool"
"of data generated everyday through tracked transactions such as barcode scanner, online purchase, and inventory tracking systems has enabled for machine learning systems to learn from this wealth of data. We will discuss the methods for finding useful associations in large databases by using simple statistical performance measures while managing the peculiarities of working with such transactional data. One significant challenge in working with the large volume of data is that it may be computationally very expensive to discover patterns from such data. Moreover, there may be cases when some of the associations occurred by chance, which can lead to potentially false knowledge. While discussing the association analysis, we will discuss both these points."
We will use the transaction data in the table below for our examples of association analysis. This simplified version of the market basket data will show how the association rules can be effectively used for the market basket analysis.
‌Definition of common terms‌‌Let us understand few common terminologies used in association analysis.
"One or more items are grouped together and are surrounded by brackets to indicate that they form a set, or more specifically, an itemset that appears in the data with some regularity. For example, in Table 9.C, {Bread, Milk, Egg} can be grouped together to form an itemset as those are frequently bought together. To generalize this concept, if I = {i1, i2,…, in} are the items in a market basket data and T = {t1, t2,…, tn} are the set of all the transactions, then each transaction ti contains a subset of items from I. A collection of zero or more items is"
Support count denotes the number of transactions in which a particular itemset is present. This is a very important property of an itemset as it denotes the frequency of occurrence for the itemset. This is expressed as
"The result of the market basket analysis is expressed as a set of association rules that specify patterns of relationships among items. A typical rule might be expressed as{Bread, Milk}"
"It should be noted that an association rule is an expression of X → Y where X and Y are disjoint itemsets, i.e. X n Y = 0.Support and confidence are the two concepts that are used for measuring the strength of an association rule. Support denotes how often a rule is applicable to a given data set."
"Confidence indicates how often the items in Y appear in transactions that contain X in a total transaction of N. Confidence denotes the predictive power or accuracy of the rule. So, the mathematical expressions are"
"It is important to understand the role of support and confidence in the association analysis. A low support may indicate that the rule has occurred by chance. Also, from its application perspective, this rule may not be a very attractive business investment as the items are seldom bought together by the customers. Thus, support can provide the intelligence of identifying the most interesting rules for analysis."
"Similarly, confidence provides the measurement for reliability of the inference of a rule. Higher confidence of a rule X → Y denotes more likelihood of to be present in transactions that contain X as it is the estimate of the conditional probability of Y given X."
"Also, understand that the confidence of X leading to Y is not the same as the confidence of Y leading to X. In our example, confidence of {Bread, Milk} → {Egg} = 0.75 but confidence"
"Still we will find that association rule learners are closely related to and share many features of the classification rule learners. As association rule learners are unsupervised, there is no need for the algorithm to be trained; this means that no prior labelling of the data is required. The programme is simply run on a data set in the hope that interesting associations are found."
"Obviously, the downside is that there is not an easy way to objectively measure the performance of a rule learner, aside from evaluating them for qualitative usefulness. Also, note that the association rule analysis is used to search for interesting connections among a very large number of variables. Though human beings are capable of such insight quite intuitively, sometimes it requires expert-level knowledge or a great deal of experience to achieve the performance of a rule-learning algorithm. Additionally, some data may be too large or complex for humans to decipher and analyse so easily."
"As discussed earlier, the main challenge of discovering an association rule and learning from it is the large volume of transactional data and the related complexity. Because of the variation of features in transactional data, the number of feature sets within a data set usually becomes very large. This leads to the problem of handling a very large number of itemsets, which grows exponentially with the number of features. If there are k items which may or may not be part of an itemset, then there is 2k ways of creating itemsets with"
"those items. For example, if a seller is dealing with 100 different items, then the learner need to evaluate 2100 = 1 × e30 itemsets for arriving at the rule, which is computationally impossible. So, it is important to filter out the most important (and thus manageable in size) itemsets and use the resources on those to arrive at the reasonably efficient association rules."
"Step 1: decouple the support and confidence requirements.the itemset {Bread, Milk, Egg} is rare in the basket transactions, then all these six rules can be discarded without computing their individual support and confidence values."
"This identifies some important strategies for arriving at the association rules:Generate Frequent Itemset: Once the minS is set for a particular assignment, identify all the itemsets that satisfy minS. These itemsets are called frequent itemsets."
"Please note that the computation requirement for identifying frequent itemsets is more intense than the rule generation. So, different techniques have been evolved to optimize the performance for frequent itemset generation as well as rule discovery as discussed in the next section."
One of the most widely used algorithm to reduce the number of itemsets to search for the association rule is known as Apriori. It has proven to be successful in simplifying the association rule learning to a great extent. The principle got its name from the fact that the algorithm utilizes a simple prior belief (i.e. a priori) about the properties of frequent itemsets:
"This principle significantly restricts the number of itemsets to be searched for rule generation. For example, if in a market basket analysis, it is found that an item like ‘Salt’ is not so frequently bought along with the breakfast items, then it is fine to remove all the itemsets containing salt for rule generation astheir contribution to the support and confidence of the rule will be insignificant."
These are very powerful principles which help in pruning the exponential search space based on the support measure and is known as support-based pruning. The key property of the support measure used here is that the support for an itemset never exceeds the support for its subsets. This is also known as the anti-monotone property of the support measure.
Let us use the transaction data in Table 9.C to illustrate the Apriori principle and its use. From the full itemset of six items
"{Bread, Milk, Egg, Butter, Salt, Apple}, there are 26 ways to create baskets or itemsets (including the null itemset) as shown in Figure 9.15:"
"‌Let us apply the Apriori principle on this data set to reduce the number of candidate itemsets (N). We could identify from the transaction Table 9.C that Salt is an infrequent item. So, by"
"applying the Apriori principle, we can say that all the itemsets which are superset of Salt will be infrequent and thus can be discarded from comparison to discover the association rule as shown in Figure 9.16."
"This approach reduces the computation effort for a good number of itemsets and will make our search process more efficient. Thus, in each such iteration, we can determine the support count of each itemset, and on the basis of the min support value fixed for our analysis, any itemset in the hierarchy that does not meet the min support criteria can be discarded to make the rule generation faster and easier."
"To generalize the example in order to build a set of rules with the Apriori principle, we will use the Apriori principle that states that all subsets of a frequent itemset must also be frequent. In other words, if {X, Y} is frequent, then both {X} and {Y} must be frequent. Also by definition, the support metric indicates how frequently an itemset appears in the data. Thus, if we know that {X} does not meet a desired support threshold, there is no reason to consider {X, Y} or any itemset containing {X}; it cannot possibly be frequent."
This logic of the Apriori algorithm excludes potential association rules prior to actually evaluating them. The actual process of creating rules involves two phases:
"evaluates the set of one-item itemsets (one-itemsets), iteration 2 involves evaluating the two-itemsets, etc.. The result of each iteration N is a set of all N-itemsets that meet the minimum support threshold. Normally, all the itemsets from iteration N are combined in order to generate candidate itemsets for evaluation in iteration N + 1, but by applying the Apriori principle, we can eliminate some of them even before the next iteration starts. If {X}, {Y}, and {Z} are frequent in iteration 1 while {W} is not frequent, then iteration 2 will consider only"
"{X, Y}, {X, Z}, and {Y, Z}. We can see that the algorithm needs to evaluate only three itemsets rather than the six that would have been evaluated if sets containing W had not been eliminated by apriori."
"By continuing with the iterations, let us assume that during iteration 2, it is discovered that {X, Y} and {Y, Z} are frequent, but {X, Z} is not. Although iteration 3 would normally begin by evaluating the support for {X, Y, Z}, this step need not occur at all. The Apriori principle states that {X, Y, Z} cannot be frequent, because the subset {X, Z} is not. Therefore, in iteration 3, the algorithm may stop as no new itemset can be generated."
"Once we identify the qualifying itemsets for analysis, the second phase of the Apriori algorithm begins. For the given set of frequent itemsets, association rules are generated from all possible subsets. For example, {X, Y} would result in candidate rules for {X} → {Y} and {Y} → {X}. These rules are evaluated against a minimum confidence threshold, and any rule that does not meet the desired confidence level is eliminated, thus finally yielding the set of strong rules."
‌Though the Apriori principle is widely used in the market basket analysis and other applications of association rule help
"in the discovery of new relationship among objects, there are certain strengths and weaknesses we need to keep in mind before employing it over the target data set:"
"Unsupervised learning is a machine learning concept where the unlabelled and unclassified information is analysed to discover hidden knowledge. The algorithm works on the data without any prior training, but they are constructed in such a way that they can identify patterns, groupings, sorting order, and numerous other interesting knowledge from the set of data."
"Clustering refers to a broad set of techniques for finding subgroups, or clusters, in a data set based on the characteristics of the objects within the data set itself in such a manner that the objects within the group are similar (or related to each other) but are different from (or unrelated to) the objects from the other groups."
"The major clustering techniques are classified in three board categories  Partitioning methods,"
"The principle of k-means algorithm is to assign each of the n data points to one of the k clusters, where k is a user-defined parameter as the number of clusters desired. The objective is to maximize the homogeneity within the clusters and to maximize the differences between the clusters. The homogeneity and differences are measured in terms of the distance between the points.The hierarchical clustering methods are used to group the data into hierarchy or tree-like structure."
"There are two main hierarchical clustering methods: agglomerative clustering and divisive clustering. Agglomerative clustering is a bottom-up technique which starts with individual objects as clusters and then iteratively merges them to form larger clusters. On the other hand, the divisive method starts with one cluster with all given objects and then splits it iteratively to form smaller clusters."
DBSCAN is one of the density-based clustering approaches that provide a solution to identify clusters of arbitrary shapes. The principle is based on identifying the dense area and sparse area within the data set and then running the clustering algorithm.
"Association rule presents a methodology that is useful for identifying interesting relationships hidden in large data sets. It is also known as association analysis, and the discovered relationships can be represented in the form of association rules comprising a set of frequent items.A common application of this analysis is the Market Basket Analysis that retailers use for cross-selling of their products."
Which of the below statement describes the difference between agglomerative and divisive clustering techniques correctly?
"Agglomerative is a bottom-up technique, but divisive is a top- down technique"
"Agglomerative is a top-down technique, but divisive is a bottom- up technique"
Which of the below is an advantage of k-medoids algorithm over k- means algorithm?
The principle underlying the Market Basket Analysis is known as
Quality of clusteringOne of the disadvantages of k-means algorithm is that the outliers may reduce the quality of the final clustering.
Assignment of observations to clusters does not change between iterations. Except for cases with a bad local minimum.Centroids do not change between successive iterations.
Which of the following clustering algorithm is most sensitive to outliers?
In which of the following situations the K-Means clustering fails to give good results?
Mention few application areas of unsupervised learning.What are the broad three categories of clustering techniques? Explain the characteristics of each briefly.
Describe the main difference in the approach of k-means and k-medoids algorithms with a neat diagram.What is a dendrogram? Explain its use.
Describe the concept of single link and complete link in the context of hierarchical clustering.How apriori principle helps in reducing the calculation overhead for a market basket analysis? Provide an example to explain.
"You are given a set of one-dimensional data points: {5, 10, 15, 20, 25, 30, 35}. Assume that k = 2 and first set of random centroid is selected as {15, 32} and then it is refined with {12, 30}."
Create two clusters with each set of centroid mentioned above following the k-means approach
Explain the Apriori algorithm for association rule learning with an example.How the distance between clusters is measured in hierarchical clustering? Explain the use of this measure in making decision on when to stop the iteration.
Discuss the strengths and weaknesses of the k-means algorithm.Explain the concept of clustering with a neat diagram.
"During a research work, you found 7 observations as described with the data points below. You want to create 3 clusters from these observations using K-means algorithm. After first iteration, the clusters C1, C2, C3 has following observations:"
If you want to run a second iteration then what will be the cluster centroids? What will be the SSE of this clustering?
"In a software project, the team is trying to identify the similarity of software defects identified during testing. They wanted to create 5 clusters of similar defects based on the text analytics of the defect descriptions. Once the 5 clusters of defects are identified, any new defect created is to be classified as one of the types identified through clustering. Explain this approach through a neat diagram. Assume 20 Defect data points which are clustered among 5 clusters and k-means algorithm was used."
"In the last 9 chapters, you have been introduced to the concepts of machine learning in great details. You started with how to decide whether a problem can be solved with machine learning, and once you have made that decision, you learnt how to start with the modelling of the problem in the machine learning paradigm. In that context, you were introduced to three types of machine learning – supervised, unsupervised, and reinforcement. Then, you explored all different popular algorithms of supervised and unsupervised learning. Now, you have gained quite some background on machine learning basics, and it is time for you to get introduced to the concept of neural network."
"Well, you have already seen right at the beginning of the book how the machine learning process maps with the human learning process. Now, it is time to see how the human nervous system has been mimicked in the computer world in the form of an artificial neural network or simply a neural network. This chapter gives a brief view of neural networks and how it helps in different forms of learning."
"The nervous system is constituted of a special type of cell, called neuron or nerve cell, which has special structures allowing it to receive or send signals to other neurons.Neurons connect with each other to transmit signals to or receive signals from other neurons. This structure essentially forms a network of neurons or a neural network."
"By virtue of billions of networked neurons that it possesses, the biological neural network is a massively large and complex parallel computing network. It is because of this massive parallel computing network that the nervous system helps human beings to perform actions or take decisions at a speed and with such ease that the fastest supercomputer of the world will also be envy of. For example, let us think of the superb flying catches taken by the fielders in the cricket world cup. It is a combination of superior calculation based on past cricketing experience, understanding of local on-ground conditions, and anticipation of how hard the ball has been hit that the fielder takes the decision about when to jump, where to jump, and how much to jump. This is a highly complex task, and you may think that not every human being is skilled enough for such a magical action. In that case, let us think of"
"something much simpler. Let us consider about a very simple, daily activity, namely swimming in the pool. Apparently, swimming may look very trivial, but think about the parallel actions and decisions that need to be taken to swim. It needs the right combination of body position, limb movement, breathe in/out, etc. to swim. To add to the challenge is the fact that water is few hundred times denser than air. So, to orient the body movement in that scale difference is a difficult proposition in itself. Coordinating the actions and taking the decisions for such a complex task, which may appear to be trivial, are possible because of the massive parallel complex network, i.e. the neural network."
"The fascinating capability of the biological neural network has inspired the inception of artificial neural network (ANN). An ANN is made up of artificial neurons. In its most generic form, an ANN is a machine designed to model the functioning of the nervous system or, more specifically, the neurons. The only difference is that the biological form of neuron is replicated in the electronic or digital form of neuron. Digital neurons or artificial neurons form the smallest processing units of the ANNs. As we move on, let us first do a deep dive into the structure of the biological neuron and then try to see how that has been modelled in the artificial neuron."
"and organs. Neurons are basic structural units of the CNS. A neuron is able to receive, process, and transmit information in the form of chemical and electrical signals. Figure 10.1 presents the structure of a neuron. It has three main parts to carry out its primary functionality of receiving and transmitting information:"
There is a very small gap between the axon terminal of one neuron and the adjacent dendrite of the neighbouring neuron. This small gap is known as synapse. The signals transmitted through synapse may be excitatory or inhibitory.
"The adult human brain, which forms the main part of the central nervous system, is approximately 1.3 kg in weight and 1200 cm3 in volume. It is estimated to contain about 100 billion (i.e. 1011) neurons and 10 times more glial or glue cells. Glial cells act as support cells for the neurons. It is believed that neurons represent about 10% of all cells in the brain. On an average, each neuron is connected to 105 of other neurons, which means that altogether there are 1016 connections.The axon, a human neuron, is 10–12 μm in diameter. Each synapse spans a gap of about a millionth of an inch wide."
"The biological neural network has been modelled in the form of ANN with artificial neurons simulating the function of biological neurons. As depicted in Figure 10.2, input signal xi (x1, x2, …, xn) comes to an artificial neuron. Each neuron has three major components:"
"A set of ‘i’ synapses having weight wi. A signal xi forms the input to the i-th synapse having weight wi. The value of weight wi may be positive or negative. A positive weight has an excitatory effect, while a negative weight has an inhibitory effect on the output of the summation junction, ysum."
"A summation junction for the input signals is weighted by the respective synaptic weight. Because it is a linear combiner or adder of the weighted input signals, the output of the summation junction, ysum, can be expressed as follows:"
"[Note: Typically, a neural network also includes a bias which adjusts the input of the activation function. However, for the sake of simplicity, we are ignoring bias for the time being. In the case of a bias ‘b’, the value of ysum would have been as follows:"
"A threshold activation function (or simply activation function, also called squashing function) results in an output signal only when an input signal exceeding a specific threshold value comes as an input. It is similar in behaviour to the biological neuron which transmits the signal only when the total input signal meets the firing threshold."
"Output of the activation function, yout, can be expressed as follows:"
‌TYPES OF ACTIVATION FUNCTIONS‌‌There are different types of activation functions. The most commonly used activation functions are highlighted below.
"Step/threshold function is a commonly used activation function. As depicted in Figure 10.3a, step function gives 1 as output if the input is either 0 or positive. If the input is negative, the step function gives 0 as output. Expressing mathematically,"
"This function is differentiable, except at a single point x = 0."
"Sigmoid function, depicted in Figure 10.5, is by far the most commonly used activation function in neural networks. The need for sigmoid function stems from the fact that many learning algorithms require the activation function to be differentiable and hence continuous. Step function is not suitable in those situations as it is not continuous. There are two types of sigmoid function:"
"‌The slope at origin is k/4. As the value of k becomes very large, the sigmoid function becomes a threshold function."
"A bipolar sigmoid function, depicted in Figure 10.5b, is of the form"
"Hyperbolic tangent function is another continuous activation function, which is bipolar in nature. It is a widely adopted activation function for a special type of neural network known as backpropagation network (discussed elaborately in Section 10.8). The hyperbolic tangent function is of the form"
Works related to neural network dates long back to the 1940s. Warren McCulloch and Walter Pitts in 1943 created a computational network inspired by the biological processes in the brain. This model paved the way for neural networks.
The next notable work in the area of neural network was in the late 1940s by the Canadian psychologist Donald Olding Hebb. He proposed a learning hypothesis based on the neurons and synaptic connection between neurons.This became popular as Hebbian learning.
"In 1958, the American psychologist Frank Rosenblatt refined the Hebbian concept and evolved the concept of perceptron. It was almost at the same time, in 1960, that Professor Bernard Widrow of Stanford University came up with an early single-layer ANN named ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element). Marvin Lee Minsky and Seymour Aubrey Papert stated two key issues of perceptron in their research paper in 1969. The first issue stated was theinability of perceptron of processing the XOR (exclusive- OR) circuit. The other issue was lack of processing power of computers to effectively handle the large neural networks."
"From here till the middle of the 1970s, there was a slowdown in the research work related to neural networks. There was a renewed interest generated in neural networks and learning with Werbos’ backpropagation algorithm in 1975. Slowly, as the computing ability of the computers increased drastically, a lot of research on neural network has been conducted in the later decades. Neural network"
"evolved further as deep neural networks and started to be implemented in solving large-scale learning problems such as image recognition, thereby getting a more popular name as deep learning."
"The McCulloch–Pitts neural model (depicted in Fig. 10.6), which was the earliest ANN model, has only two types of inputs – excitatory and inhibitory. The excitatory inputs have weights of positive magnitude and the inhibitory weights have weights of negative magnitude. The inputs of the McCulloch– Pitts neuron could be either 0 or 1. It has a threshold function as activation function. So, the output signal yout is 1 if the input ysum is greater than or equal to a given threshold value, else 0."
"To analyse the situations using the McCulloch–Pitts neural model, we can consider the input signals as follows:"
"The truth table built with respect to the problem is depicted in Figure 10.7b. From the truth table, we can conclude that in the situations where the value of yout is 1, John needs to carry an umbrella. Hence, he will need to carry an umbrella in situations 2, 3, and 4. Surprised, as it looks like a typical logic problem related to ‘OR’ function? Do not worry, it is really an implementation of logical OR using the McCulloch–Pitts neural model."
"Rosenblatt’s perceptron is built around the McCulloch–Pitts neural model. The perceptron, as depicted in Figure 10.7, receives a set of input x1, x2,…, xn. The linear combiner or the adder node computes the linear combination of the inputs applied to the synapses with synaptic weights being w1, w2,"
"sum is positive or negative. If the input of the hard limiter node is positive, the output is +1, and if the input is negative, the output is –1. Mathematically, the hard limiter input is"
"However, perceptron includes an adjustable value or bias as an additional weight w0. This additional weight w0 is attached to a dummy input x0, which is always assigned a value of 1."
"The objective of perceptron is to classify a set of inputs into two classes, c1 and c2. This can be done using a very simple decision rule – assign the inputs x0, x1, x2, …, xn to c1 if the output of the perceptron, i.e. yout, is +1 and c2 if yout is –1. So, for an n-dimensional signal space, i.e. a space for ‘n’ input signals x0, x1, x2, …, xn, the simplest form of perceptron will have two decision regions, resembling two classes, separated by a hyperplane defined by"
"‌Therefore, for two input signals denoted by variables x1 and"
"So, for a perceptron having the values of synaptic weights w0, w1, and w2 as –2, ½, and ¼, respectively, the linear decision boundary will be of the form"
"So, any point (x1, x2) which lies above the decision boundary, as depicted by Figure 10.9, will be assigned to class c1 and the points which lie below the boundary are assigned to class c2."
‌Let us examine if this perceptron is able to classify a set of points given below:
"As depicted in Figure 10.10, we can see that on the basis of activation function output, only points p1 and p2 generate an output of 1. Hence, they are assigned to class c1 as expected. On the other hand, p3 and p4 points having activation function output as negative generate an output of 0. Hence, they are assigned to class c2, again as expected."
"Thus, we can see that for a data set with linearly separable classes, perceptrons can always be employed to solve classification problems using decision lines (for two- dimensional space), decision planes (for three-dimensional space), or decision hyperplanes (for n-dimensional space)."
"Appropriate values of the synaptic weights w0, w1, w2, …, wn can be obtained by training a perceptron. However, one assumption for perceptron to work properly is that the two classes should be linearly separable (as depicted in Figure 10.12a), i.e. the classes should be sufficiently separated from each other. Otherwise, if the classes are non-linearly separable (as depicted in Figure 10.12b), then the classification problem cannot be solved by perceptron."
"As we can see in Figure 10.13, the data is not linearly separable. Only a curved decision boundary can separate the classes properly."
"To address this issue, the other option is to use two decision lines in place of one. Figure 10.14 shows how a linear decision boundary with two decision lines can clearly partition the data."
This is the philosophy used to design the multi-layer perceptron model. The major highlights of this model are as follows:
The diagram in Figure 10.15 resembles a fully connected multi-layer perceptron with multiple hidden layers between the input and output layers. It is called fully connected because any neuron in any layer of the perceptron is connected with all neurons (or input nodes in the case of the first hidden layer) in the previous layer. The signals flow from one layer to another layer from left to right.
‌The supervised learning algorithm adopted by the ADALINE network is known as Least Mean Square (LMS) or Delta rule.A network combining a number of ADALINEs is termed as MADALINE (many ADALINE). MADALINE networks can be used to solve problems related to nonlinear separability.
‌Both perceptron and ADALINE are neural network models. Both of them are classifiers for binary classification. They have linear decision boundary and use a threshold activation function.
"As we have seen till now, ANN is a computational system consisting of a large number of interconnected units called"
artificial neurons. The connection between artificial neurons can transmit signal from one neuron to another. There are multiple possibilities for connecting the neurons based on which architecture we are going to adopt for a specific solution. Some of the choices are listed below:
"Single-layer feed forward is the simplest and most basic architecture of ANNs. It consists of only two layers as depicted in Figure 10.17 – the input layer and the output layer. The input layer consists of a set of ‘m’ input neurons X1, X2,"
"…, Xm connected to each of the ‘n’ output neurons Y1, Y2, …, Yn. The connections carry weights w11, w12, …, wmn. The input layer of neurons does not conduct any processing – they pass the input signals to the output neurons. The computations are performed only by the neurons in the output layer. So, though it has two layers of neurons, only one layer is performing the computation. This is the reason why the network is known as single layer in spite of having two layers of neurons. Also, the signals always flow from the input layer to the output layer.Hence, this network is known as feed forward."
for the k-th output neuron. The signal output from each output neuron will depend on the activation function used.
The net signal input to the neuron in the hidden layer is given by
The competitive network is almost the same in structure as the single-layer feed forward network. The only difference is that the output neurons are connected with each other (either partially or fully). Figure 10.19 depicts a fully connected competitive network.
"In competitive networks, for a given input, the output neurons compete amongst themselves to represent the input. It represents a form of unsupervised learning algorithm in ANN that is suitable to find clusters in a data set."
"We have seen that in feed forward networks, signals always flow from the input layer towards the output layer (through the hidden layers in the case of multi-layer feed forward networks), i.e. in one direction. In the case of recurrent neural networks, there is a small deviation. There is a feedback loop,as depicted in Figure 10.20, from the neurons in the output layer to the input layer neurons. There may also be self-loops."
"‌Now that we have a clear idea about neurons, how they form networks using different architectures, what is activation function in a neuron, and what are the different choices of activation functions, it is time to relate all these to our main focus, i.e. learning. First, we need to understand what is learning in the context of ANNs? There are four major aspects which need to be decided:"
The value of weights attached with each interconnection between neurons
"‌As we have seen earlier, a neural network may have a single layer or multi-layer. In the case of a single layer, a set of neurons in the input layer receives signal, i.e. a single feature per neuron, from the data set. The value of the feature is transformed by the activation function of the input neuron. The signals processed by the neurons in the input layer are then forwarded to the neurons in the output layer. The neurons in the output layer use their own activation function to generate the final prediction.More complex networks may be designed with multiple hidden layers between the input layer and the output layer. Most of the multi-layer networks are fully connected."
"However, certain networks, such as the recurrent network, also allow signals to travel from the output layer to the input layer.This is also an important consideration for choosing the correct learning model."
"In the case of a multi-layer network, the number of nodes in each layer can be varied. However, the number of nodes or neurons in the input layer is equal to the number of features of the input data set. Similarly, the number of output nodes will depend on possible outcomes, e.g. number of classes in the case of supervised learning. So, the number of nodes in each"
"of the hidden layers is to be chosen by the user. A larger number of nodes in the hidden layer help in improving the performance. However, too many nodes may result in overfitting as well as an increased computational expense."
"We have a set of points with known labels as given below. We have to train an ANN model using this data, so that it can classify a new test data, say p5 (3, –2).The answer is a big NO."
"For solving a learning problem using ANN, we can start with a set of values for the synaptic weights and keep doing changes to those values in multiple iterations. In the case of supervised learning, the objective to be pursued is to reduce the number of misclassifications. Ideally, the iterations for making changes in weight values should be continued till there is no misclassification. However, in practice, such a stopping"
"‌criterion may not be possible to achieve. Practical stopping criteria may be the rate of misclassification less than a specific threshold value, say 1%, or the maximum number of iterations reaches a threshold, say 25, etc. There may be other practical challenges to deal with, such as the rate of misclassification is not reducing progressively. This may become a bigger problem when the number of interconnections and hence the number of weights keeps increasing. There are ways to deal with those challenges, which we will see in more details in the next section."
"So, to summarize, learning process using ANN is a combination of multiple aspects – which include deciding the number of hidden layers, number of nodes in each of the hidden layers, direction of signal flow, and last but not the least, deciding the connection weights."
"Multi-layer feed forward network is a commonly adopted architecture. It has been observed that a neural network with even one hidden layer can be used to reasonably approximate any continuous function. The learning method adopted to train a multi-layer feed forward network is termed as backpropagation, which we will study in the next section."
"We have already seen that one of the most critical activities of training an ANN is to assign the inter-neuron connection weights. It can be a very intense work, more so for the neural networks having a high number of hidden layers or a high number of nodes in a layer. In 1986, an efficient method of training an ANN was discovered. In this method, errors, i.e. difference in output values of the output layer and the expected values, are propagated back from the output layer to the preceding layers. Hence, the algorithm implementing thismethod is known as backpropagation, i.e. propagating the errors backward to the preceding layers."
"The backpropagation algorithm is applicable for multi-layer feed forward networks. It is a supervised learning algorithm which continues adjusting the weights of the connected neurons with an objective to reduce the deviation of the output signal from the target output. This algorithm consists of multiple iterations, also known as epochs. Each epoch consists of two phases –The iterations continue till a stopping criterion is reached. Figure 10.21 depicts a reasonably simplified version of the backpropagation algorithm."
"One main part of the algorithm is adjusting the interconnection weights. This is done using a technique termed as gradient descent. In simple terms, the algorithm calculates the partial derivative of the activation function by each interconnection weight to identify the ‘gradient’ or extent of change of the weight required to minimize the cost function.Quite understandably, therefore, the activation function needs to be differentiable. Let us try to understand this in a bit more details."
"A real-world simile for the gradient descent algorithm is a blind person trying to come down from a hill top without anyone to assist. The person is not able to see. So, for the person who is not able to see the path of descent, the only"
"option is to check in which direction the land slope feels to be downward. One challenge of this approach arises when the person reaches a point which feels to be the lowest, as all the points surrounding it is higher in slope, but in reality it is not so. Such a point, which is local minima and not global minima, may be deceiving and stalls the algorithm before reaching the real global minima."
"for the k-th neuron in the hidden layer. If fy is the activation function of the hidden layer, then"
"for the k-th neuron in the output layer. Note that the input signals to X0 and Y0 are assumed as 1. If fz is the activation function of the hidden layer, then"
"If tk is the target output of the k-th output neuron, then the cost function defined as the squared error of the output layer is given by"
"So, as a part of the gradient descent algorithm, partial derivative of the cost function E has to be done with respect to each of the interconnection weights w’01,w’02, … ,w’nr."
for the interconnection weight between the j-th neuron in the hidden layer and the k-th neuron in the output layer. This expression can be deduced to
"In the same way, we can perform the calculations for the interconnection weights between the input and hidden layers. The weights and bias for the interconnection between the input and hidden layers need to be updated as follows:"
"Learning rate is a user parameter which increases or decreases the speed with which the interconnection weights of a neural network is to be adjusted. If the learning rate is too high, the adjustment done as a part of the gradient descent process may miss the minima. Then, the training may not converge, and it may even diverge. On the other hand, if the learning rate is too low, the optimization may consume more time because of the small steps towards the minima."
"A balanced approach is to start the training with a relatively large learning rate (say 0.1), because in the beginning, random weights are far from optimal. Then, the learning rate should be decreased during training (say exponentially lower values, e.g. 0.01, 0.001, etc.) to allow more fine-grained weight updates."
"Neural networks, as we have already seen in this chapter, are a class of machine learning algorithms. As we have also seen, there are multiple choices of architectures for neural networks,"
"multi-layer neural network being one of the most adopted ones. However, in a multi-layer neural network, as we keep increasing the number of hidden layers, the computation becomes very expensive. Going beyond two to three layers becomes quite difficult computationally. The only way to handle such intense computation is by using graphics processing unit (GPU) computing."
"When we have less number of hidden layers – at the maximum two to three layers, it is a normal neural network, which is sometimes given the fancy name ‘shallow neural network’. However, when the number of layers increases, it is termed as deep neural network. One of the earliest deep neural networks had three hidden layers. Deep learning is a more contemporary branding of deep neural networks, i.e. multilayer neural networks having more than three layers.More detailed understanding of deep learning is beyond the scope of this book."
"‌SUMMARY‌In an ANN, the biological form of neuron is replicated in the electronic or digital form of neuron."
There are four major aspects which need to be decided while learning using ANN
"In the backpropagation algorithm, errors are propagated back from the output layer to the preceding layers. The backpropagation algorithm is applicable for multi-layer feed forward network.A technique termed as gradient descent is used to adjust the interconnection weights between neurons of different layers."
"When the number of hidden layers in multi-layer neural networks is higher than three, it is termed as deep neural network. Deep learning is a more contemporary branding of deep neural networks."
SAMPLE QUESTIONS MULTIPLE CHOICE QUESTIONSANN is made up of neurons.
axonA neuron is able to       information in the form of chemical and electrical signals.
all of the abovesummation junction is a       for the input signals.
None of the aboveStep function gives     as output if the input is either 0 or positive.
4. None of the aboveA binary sigmoid function has range of     .
"4. (1, 0)The inputs of the McCulloch–Pitts neuron could be     ."
none of the aboveSingle-layer feed forward network consists of layers.
"manyIn competitive networks, output neurons are connected with ."
Which of the following are critical aspects of learning in ANN?
"In the backpropagation algorithm, multiple iterations are known as"
none of the aboveDeep neural networks generally have more than hidden layers.
"4. none of the aboveTo handle intense computation of deep learning, is needed."
What is the function of a summation junction of a neuron? What is threshold activation function?
Explain the McCulloch–Pitts model of neuron.Explain the ADALINE network model.
What is the constraint of a simple perceptron? Why it may fail with a real-world data set?
Explain the competitive network architecture of ANN.Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network.
What are the advantages and disadvantages of neural networks?
Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?What are the different types of activation functions popularly used? Explain each of them.
Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem.What is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN.
"Explain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
"Explain, in details, the backpropagation algorithm. What are the limitations of this algorithm?Describe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
What are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?
"‌In the previous chapters, you have got good understanding of supervised learning algorithm and unsupervised learning algorithm. In this chapter, we will discuss learning types which were not covered in the earlier chapters which includes representation learning, active learning, instance- based Learning, association rule, and ensemble learning.By the end of this chapter, you will gain sufficient knowledge in all the aspects of learning and become ready to start solving problems on your own."
"In this chapter, we will discuss learning types which were not covered in the earlier chapters which include representation learning, active learning, instance-based Learning, association rule, and ensemble learning."
"Another name for representation learning is feature learning. While many enterprises nowadays retain substantial amounts of business-related data, most of those are often unstructured and unlabelled. Creating new labels is generally a time- consuming and expensive endeavour. As a result, machine learning algorithms that can straight away mine structures from unlabelled data to improve the business performance are reasonably valued. Representation learning is one such type where the extraction from the overall unlabelled data happens using a ‘neural network’. The main objective of representation learning (feature learning) is to find an appropriate representation of data based on features to perform a machine learning task. Representation learning has become a field in itself because of its popularity. Refer the chapter 4 titled ‘Basics of Feature Engineering’ to understand more on feature and feature selection process. An example of some of the characteristics of a triangle are its corners (or vertices), sides, and degree of angle. Every triangle has three corners and three angles. Sum of the three angles of a triangle is equal to 180°."
Consider you are tasked to classify/identify the types of triangles (Refer Table 11.1). The way you do that is to find unique characteristics of the type of triangle given as input.
"The system may work something like thisGreat! Now, we have a primary representational working system to detect the type of triangle."
"What happens when we begin to input shapes like square, rectangle, circle, or all sort of shapes instead of a triangle?"
What if the image contains both a triangle and a circle? Designing to adapt to this kind of features requires deep domain expertise as we start working with real-world applications. Deep Learning goes one step further and learns/tries to learn features on its own. All we would do is to feed in an image as input and let the system learn features like human beings do.
"Representation learnings are most widely used in words, speech recognition, signal processing, music, and image identification. For example, a word can have meaning based on the context."
"In Figure 11.1, V represents the input data, and H represents the causes. When the causes (H) explains the data (V) we observe, it is called as Recognition. When unknown causes(H) are combined, it can also generate the data (V), it is called as generative (or) Generation."
"Representation learning is inspired by the fact that machine learning tasks such as classification regularly requires numerical inputs that are mathematically and computationally easy to process. However, it is difficult to define the features algorithmically for real-world objects such as images and videos. An alternative is to discover such features orrepresentations just through examination, without trusting on explicit algorithms. So, representation learning can be either supervised or unsupervised."
‌Supervised neural networks and multilayer perceptron‌Refer the chapter 10 titled ‘Basics of Neural Network’ to understand more on the topic of the neural network.
"Independent component analysis, or ICA is similar to principal component analysis (PCA), with the exception that the components are independent and uncorrelated. ICA is used primarily for separating unknown source signals from their observed linear mixtures after the linear mixture with an unfamiliar matrix (A) . Nil information is known about the sources or the mixing process except that there are N different recorded mixtures. The job is to recuperate a version(U) of the sources (S) which are identical except for scaling and permutation, by finding a square matrix (W) specifying spatial filters that linearly invert the mixing process, i.e. U = WX. Maximizing the joint entropy, H(y), of the output of a neural processor minimizes the mutual information among the output components (Refer Fig. 11.2)."
"Autoencoders belong to the neural network family (Refer Fig. 11.3) and are similar to Principal Component Analysis (PCA). They are more flexible than PCA. Autoencoders represents both linear and non-linear transformation, whereas PCA represents only linear transformation. The neural network’s (autoencoder) target output is its input (x) in a different form (x’). In Autoencoders, the dimensionality of the input is equal to the dimensionality of the output, and essentially what we want is x’ = x. x’ = Decode (Encode(x))"
"‌To get the value from the hidden layer, we multiply the (input to hidden) weights by the input. To get the value from the output, we multiply (hidden to output) weights by the hidden layer values."
"Do a feed-forward pass to compute activations at all hidden layers, then at the output layer to obtain an output X ’"
Measure the deviation X ’ from the input XBackpropagate the error through the net and perform weight updates.
‌Various forms of clustering‌Refer chapter 9 titled ‘Unsupervised Learning’ to know more about various forms of Clustering.
"During each iteration, I, P (total population) is broken up into three subsets.P(K, i): Data points where the label is known (K)."
"P(C, i): A subset of P(U, i)that is chosen (C) to be labelled.Current research in this type of active learning concentrates on identifying the best method P(C, i) to chose (C) the data points."
"Some active learning algorithms are built upon aupport vector machines (SVMs) to determine the data points to label. Such methods usually calculate the margin (W) of each unlabelled datum in P(U, i) and assume W as an n-dimensional space distance commencing covering datum and the splitting hyperplane."
"‌Minimum marginal hyperplane (MMH) techniques consent that the data records with the minimum margin (W) are those that the SVM is most uncertain about and therefore should be retained in P(C, i) to be labelled. Other comparable approaches, such as Maximum Marginal Hyperplane, select data with the significant W (n-dimensional space). Other approaches to select a combination of the smallest as well as largest Ws."
Few logics for determining which data points should be labelled are
"Uncertainty sampling: In this method, the active learning algorithm first tries to label the points for which the currentmodel is least specific (as this means a lot of confusion) on the correct output."
"Expected error reduction: In this method, the active learning algorithm first tries to label the points that would mostly reduce the model’s generalization error.Variance reduction: In this method, the active learning algorithm first tries to label the points that would reduce output variance (which is also one of the components of error)."
"In instance-based learning (memory-based learning), the algorithm compares new problem instances with instances already seen in training, which have been stored in memory for reference rather than performing explicit generalization. It is called instance-based as it constructs hypotheses from the training instances (memories) themselves. Computational complexity O(n) of the hypothesis can grow when we have some data (n). It can quickly adapt its model to previously unseen data. New instances are either stored or thrown away based on the previously set criteria."
"Examples of instance-based learning include K-nearest neighbour algorithm (K-NN), kernel machines (support vector"
‌All these algorithms store already known instances and compute distances or similarities between this instance and the training instances to make the final decision. Instance reduction algorithm can be used to overcome memory complexity problem and overfitting problems. Let us look into radial basis function (RBF) in detail.
"Radial basis function (RBF) networks typically have three layers: one input layer, one hidden layer with a non-linear RBF activation function, and one linear output layer (Refer Fig.11.4)."
"In the above diagram, an input vector(x) is used as input to all radial basis functions, each with different parameters. The input layer (X) is merely a fan-out layer, and no processing happens here. The second layer (hidden) performs radial basis functions, which are the non-linear mapping from the input space (Vector X) into a higher order dimensional space. The output of the network (Y) is a linear combination of theThe distinctive part of the radial basis function network (RFFN) is the procedure implemented in the hidden layer. The idea is that the patterns in the input space form clusters."
"Beyond this area (clustering area, RFB function area), the value drops dramatically. The Gaussian function is the most commonly used radial-basis function. In an RBF network, r is the distance from the cluster centre."
"Space (distance) computed from the cluster centre is usually the Euclidean distance. For each neuron that is part of the hidden layer, the weights represent the coordinates of the centre of the cluster. Therefore, when that neuron receives an input pattern, X, the distance is found using the following equation:"
"The variable sigma, σ, denotes the width or radius of the bell-shape and is to be determined by calculation. When the distance from the centre of the Gaussian reaches σ, the output drops from 1 to 0.6."
"‌The output y(t) is a weighted sum of the outputs of the hidden layer, given by"
"In radial functions, the response decreases (or increases) monotonically with distance from a central point and they are radially symmetric. The centre, the distance scale, and the exact shape of the radial function are the necessary considerations of the defined model. The most commonly used radial function is the Gaussian radial filter, which in case of a scalar input is"
"Its parameters are its centre c and its radius β (width), illustrates a Gaussian RBF with centre c = 0 and radius β = 1. A Gaussian RBF monotonically decreases with distance from the centre."
Association rule learning is a method that extracts rules that best explain observed relationships between variables in data. These rules can discover important and commercially useful associations in large multidimensional data sets that can be exploited by an organization. The most popular association rule learning algorithms are Apriori algorithm and Eclat process.
"Apriori is designed to function on a large database containing various types of transactions (for example, collections of products bought by customers, or details of websites visited by customers frequently). Apriori algorithm uses a ‘bottom-up’ method, where repeated subsets are extended one item at a time (a step is known as candidate generation, and groups of candidates are tested against the data). The Apriori Algorithm is a powerful algorithm for frequent mining of itemsets for boolean association rules. Refer chapter 9 titled ‘unsupervised learning’ to understand more on this algorithm."
"Step 3. Intersect tidlist of {a} with the tidlists of all other items, resulting in tidlists of {a,b}, {a,c}, {a,d}, … = {a}–conditional database (if {a} removed).Step 4. Repeat from 1 on {a}–conditional database 5. Repeat for all other items."
Ensemble means collaboration/joint/group. Ensemble methods are models that contain many weaker models that are autonomously trained and whose forecasts are combined approximately to create the overall prediction model. More efforts are required to study the types of weak learners and various ways to combine them.
Stacked Generalization (blending) Gradient Boosting Machines (GBM)
‌Bootstrap Aggregation (bagging) works using the principle of the Bootstrap sampling method. Given a training data set D
"In the above example, the original sample has eight data points. All the Bootstrap samples also have eight data points.Within the same bootstrap sample, data points are repeated due to selection with the replacement."
"‌Just like bagging, boosting is another key ensemble-based technique. Boosting is an iterative technique. It decreases the biasing error. If an observation was classified incorrectly, then it boosts the weight of that observation. In this type of ensemble, weaker learning models are trained on resampled data, and the outcomes are combined using a weighted voting approach based on the performance of different models.Adaptive boosting or AdaBoost is a special variant of a boosting algorithm. It is based on the idea of generating weak learners and learning slowly."
"As we know already, boosting is the machine learning algorithm, which boosts week learner to strong learner. We may already know that a learner estimates a target function from training data. Gradient boosting is the boosting with Gradient."
"Gradient: Consider L as a function of an ith variable, other N – 1 variables are fixed at the current point, by calculating derivative of L at that point (L is differentiable), we have – if the derivative is positive/negative – a decrease/increase value of the ith variable in order to make the loss smaller (*).Applying the above calculation for all i = 1… N, we will get N derivative values forming a vector, so-called gradient of an N- variable function at the current point."
"This is an extension made to another method (typically regression method) that penalizes models based on their complexity, favouring simpler models that are also better at generalizing. Regularization algorithms have been listed separately here because they are famous, influential, and generally simple modifications made to other methods."
Least Absolute Shrinkage and Selection Operator (LASSO) Elastic Net
Least-Angle Regression (LARS)Refer chapter 8 titled “Supervised Learning: Regression” for Ridge regression and the LASSO method.
"When we are working with high-dimensional data sets with a large number of independent variables, correlations (relationships) amid the variables can be often result in multicollinearity. These correlated variables which are strictly related can sometimes form groups or clusters called as an elastic net of correlated variables. We would want to include the complete group in the model selection even if just one variable has been selected."
Autoencoders belongs to neural network family and are similar to PCA (Principal Component Analysis). They are more flexible than PCA. Active learning is a type of semi-supervised learning in which a learning algorithm can interactively query (question) the user to obtain the desired outputs at new data points.
"Uncertainty sampling: In this method, the active learning algorithm first tries to label the points for which the current model is least specific (a that means much confusion) on the correct output."
"Examples of instance-based learning include K-nearest neighbor algorithm (K-NN), Kernel machines (Support Vector Machine, PCA) and Radial Basis Function networks (RBF networks).Radial basis function (RBF) networks typically have three layers: one input layer, one hidden layer with a non-linear RBF activation function and one linear output layer."
Association rule learning is methods that extract rules that best explain observed relationships between variables in data. These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.
"Apriori is designed to function on a large database containing various types of transactions (for example, collections of products bought by customers, or details of websites visited by customers frequently)."
"ECLAT stands for “Equivalence Class Clustering and bottom-up Lattice Traversal.” Eclat algorithm is another set of frequent itemset generation similar to Apriori algorithm. Three traversal approaches such as “Top- down,” “Bottom-up” and Hybrid approaches are supported."
Ensemble means collaboration/joint/Group. Ensemble methods are models that contain many weaker models that are autonomously trained and whose forecasts are joined approximately to create the overall prediction model.
"In representation learning, when unknown causes (H) are combined, it can also generate the data (V), it is called as"
"When the causes (H) explains the data (V) we observe, it is called as"
"When unknown causes (H) are combined, it can also generate the data (V), it is called as"
"In this method, the active learning algorithm first tries to label the points for which the current model is least specific (as this means a lot of confusion) on the correct output."
"Expected error reductionA range of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the ‘Group of people’ disagrees the most."
"In this method, the active learning algorithm first tries to label the points that would most change the existing model itself"
"Expected error reductionIn this method, the active learning algorithm first tries to label the points that would mostly reduce the model’s generalization problem."
Which of the following is not a type of Ensemble learning algorithms?
Define Independent Component Analysis? What is the primary usage of it?
What are the Pros and Cons of Instance-Based Learning (IBL) Method?
LONG-ANSWER TYPE QUESTIONS (10 MARKS QUESTIONS)Derive primary representational working system to detect the type of triangle.
Discuss Generation and Recognition in Representation Learning
Discuss various Association Rule Learning Algorithm in detailWhat is Ensemble Learning Algorithm? Discuss various types.
"‌Before starting with machine learning programming in R, we need to fulfil certain pre-requisites. Quite understandably, the first and foremost activity is to install R and get started with the basic programming interface of R, i.e. R console. Then, we need to become familiar with the R commands and scripting inR. In this section, we will have a step-by-step guide of fulfilling these pre-requisites."
"R 3.5.0 or higher (https://cran.r-project.org/bin/windows/base/) RStudio 1.1.453 or higher (Optional, only if you want to leverage the advantage of using an integrated development environment (IDE)."
The Comprehensive R Archive Network (or CRAN) is a worldwide network of ftp and web servers that store identical and up-to-date versions of code and documentation for R.RStudio is a free and open-source IDE for R.
Open new / pre-existing scripts in R console as shown in Figure A.1:
"Scripts can be written / edited on the R editor window, and console commands can be directly executed on the R console window as shown in"
Factor: The factor stores the nominal values as a vector of integers in the range [1…k] (where k is the number of unique values in the nominal variable) and an internal vector of character strings (the original values) mapped to these items.
Running the function (after compiling the script by using source (‘script_name’)):
"‌For advanced data manipulation, the dplyr library of R (developed by Hadley Wickham et al) can be leveraged. It is the next version of the plyr package focused on working with data frames (hence the name “d”plyr)."
Some additional options to project data elements on the basis of conditions are as follows:
"In R, the pipe (%>%) operator allows to pipe the output from one function to the input of another function. Instead of nesting functions (reading from inside to outside), the idea of piping is to read the functions from left to right."
"Now that we are reasonably familiar with the basic R commands, we have acquired the ability to start machine learning programming in R. But before starting the actual modelling work, we have to first understand the data using the concepts highlighted in Chapter 2. Also, there might be some issues in the data, which we will reveal during data exploration. We have to remediate that too."
"So first, let us find out how to do data exploration in R. There are two ways to explore and understand data:"
By using certain statistical functions to understand the central tendency and spread of the data
By visually observing the data in the form of plots or graphs
"Let us start with the first approach of understanding the data through statistical techniques. As we have seen in Chapter 2, for any data set, it is critical to understand the central tendency and spread of the data. We have also seen that the standard statistical measures used are as follows:"
"Position of the different data values – quartiles, interquartile range (IQR)"
"In R, there is a function summary, which generates the summary statistics of the attributes of a data set. It gives the first basic understanding of the data set, which can trigger thoughts about the data set and the anomalies that may be present. We will use another diagnostic function, str, which compactly provides the structure of a data frame along with"
"the data types of the different attributes. So, let us start exploring a data set Auto MPG data set from the University of California, Irvine (UCI) machine learning repository. We will run the str and summary commands for the Auto MPG data set."
"These measures give quite good understanding of the data set attributes. Now, note that the attribute car.name is not showing these values and showing something else. Why is that so and what are the values that it is showing? Let us try to understand the reason for this difference."
"The attribute car.name is a nominal, i.e. categorical attribute. As we have already seen in Chapter 2, mathematical or statistical operations are not possible for a nominal variable. Hence, only the unique values for the attribute along with the number of occurrences or frequency are given. We can obtain an exhaustive list of nominal attributes using the following R command."
"Next, let us try to explore whether any variable has any issue with the data values where a cleaning may be required. As discussed in Chapter 2, there may be two primary data issues: missing values and outliers."
"Let us first try to determine whether there is any missing value for any of the attributes. Let us use a small piece of R code to find out whether there is any missing/ unwanted value for an attribute in the data. If there is such issue, return the rows in which the attribute has missing/unwanted values. By checking all the attributes, we find that the attribute ‘horsepower’ has missing values."
"There are six rows in the data set, which have missing values for the attribute ‘horsepower’. We will have to remediate these rows before we proceed with the modelling activities. We will do that shortly."
"The easiest and most effective method to detect outliers is from the box plot of the attributes. In the box plot, outliers are very clearly highlighted. When we explore the attributes using box plots in a short while, we will have a clear view of this aspect.‌Let us quickly see the other R commands for obtaining statistical measures of the numeric attributes."
"To perform data exploration (as well as data visualization), the ggplot2 library of R can be leveraged. Created by Hadley Wickham, the ggplot2 library offers a comprehensive graphics module for creating elaborate and complex plots."
Let us now understand the different graphs that are used for data exploration and how to generate them using R code.A separate window opens in R console with the box plot generated as shown in Figure A.3.
"The output of the command, i.e. the scatter plot of the feature pair petal length and sepal length of the iris data set, is shown in Figure A.6."
"‌The primary data pre-processing activities are remediating data issues related to outliers and missing values. Also, feature subset selection is quite a critical area of data pre-processing. Let us understand how to write programmes for achieving these purposes."
"As we saw in Chapter 2, the primary measures for remediating outliers and missing values are as follows:"
"Imputing the value (i.e. outlier/missing value) with a standard statistical measure, e.g. mean or median or mode for that attributeEstimate the value (i.e. outlier/missing value) on the basis of value of the attribute in similar records and replace with the estimated value."
"We have to first identify the outliers. We have already seen in boxplots that outliers are clearly visible when we draw the box plot of a specific attribute. Hence, we can use the same concept as shown in the following code:"
"For missing value identification and removal, the below code is used:"
"The code for identification of outliers or missing values will remain the same. For imputation, depending on which statistical function is to be used for imputation, the code will be as follows:Almost the same code can be used for imputing missing values with the only difference being in the identification of the relevant rows."
"The code for identification of outlier values will remain the same. For capping, generally a value of 1.5 times the IQR is used for imputation, and the code will be as follows:"
"To start using the functions of the caret package, we need to include the caret as follows:"
"The first step before the start of modelling, in the case of supervised learning, is to load the input data, holdout a portion of the input data as test data, and use the remaining portion as training data for building the model. Below is the standard procedure to do it."
executing the same code or R function may result in different training data sets. The model trained will also be somewhat different.
"In R, there is a set.seed function which sets the starting point of the random number generator used internally to pick the records. This ensures that random numbers of a specific sequence are used every time, and hence, the same records (i.e. records having the same sequence number) are picked every time and the model is trained in the same way. This is extremely critical for the reproducibility of results, i.e. every time, the same machine learning program generates the same set of results. The code is as follows:"
"Training the Model‌Once the model preparatory steps such as data holdout, etc. are completed, the actual training starts using the following code or similar codes."
"standard error, multiple R-squared, etc., both for simple and multiple linear regression."
"As in the above case, we can use the dummy.code function of the psych package to encode categorical variables. Following is a small code for the same."
"‌For performing principal component analysis (PCA), we can use the prcomp function of the stats package. Following is the code using the iris data set. PCA should be applied to the predictors. The class variable can be used to visualize the principal components."
"For performing singular value decomposition, we can use the svd function of the stats package. Following is the code using the iris data set.For performing linear discriminant analysis, we can use the lda function of the MASS package. Following is the code using the UCI data set btissue."
"feature selection functionalities, the FSelector package of R can be used.Below is a programme to perform feature subset selection before applying the same for training a model."
The e1O71 package is an important R package which contains many statistical functions along with some critical classification algorithms such as Naïve Bayes and support vector machine (SVM). It is created by David Meyer and team and maintained by David Meyer.
"‌To implement this classifier, the naiveBayes function of the e1O71 package has been used. The full code for the implementation is given below.To implement this classifier, the knn function of the class package has been used. The full code for the implementation is given below."
"‌To implement this classifier implementation, the train function of the caret package can be used with a parameter method = “rpart” to indicate that the train function will use the decision tree classifier. Optionally, the rpart function of the rpart package can also be used. The full code for the implementation is given below."
"To implement this classifier, the train function of the caret package can be used with the parameter method = “rf” to indicate that the train function will use the decision tree classifier. Optionally, the randomForest function of theTo implement this classifier, the svm function of the e1O71 package has been used. The full code for the implementation is given below."
"As discussed in Chapter 9, two main algorithms used for regression are simple linear regression and multiple linear regression. Implementation of both the algorithms in R code is shown below."
"To implement the k-means algorithm, the k-means function of the cluster package has been used. Also, because silhouette width is a more generic performance measure, it has been used here. The complete R code for the implementation is given below."
"‌Before starting to do machine learning programming in Python, please install Python Anaconda distribution from https://anaconda.org/anaconda/python. To write any python script, open any text editor and write your python script and save with the extension .py. You may also use the Spyder (Scientific PYthon Development EnviRonment), which is a powerful and interactive development environment for the Python language with advanced editing, interactive testing, debugging, and introspection features. In this chapter, all the examples have been created working in Spyder."
"In general, Annaconda Python by default contains all the basic required packages for starting programming in machine learning. But in case, any extra package is needed, ‘pip’ is the recommended installer."
"After installing, libraries need to be loaded by invoking the command:Try out each of the following commands."
"Though Python has all the basic data types, Python variables do not need explicit declaration to reserve memory space. The declaration happens automatically when you assign a value to a variable. The equal sign (=) is used to assign values to variables."
‌The operand to the left of the = operator is the name of the variable and the operand to the right of the = operator is the value stored in the variable. For example –
‌Running the function (after compiling the script using source (“script_name”):
"Often, we get data sets with duplicate rows, which is nothing but noise. Therefore, before training the model, we need to"
make sure we get rid of such inconsistencies in the data set. We can remove duplicate rows using code:We frequently find missing values in our data set. We can drop the rows/columns with missing values using the code below.
"Now that we are kind of familiar with the basic Python commands, we have the ability to start machine learning programming in Python. But before starting to do actual modelling work, we have to first understand the data using the concepts highlighted in Chapter 2. Also, there might be some issues in the data which you will unveil during data exploration. You have to remediate that too."
By using certain statistical functions to understand the central tendency and spread of the data
By visually observing the data in the form of plots or graphs
"Let’s start with the first approach of understanding the data through the statistical techniques. As we have seen in Chapter 2, for any data set, it is critical to understand the central tendency and spread of the data. We have also seen the standard statistical measures are as follows:"
"Position of the different data values – quartiles, inter-quartile range (IQR)"
"Also, there is a function describe which generates the summary statistics of a data set. It gives the first basic understanding of the data set which can trigger thought about the data set and the anomalies that may be present. So, let’s start off exploring a data set Auto MPG data set from the University of California, Irvine (UCI) machine learning repository. We will run the info and describe commands for the Auto MPG data set."
"‌Looking closely at the output of the describe command, there are seven measures listed for the attributes (well, most of them). These are"
"These gives a pretty good understanding of the data set attributes. Now, note that one attribute – car.name, has a data type object whereas the remaining attributes have either integer or float data type. Why is that so? This is because this attribute is a categorical attribute while the other ones are numeric."
"Let’s first try to figure out if there is any missing value for any of the attributes. Let’s use a small piece of Python code find out if there is any missing/unwanted value for an attribute in the data. If there is, return the rows in which the attributehas missing/unwanted values. Checking for all the attributes, we find that the attribute ‘horsepower’ has missing values."
‌There are six rows in the data set which have missing values for the attribute ‘horsepower’. We will have to remediate these rows before we proceed with the modelling activities. We will do that shortly.
"The easiest and most effective way to detect outliers is from the box plot of the attributes. In the box plot, outliers are very clearly highlighted. When we explore the attributes using box plots in a short while, we will have a clear view of this."
We can get the other statistical measure of the attributes using the following Python commands from the Numpy library:
"In order to start using the library functions of matplotlib, we need to include the library as follows:Let’s now understand the different graphs that are used for data exploration and how to generate them using Python code."
"‌We will use the “iris” data set, a very popular data set in the machine learning world. This data set consists of three different types of iris flower: Setosa, Versicolour, and Virginica, the columns of the data set being - Sepal Length, Sepal Width, Petal Length, and Petal Width. For using this data set, we will first have to import the Python library datasets using the code below.A box plot for each of the four predictors is generated as shown in Figure B.2."
"As we can see, Figure B.2 gives the box plot for the entire iris data set i.e. for all the features in the iris data set, there is a component or box plot in the overall plot. However, if we want to review individual features separately, we can do that too using the following Python command.The output of the command i.e. the box plot of an individual feature, sepal width, of the iris data set is shown in Figure B.3."
"The output of the command, i.e. the histogram of an individual feature, Sepal Length, of the iris data set is shown in Figure B.4."
"‌The primary data pre-processing activities are remediating data issues related to outliers and missing values. Also, feature subset selection is quite a critical area of data pre-processing. Let’s understand how to write programs for achieving these purposes."
"Imputing the value (i.e. outliers/missing value) with a standard statistical measure, e.g. mean or median or mode for that attributeEstimate the value (i.e. outlier / missing value) based on value of the attribute in similar records and replace with the estimated value."
"There is an alternate option too. As we know, outliers have abnormally high or low value from other elements. So the other option is to use threshold values to detect outliers so that they can be removed. A sample code is given below."
"Then join back the imputed rows and the remaining part of the data set.In a similar way, outlier values can be imputed. Only difference will be in identification of the relevant rows."
"The code for identification of outliers values will remain the same. For capping, generally a value 1.5 times the (inter-"
"quartile range) IQR is to be used for imputation, code will be as follows:"
"Scikit-learn (“SciKit” = SciPy Toolkit) is a machine learning library for the Python programming language. It contains various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means, etc. Built on NumPy, SciPy, and matplotlib, it is designed to interoperate smoothly between the libraries."
"‌The first step before starting to model, in case of supervised learning, is to load the input data, hold out a portion of the input data as test data and have the remaining portion as training data for building the model. Below is the standardway to do it. Scikit-learn provides a function to split input data set into training and test data sets."
"When we do data holdout, i.e. splitting of the input data into training and test data sets, the records selected for each set are picked randomly. So, it is obvious that executing the same code may result in different training data set. So the model trained will also be somewhat different. In Python Scikit function train_test_split, the parameter random_state sets the starting point of the random number generator used internally to pick the records. This ensures that random numbers of a specific sequence is used every time and hence the same records (i.e. records having same sequence number) are picked every time and the model is trained in the same way. This is extremely critical for the reproducibility of results i.e. every time, the same machine learning program generates the same set of results."
"Let’s do k-fold cross-validation with 10 folds. For creating the cross-validation, there are two options. KFold function of either sklearn.cross_validation or sklearn.model_ selection can be used as follows:"
"As discussed in Capter 3, bootstrap resampling is used to generate samples of any given size from the training data. It uses the concept of simple random sampling with repetition. To generate bootstrap sample in Python, resample function of sklearn.utils library can be used. Below is a sample code"
"Once the model preparatory steps, like data holdout, etc. are over, the actual training happens. The sklearn framework in Python provides most of the models which are generally used in machine learning. Below is a sample code."
"Evaluating model performanceAs we have already seen in Chapter 3, the primary measure of performance of a classification model is its accuracy."
"Accuracy of a model is calculated based on correct classifications made by the model compared to total number of classifications. In Python, sklearn.metrics provides accuracy_score functionality to evaluate the accuracy of a classification model. Below is the code for evaluating the accuracy of a classifier model."
"‌The confusion_matrix functionality of sklearn.metrics helps in generation the confusion matrix, discussed in detail in Chapter 3. Below is the code for finding the confusion matrix of a classifier model. Also, in Figure B.6, is a quick analysis of the output confusion matrix as obtained from the Python code."
"We have seen in Chapter 3, R-squared is an effective measure of performance of a regression model. In Python, sklearn.metrics provides mean_squared_error and r2_score functionality to evaluate the accuracy of a regression model. Below is a sample code."
"As we have seen in Chapter 3, there are two popular measures of cluster quality – purity and silhouette width. Purity can be calculated only when class label is known for the data set subjected to clustering. On the other hand, silhouette width can be calculated for any data set."
"We will use a Lower Back Pain Symptoms data set released by Kaggle (https://www.kaggle.com/sammy123/lower-back-pain- symptoms-dataset). The data set consists of 310 observations, 13 attributes (12 numeric predictors, 1 binary class attribute)."
"For calculating Silhouette Width, silhouette_score function of sklearn. metrics library can be used for calculating Silhouette width. Below is the code for calculating Silhouette Width. A full implementation can be found in a later section A.5.3."
"Feature constructionFor doing feature construction, we can use pandas library of Python. Following is a small code for implementing feature construction."
‌Dummy coding categorical (nominal) variables:get_dummies function of pandas library can be used to dummy code categorical variables. Following is a sample code for the same.
"For doing principal component analysis or in other words – to get the principal components of a set of features in a data set, we can use PCA function of sklearn.decomposition library. However, features should be standardized (i.e. transform to unit scale with mean = 0 and variance = 1), for which StandardScaler function of sklearn. preprocessing library can be used. Following is the code using the iris data set. PCA should be applied on the predictors. The target / class variable can be used to visualize the principal components (Figure B.7)."
We have seen in Chapter 4 that singular value decomposition of a matrix A (m X n) is a factorization of the form:
"A = UΣV = UsVT (representing Σ or Sigma by “s”)For implementing singular value decomposition in Python, we can use svd function of scipy.linalg library. Following is the code using the iris data set."
"For implementing singular value decomposition in Python, we can use LinearDiscriminantAnalysis function of sklearn.discriminant_analysis library. Following is the code using the UCI data set btissue."
"Feature subset selection is a topic of intense research. There are many approaches to select a subset of features which can improve model performance. It is not possible to cover all such approaches as a part of this text. However, sklearn.feature_selection module can be used for applying basic feature selection. There are multiple options of feature selection offered by the module, one of which is presented below:"
"Also, we have used benchmark data sets from UCI repository (will also keep it available online).For Naive Bayes classifier implementation, GaussianNB library of sklearn.naive_ bayes has been used. The full code for the implementation is given below."
"0.84375In the code below, in addition to reviewing the model accuracy, we will also review the confusion matrix."
"[ 4, 3]], dtype=int64)For implementing Random Forest classifier, svm library of sklearn framework has been used. The full code for the implementation is given below."
"For implementing Random Forest classifier, linear_model library of sklearn framework has been used. The full code for the implementation is given below."
"For the implementation of kmeans algorithm, KMeans function of sklearn.cluster library has been used. Also, since silhouette width is a more generic performance measure, it has been used here. silhouette_score function of sklearn. metrics library has been used for calculating Silhouette width. The full code for the implementation is given below (Figure B.8)."
"For implementing neural network in Python, neurolab is a great option. It is a simple and powerful neural network library for Python. You can install neurolab by using the command below from the command prompt:Please refer to the Figure B.9 for more details."
"Let’s first implement a single layer feed forward neural network. We can use the same set of data as provided as an example in Chapter 10, Section 10.6.2. The data is as below and output shown in Figure B.10."
"‌Next let’s implement a multi layer feed forward neural network. We can have multiple hidden layers in this case. In this case, we will use multi layer neural network as a regressor"
"i.e. to predict a continuous variable. A set of 75 values of predictor variable is first generated within a specified range. The corresponding target values are generated using the equation y = 4x2 + 7.6 and normalized. Using these set of points, a multi layer neural network is trained. Then using the trained model, predictions are generated and compared with the actual data points generated from the regression function.Output is presented in Figures B.11 and B.12."
‌MACHINE LEARNING L AB USING PY THON – PROPOSED DAY- WISE SCHEDULE
"In the previous two appendices, we have discussed how R and Python can be used to create the machine learning algorithms. We also talked about different sets of libraries available for easy deployment of machine learning. With that perspective, let’s now see how machine learning is used in real life to solve a business problem, which otherwise would have taken significant manual effort."
"In a telecom service center, the operators are responsible for providing the technical resolution to the problems that the customers face. Recently, due to huge surge in telecom user base, the service center of Newtel Infocomm is facing a huge challenge in dealing with the increase in service request volume. Newtel has two options – either it can increase the number of operators, thus increasing the operating cost substantially. Alternatively, it can explore if technology can help to solve the problem, without much additional cost, and thus help it remain profitable. The service center of Newtel engaged Global Tech (GT), a company known for providing state-of-the-art machine learning solutions to solve customerproblems. After careful review of the problem and the underlying root cause, GT come up with a Machine Learning solution, briefly highlighted below."
"GT observed that there are many service requests coming to Newtel service centers, which are repetitive in nature in terms of the type of problem. This was evident from the text of the service requests. Also, in most of the cases, there are some standard steps that the customer needs to follow to get the issue resolved. So, GT experts inferred, that buckets of service requests can be created through machine learning algorithm based on the past service requests. For each bucket, a set of corresponding standard resolutions can be associated."
"Whenever a new service request comes to Newtel service center, another machine learning algorithm should identify the bucket to which the new request should be tagged to, pull out the corresponding standard resolution, and send it to the customer. All these will happen without any human intervention. So, additional call volumes can be handled without any additional personnel in place. This will significantly reduce the additional manpower requirement leading to an impact on operating cost and profitability."
"In the solution provided by GT, service requests in the repository were clustered using a combination of text analytics of select descriptive fields, time taken to resolve each request, and a user-selected categorical variable. The specific text analytics technique chosen was Word2Vector and the clustering technique was DBSCAN. For text classification to classify the new service request to one of the buckets created, Cosine Similarity algorithm was used. Let’s review the technical solution in some more details."
"Load required libraries of Python. These sets of libraries are needed to load data, perform computations and display output. Some of the important libraries are"
nltk.data and nltk.corpus—Natural language processing library
import genism—for text analytics and clustering we will be converting word to vector. Word2vec function implemented by genism is useful here
from sklearn.metrics.pairwise import cosine_similarity—To compute service request description similarity
"Based on the source of data, like spreadsheet or csv dump of service requests or a direct access to service request database, load the data into the program in form of array of strings. The csv file containing the data set is available as online material for this book. The structure of the file is given below:"
"Identify the information from the data set that can help in making the data categorization or the clustering easy. In our example, we will use the service center’s assignment group or department as the driver for clustering. Below is some example of the assignment groups that can categorize the service requests."
Calculate the average feature vector for each one and return a 2D numpy array.This array set is now your training data for running the clustering.
DBSCAN algorithm is used for this clustering which will take the density-based clustering approach. The position of vectors created above are checked and the high-density areas are taken as a new cluster whereas clusters are separated by low-density areas.
The cluster that is generated as the result of DBSCAN is shown in Figure C.1. The estimated number of clusters: 14 and Silhouette Coefficient: 0.367
Create the vector from the description text of the SR using word2vec function
Calculate the similarity score for the vector using the cosine_similarity function
"Find the cluster the service request is assigned to, based on the max similarity score, averaged across all service requests in the cluster"
"If no cluster is found matching this SR vector, then this SR is unlike any in the training repository and is not assigned to any cluster"
"The service request processing of Newtel Infocomm has been automated now. GT has used machine learning technique to achieve the automation. As a part of the solution, the unlabeled service requests have been grouped into logical clusters using"
"the DBSCAN algorithm. Also, whenever any new service request arrives, the cosine similarity algorithm finds out the relevant bucket for it so that some predefined, standard resolution can be sent automatically to the customer. There can be more such ways to solve this same business problem, as well as many scenarios which can be addressed through this same approach. So, look out for real-life business problems and think about automating the resolutions thorough intelligent use of machine learning."
"PART A (Multiple Choice Type Questions)computer program is said to learn from with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with it."
This type of learning to be used when there is no idea about the class or label of a particular data
"None of the aboveFor understanding relationship between two variables,       can be used."
Which of the following is a performance measure for regression?
"In a certain basketball club, there are 4% of male players who are over 6 feet tall and 1% of female players who are over 6 feet tall. The ratio of male to female players in the total player population is male:female = 2:3. A player is selected at random from among all those over 6 feet tall. What is the probability that the player is a female?"
The probability that a particular hypothesis holds for a data set based on the Prior is called
Predicting whether a tumour is malignant or benign is an example of
"Categorical attribute.are the data points (representing classes), the important component in a data set, which are near the identified set of lines (hyper plane)."
Which of the following is not an inductive bias in a decision tree?
Trees that place nodes near the root with high information gain are preferred
"When you find many noises in data, which of the following options would you consider in k-NN?"
Part B (Short Answer Type Questions)1. Define the concept of consistent learners.
2. Explain how Naïve Bayes classifier is used for Online Sentiment Analysis.?What is supervised learning? Give any two examples.
similarity.Explain the competitive network architecture of ANN.
"Part C (Long Answer Type Questions)1. Compare supervised, unsupervised, and reinforcement learning."
"2. An antibiotic resistance test (random variable T) has 1% false positives (i.e. 1% of those not resistance to an antibiotic show positive in the test), and 5% false negatives (i.e. 5% of those actually resistant to an antibiotic test negative). Let’s assume that 2% of those tested are resistant to antibiotics. Determine the probability that somebody who tests positive is actually resistant (random variable D)."
1. What are the different techniques for data pre-processing? Explain the filter approach of feature selection. How is it different from the wrapper approach?
"2. Use a simple perceptron with weights w0, w1, and w2 as –1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, –3);(–8, –3); (–3, 0)."
"1. Explain, in details, the different components of a box plot? When will the lower whisker be longer than the upper whisker? How can outliers be detected using box plot?2. During a research work, you found seven observations as described with the data points below. You want to create three clusters from these observations using K-means algorithm."
"After first iteration, the clusters C1, C2, C3 has following observations:"
If you want to run a second iteration then what will be the cluster centroids?
1. What is over-fitting? When does it happen? How can we avoid over-fitting?
"In an exam, there were 20 multiple-choice questions. Each question had 44 possible options. A student knew the answer to 10 questions, but the other 10 questions were unknown for him and he chose answers randomly. If the score of the student X is equal to the total number of correct answers, then find out the PMF of X. What is P(X > 15)?"
"There are 100 men on a ship. If Xi is the weight of the ith man on the ship and Xi’s are independent and identically distributed and also E(Xi) = μ = 170 and σXi= σ = 30. Find the probability that the total weight of the men on the ship exceeds 18,000."
Which of the following is not a type of Ensemble learning algorithms?
Elastic NetA neuron is able to       information in the form of chemical and electrical signals.
"All of the aboveIn SVM,       functions take low-dimensional input space and transform it to a higher dimensional space."
Hyper planeFeature       involves transforming a given set of input features to generate a new set of more powerful features.
"Out of 200 emails, a classification model correctly predicted 150 spam emails and 30 ham emails. What is the accuracy of the model?"
4. 60%Conversion of a text corpus to a numerical representation is done using process.
Reinforcement learning algorithmSingle-layer feed forward network consists of layers.
Manyrefers to the transformations applied to the identified data before feeding the same into the algorithm.
"Which of the following will be Euclidean distance between the two data points A (4, 3) and B (2, 3)?"
"Explain, in brief, how the Market Basket Analysis uses the concepts of association analysis.What is feature transformation? Explain the process of transforming numeric features to categorical features."
Part C (Long Answer Type Questions)1. Explain the process of feature engineering in context of text categorization problem.
"2. For preparation of the exam, a student knows that one question is to be solved in the exam, which is either of the types A, B, or C. The probabilities of A, B, or C appearing in the exam are 30%, 20%, and 50%, respectively. During the preparation, the student solved 9 of 10 problems of type A, 2 of 10 problems of type B, and 6 of 10 problems of type C. Given that the student solved the problem, what is the probability that it was of type A?"
1. What are the two major data issues in machine learning? How can these issues be remediated?
"2. Following is the training data for a group of athletes. Based on this data, use k-NN algorithm and classify Sayan (Weight = 56 kg., Speed = 10 kmph) as a Good, Average, or Poor sprinter."
"You are given a set of one-dimensional data points: {5, 10, 15,"
"20, 25, 30, 35}. Assume that k= 2 and first set of random centroidis selected as {15, 32} and then it is refined with {12, 30}."
Create two clusters with each set of centroid mentioned above following the K-means approach
1. Why a multilayer neural network is required? What are the steps in backpropagation algorithm?
"2. Compare the Jaccard index and similarity matching coefficient of two features having values (1, 1, 1, 1, 0, 0, 1, 1) and (1, 0, 1, 1, 0,1, 0, 1)."
PART A (Multiple Choice Type Questions)Ordinal data can be naturally       .
"Which of the following will be Manhattan Distance between the two data point A (8,3) and B (4,3)?"
"In this method, the active learning algorithm first tries to label the points that would mostly reduce the model’s generalization problem"
"Two boxes containing chocolates are placed on a table. The boxes are labelled B1 and B2. Box B1 contains 6 Cadbury chocolates and 5 Amul chocolates. Box B2 contains 3 Cadbury chocolates and 8 Nestle chocolates. The boxes are arranged so that the probability of selecting box B1 is 1⁄3 and the probability of selecting box B2 is 2⁄3. Sneha is blindfolded and asked to select a chocolate. She will win Rs. 10,000 if she selects a Cadbury chocolate. If she wins Rs 10,000, what is the probability that she selected Cadbury from the first box?"
What sizes of training data sets are not best suited for SVM?
"Training data set size does not matter.In       approach, identification of best feature subset is done using the induction algorithm as a black box."
Which of the following clustering algorithm is most sensitive to outliers?
2. Mention about some of the applications of Representational learning.What is reinforcement learning? Give any 2 examples.
"While predicting win-loss of teams in World Cup Football using a classification model, following are the data recorded:"
"Calculate the accuracy, sensitivity, precision and F-measure of the model.Explain the single-layer feed forward architecture of ANN."
Part C (Long Answer Type Questions)1. What is a target function? Express target function in context of a real-life example. Explain bias-variance trade-off in context of model fitting.
"2. In an airport security checking system the passengers are checked to find out any intruder. Let I with i ∈ {0, 1} be the random variable which indicates whether somebody is an intruder (i = 1) or not (i = 0) and A with a ∈ {0, 1} be the variable indicating alarm. An alarm will be raised if an intruder is identified with probability P (A = 1| I = 1) = 0.98, a non-intruder with probability P (A = 1| I = 0) = 0.001 which implies the error factor. In the population of passengers, the probability of someone is intruder is, P (I = 1) = 0.00001. What is the probability that an alarm is raised when a person actually is an intruder?"
1. Explain the learning process of an ANN. What are the limitations of backpropagation algorithm?
"2. A CCTV is installed in a bank to monitor the incoming customers and take a photograph. Though there is continuous flow of customers we create bins of timeframe of 5 minutes each. In each time frame of 5 min there may be a customer moving into the bank with probability 5%, or there is no customer (again, for simplicity we assume that either there is 1 customer or none, not the case of multiple customers). If there is a customer, it will be detected by the CCTV with a probability of 99%. If there is no customer, the camera will take false photograph by detecting other thing’s movement with a probability of 10%. How many customers enter the bank on average per day (10 hours)? How many false photographs (there is a photograph taken even though there is no customer) and how many missed photographs (there is no photograph even though there is a customer) are there on average per day.1. Can the performance of a learning model be improved? Elaborate your answer."
Explain some strengths and weaknesses of Decision Tree classifier.What are the broad three categories of clustering techniques? Explain the characteristics of each briefly.
"ADALINE network model. See adaptive linear neural element network model adaptive boosting, 86, 311"
"ANN. See artificial neural network (ANN) anomaly checking, clustering, 244"
"Rosenblatt’s perceptron. See Rosenblatt’s perceptron single-layer feed forward network, 287–288 structure of, 275"
"association rule, 262–264. See also association analysis Apriori algorithm for, 264–265"
"AUC value. See area under curve value Auto MPG data set, 36, 326, 366"
"Bayesian concept learning. See also Bayes’ theorem; Bayesian Belief network brute-force Bayesian algorithm, 154–156"
"Naïve Bayes classifier. See Naïve Bayes classifiers optimal classification, 156–157"
"biological neural network, 273, 274. See also artificial neural network (ANN) biological neuron, 273–274"
"BLUE. See best linear unbiased estimator (BLUE) boosting, 86, 310–311"
"bootstrap sampling, 70, 71, 335, 375–376 box and whisker plot. See box plots"
"cdf. See cumulative distribution function (cdf) central limit theorem, 132, 138"
"classification algorithms, 180 decision tree. See decision tree k-nearest neighbour, 181–186 random forest model, 199–201"
"partitioning-based. See partitioning-based clustering text data mining, 243"
"CNS. See central nervous system (CNS) competitive network, 289"
"contingency table. See two-way cross-tabulations continuous numeric features, 164–165 continuous random variables, 125–126"
CRAN. See Comprehensive R Archive Network (CRAN) cross-tab. See two-way cross-tabulations
"feature subset selection. See feature subset selection feature extraction, 93, 99"
"feature selection. See feature subset selection feature subset selection, 56–57, 93, 102, 344–345"
feature construction. See feature construction feature extraction. See feature extraction
"GPU. See graphics processing unit (GPU) gradient boosting machines (GBM), 311 gradient descent, 292"
ICA. See independent component analysis (ICA) ICU. See intensive care unit (ICU)
"independence, 166–170. See also conditional independence independent component analysis (ICA), 303–304 independent variables, 216, 227–230"
"LMS. See least mean square (LMS) logistic regression, 233–236"
"LOOCV. See leave-one-out cross-validation (LOOCV) loops, 320–321"
"MAP hypothesis. See maximum a posteriori (MAP) hypothesis margin, 203"
MMH. See maximum margin hyperplane (MMH); minimum marginal hyperplane (MMH)
"neural network, 302, 392–395. See also artificial neural network (ANN) multi-layer feedforward, 350, 352"
"PAM algorithm. See partitioning around medoids (PAM) algorithm pandas library, 361, 378"
PCA. See principal component analysis (PCA) pdf. See probability density function (pdf)
"principal component analysis (PCA), 100–101, 303, 341–342, 381–383"
"data exploration. See data exploration data handling commands, 361–365 data holdout, 374"
"supervised learning. See supervised learning variables, 357–358"
"qualitative data. See categorical data quantitative data. See numerical data query by committee, 306"
"mathematical operations on data types, 322–323 model training. See model training"
"ReLU function. See rectified linear unit (ReLU) function remove outliers, 54"
"active learning. See Active learning association rule learning algorithm, 308–309 autoencoders, 304–305"
"RFFN. See radial basis function network (RFFN) ridge regression, 231"
"ROC curve. See receiver operating characteristic (ROC) curve root node, 187"
"SAS. See Statistical Analysis System (SAS) scatter plot, 49–51, 331–332, 371"
"SPSS. See Statistical Package for the Social Sciences (SPSS) Spyder (Scientific PYthon Development EnviRonment), 355 squares of the errors"
"SRSWR. See Simple Random Sampling with Replacement (SRSWR) state space, 124"
"supervised learning, 11, 19, 29, 176. See also unsupervised learning"
"classification, 13–14, 75–81, 336–337, 345 classification algorithms. See classification algorithms"
"classification learning steps. See classification learning steps classification model, 177–178, 376–377, 386–389"
"SVD. See singular value decomposition (SVD) svd function, 342"
"TID list. See Transaction IDs (TID list) total probability rule, 120"
"unsupervised learning, 15–17, 19, 82–84, 105, 241, 349–350. See also supervised learning"
"VIF. See variance inflation factor (VIF) Voronoi diagram, 251"
"This book is sold subject to the condition that it shall not, by way of trade or otherwise, be lent, resold, hired out, or otherwise circulated without the publisher’s prior written consent in any form of binding or cover other than that in which it is published and without a similar condition including this condition being imposed on the subsequent purchaser and without limiting the rights under copyright reserved above, no part of this publication may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording or otherwise), without the prior written permission of both the copyright owner and the publisher of this book."
"Head Office: 15th Floor, Tower-B, World Trade Tower, Plot No. 1, Block-C, Sector 16, Noida 201 301, Uttar Pradesh, India.Registered office: 4th Floor, Software Block, Elnet Software City, TS-140, Block 2 & 9, Rajiv Gandhi Salai, Taramani, Chennai 600 113, Tamil Nadu, India."
