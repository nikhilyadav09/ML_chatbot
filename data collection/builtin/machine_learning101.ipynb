{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/01_introduction.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/02_learning_models.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/03_bias_variance.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/04_covariance_correlation.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/05_model_metrics.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/06_underfitting_overfitting.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/07_model_performance.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/08_gradient_decent.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/09_regression.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/10_simple_linear_regression.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/11_example_simple_linear_regression.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/12_multiple_linear_regression.html\n",
      "Scraping: https://machinelearning101.readthedocs.io/en/latest/_pages/13_example_multiple_linear_regression.html\n",
      "Content saved to ml101_content.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "class ML101Scraper:\n",
    "    def __init__(self, base_url='https://machinelearning101.readthedocs.io/en/latest/_pages/'):\n",
    "        self.base_url = base_url\n",
    "        self.scraped_content = {}\n",
    "\n",
    "    def extract_text_by_topic(self, soup):\n",
    "        \"\"\"Extract chapter and subtopics with their content.\"\"\"\n",
    "        content = {}\n",
    "        chapter_title = soup.find('h1').get_text(strip=True)  # Assuming the main chapter title is in <h1>\n",
    "        \n",
    "        content[chapter_title] = {}\n",
    "        \n",
    "        # Find all subheadings and paragraphs\n",
    "        subtopics = soup.find_all(['h2', 'h3'])\n",
    "        for subtopic in subtopics:\n",
    "            subtopic_title = subtopic.get_text(strip=True)\n",
    "            subtopic_content = []\n",
    "            \n",
    "            current = subtopic.find_next_sibling()\n",
    "            while current and current.name not in ['h1', 'h2', 'h3']:\n",
    "                if current.name in ['p', 'ul', 'ol']:\n",
    "                    subtopic_content.append(current.get_text(strip=True))\n",
    "                current = current.find_next_sibling()\n",
    "            \n",
    "            content[chapter_title][subtopic_title] = ' '.join(subtopic_content)\n",
    "        \n",
    "        return content\n",
    "\n",
    "    def scrape_page(self, page_url):\n",
    "        \"\"\"Scrape content from a single page and find the next button.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(page_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract page content\n",
    "            page_content = self.extract_text_by_topic(soup)\n",
    "            \n",
    "            # Find the link to the next page\n",
    "            next_button = soup.find('a', {'accesskey': 'n'})  # 'n' access key for next button\n",
    "            next_page_url = next_button['href'] if next_button else None\n",
    "            \n",
    "            return page_content, next_page_url\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error scraping {page_url}: {e}\")\n",
    "            return {}, None\n",
    "\n",
    "    def scrape_all_pages(self, start_page='01_introduction.html'):\n",
    "        \"\"\"Scrape all pages starting from the initial page.\"\"\"\n",
    "        current_url = self.base_url + start_page\n",
    "        \n",
    "        while current_url:\n",
    "            print(f\"Scraping: {current_url}\")\n",
    "            page_content, next_relative_url = self.scrape_page(current_url)\n",
    "            self.scraped_content.update(page_content)\n",
    "            \n",
    "            if next_relative_url:\n",
    "                current_url = self.base_url + next_relative_url\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def save_to_json(self, filename='ml101_content.json'):\n",
    "        \"\"\"Save the scraped content to a JSON file.\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.scraped_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Content saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    scraper = ML101Scraper()\n",
    "    scraper.scrape_all_pages()\n",
    "    scraper.save_to_json()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
