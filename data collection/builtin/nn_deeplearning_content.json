{
    "CHAPTER 1": {
        "Perceptrons": [
            "What is a neural network?  To get started, I'll explain a type of\rartificial neuron called aperceptron.\rPerceptrons weredevelopedin the 1950s and 1960s by the scientistFrank\r  Rosenblatt, inspired by earlierworkbyWarren\r  McCullochandWalter\r  Pitts.  Today, it's more common to use other\rmodels of artificial neurons - in this book, and in much modern work\ron neural networks, the main neuron model used is one called thesigmoid neuron.  We'll get to sigmoid neurons shortly.  But to\runderstand why sigmoid neurons are defined the way they are, it's\rworth taking the time to first understand perceptrons.",
            "So how do perceptrons work?  A perceptron takes several binary inputs,\r$x_1, x_2, \\ldots$, and produces a single binary output:In the example shown the perceptron has three inputs, $x_1, x_2, x_3$.\rIn general it could have more or fewer inputs.  Rosenblatt proposed a\rsimple rule to compute the output.  He introducedweights, $w_1,w_2,\\ldots$, real numbers\rexpressing the importance of the respective inputs to the output.  The\rneuron's output, $0$ or $1$, is determined by whether the weighted sum\r$\\sum_j w_j x_j$ is less than or greater than somethreshold\r  value.  Just like the weights, the\rthreshold is a real number which is a parameter of the neuron.  To put\rit in more precise algebraic terms:\\begin{eqnarray}\r  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\r      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\r      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\r      \\end{array} \\right.\r\\tag{1}\\end{eqnarray}\rThat's all there is to how a perceptron works!",
            "That's the basic mathematical model.  A way you can think about the\rperceptron is that it's a device that makes decisions by weighing up\revidence.  Let me give an example.  It's not a very realistic example,\rbut it's easy to understand, and we'll soon get to more realistic\rexamples.  Suppose the weekend is coming up, and you've heard that\rthere's going to be a cheese festival in your city.  You like cheese,\rand are trying to decide whether or not to go to the festival.  You\rmight make your decision by weighing up three factors:Is the weather good?Does your boyfriend or girlfriend want to accompany you?Is the festival near public transit? (You don't own a car).We can represent these three factors by corresponding binary variables\r$x_1, x_2$, and $x_3$.  For instance, we'd have $x_1 = 1$ if the\rweather is good, and $x_1 = 0$ if the weather is bad.  Similarly, $x_2\r= 1$ if your boyfriend or girlfriend wants to go, and $x_2 = 0$ if\rnot.  And similarly again for $x_3$ and public transit.",
            "Now, suppose you absolutely adore cheese, so much so that you're happy\rto go to the festival even if your boyfriend or girlfriend is\runinterested and the festival is hard to get to.  But perhaps you\rreally loathe bad weather, and there's no way you'd go to the festival\rif the weather is bad.  You can use perceptrons to model this kind of\rdecision-making.  One way to do this is to choose a weight $w_1 = 6$\rfor the weather, and $w_2 = 2$ and $w_3 = 2$ for the other conditions.\rThe larger value of $w_1$ indicates that the weather matters a lot to\ryou, much more than whether your boyfriend or girlfriend joins you, or\rthe nearness of public transit.  Finally, suppose you choose a\rthreshold of $5$ for the perceptron.  With these choices, the\rperceptron implements the desired decision-making model, outputting\r$1$ whenever the weather is good, and $0$ whenever the weather is bad.\rIt makes no difference to the output whether your boyfriend or\rgirlfriend wants to go, or whether public transit is nearby.",
            "By varying the weights and the threshold, we can get different models\rof decision-making.  For example, suppose we instead chose a threshold\rof $3$.  Then the perceptron would decide that you should go to the\rfestival whenever the weather was goodorwhen both the\rfestival was near public transitandyour boyfriend or\rgirlfriend was willing to join you.  In other words, it'd be a\rdifferent model of decision-making.  Dropping the threshold means\ryou're more willing to go to the festival.",
            "Obviously, the perceptron isn't a complete model of human\rdecision-making!  But what the example illustrates is how a perceptron\rcan weigh up different kinds of evidence in order to make decisions.\rAnd it should seem plausible that a complex network of perceptrons\rcould make quite subtle decisions:In this network, the first column of perceptrons - what we'll call\rthe firstlayerof perceptrons - is making three very simple\rdecisions, by weighing the input evidence.  What about the perceptrons\rin the second layer?  Each of those perceptrons is making a decision\rby weighing up the results from the first layer of decision-making.\rIn this way a perceptron in the second layer can make a decision at a\rmore complex and more abstract level than perceptrons in the first\rlayer.  And even more complex decisions can be made by the perceptron\rin the third layer.  In this way, a many-layer network of perceptrons\rcan engage in sophisticated decision making.",
            "Incidentally, when I defined perceptrons I said that a perceptron has\rjust a single output.  In the network above the perceptrons look like\rthey have multiple outputs.  In fact, they're still single output.\rThe multiple output arrows are merely a useful way of indicating that\rthe output from a perceptron is being used as the input to several\rother perceptrons.  It's less unwieldy than drawing a single output\rline which then splits.",
            "Let's simplify the way we describe perceptrons.  The condition $\\sum_j\rw_j x_j > \\mbox{threshold}$ is cumbersome, and we can make two\rnotational changes to simplify it.\r The first change is to write\r$\\sum_j w_j x_j$ as a dot product, $w \\cdot x \\equiv \\sum_j w_j x_j$,\rwhere $w$ and $x$ are vectors whose components are the weights and\rinputs, respectively.  The second change is to move the threshold to\rthe other side of the inequality, and to replace it by what's known as\rthe perceptron'sbias, $b \\equiv\r-\\mbox{threshold}$.  Using the bias instead of the threshold, the\rperceptron rule can be\rrewritten:\\begin{eqnarray}\r  \\mbox{output} = \\left\\{ \r    \\begin{array}{ll} \r      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\r      1 & \\mbox{if } w\\cdot x + b > 0\r    \\end{array}\r  \\right.\r\\tag{2}\\end{eqnarray}\rYou can think of the bias as a measure of how easy it is to get the\rperceptron to output a $1$.  Or to put it in more biological terms,\rthe bias is a measure of how easy it is to get the perceptron tofire.  For a perceptron with a really big bias, it's extremely\reasy for the perceptron to output a $1$.  But if the bias is very\rnegative, then it's difficult for the perceptron to output a $1$.\rObviously, introducing the bias is only a small change in how we\rdescribe perceptrons, but we'll see later that it leads to further\rnotational simplifications.  Because of this, in the remainder of the\rbook we won't use the threshold, we'll always use the bias.",
            "I've described perceptrons as a method for weighing evidence to make\rdecisions.  Another way perceptrons can be used is to compute the\relementary logical functions we usually think of as underlying\rcomputation, functions such asAND,OR, andNAND.  For example, suppose we have a perceptron with two\rinputs, each with weight $-2$, and an overall bias of $3$.  Here's our\rperceptron:Then we see that input $00$ produces output $1$, since\r$(-2)*0+(-2)*0+3 = 3$ is positive.  Here, I've introduced the $*$\rsymbol to make the multiplications explicit.  Similar calculations\rshow that the inputs $01$ and $10$ produce output $1$.  But the input\r$11$ produces output $0$, since $(-2)*1+(-2)*1+3 = -1$ is negative.\rAnd so our perceptron implements aNANDgate!",
            "",
            "TheNANDexample shows that we can use perceptrons to compute\rsimple logical functions.\r In fact, we can use\rnetworks of perceptrons to computeanylogical function at all.\rThe reason is that theNANDgate is universal for\rcomputation, that is, we can build any computation up out ofNANDgates.  For example, we can useNANDgates to\rbuild a circuit which adds two bits, $x_1$ and $x_2$.  This requires\rcomputing the bitwise sum, $x_1 \\oplus x_2$, as well as a carry bit\rwhich is set to $1$ when both $x_1$ and $x_2$ are $1$, i.e., the carry\rbit is just the bitwise product $x_1 x_2$:To get an equivalent network of perceptrons we replace all theNANDgates by perceptrons with two inputs, each with weight\r$-2$, and an overall bias of $3$.  Here's the resulting network.  Note\rthat I've moved the perceptron corresponding to the bottom rightNANDgate a little, just to make it easier to draw the arrows\ron the diagram:One notable aspect of this network of perceptrons is that the output\rfrom the leftmost perceptron is used twice as input to the bottommost\rperceptron.  When I defined the perceptron model I didn't say whether\rthis kind of double-output-to-the-same-place was allowed.  Actually,\rit doesn't much matter.  If we don't want to allow this kind of thing,\rthen it's possible to simply merge the two lines, into a single\rconnection with a weight of -4 instead of two connections with -2\rweights.  (If you don't find this obvious, you should stop and prove\rto yourself that this is equivalent.)  With that change, the network\rlooks as follows, with all unmarked weights equal to -2, all biases\requal to 3, and a single weight of -4, as marked:Up to now I've been drawing inputs like $x_1$ and $x_2$ as variables\rfloating to the left of the network of perceptrons.  In fact, it's\rconventional to draw an extra layer of perceptrons - theinput\r  layer- to encode the inputs:This notation for input perceptrons, in which we have an output, but\rno inputs,is a shorthand.  It doesn't actually mean a perceptron with no inputs.\rTo see this, suppose we did have a perceptron with no inputs.  Then\rthe weighted sum $\\sum_j w_j x_j$ would always be zero, and so the\rperceptron would output $1$ if $b > 0$, and $0$ if $b \\leq 0$.  That\ris, the perceptron would simply output a fixed value, not the desired\rvalue ($x_1$, in the example above). It's better to think of the\rinput perceptrons as not really being perceptrons at all, but rather\rspecial units which are simply defined to output the desired values,\r$x_1, x_2,\\ldots$.",
            "The adder example demonstrates how a network of perceptrons can be\rused to simulate a circuit containing manyNANDgates.  And\rbecauseNANDgates are universal for computation, it follows\rthat perceptrons are also universal for computation.",
            "The computational universality of perceptrons is simultaneously\rreassuring and disappointing.  It's reassuring because it tells us\rthat networks of perceptrons can be as powerful as any other computing\rdevice.  But it's also disappointing, because it makes it seem as\rthough perceptrons are merely a new type ofNANDgate.\rThat's hardly big news!",
            "However, the situation is better than this view suggests.  It turns\rout that we can deviselearning\r  algorithmswhich can\rautomatically tune the weights and biases of a network of artificial\rneurons.  This tuning happens in response to external stimuli, without\rdirect intervention by a programmer.  These learning algorithms enable\rus to use artificial neurons in a way which is radically different to\rconventional logic gates.  Instead of explicitly laying out a circuit\rofNANDand other gates, our neural networks can simply learn\rto solve problems, sometimes problems where it would be extremely\rdifficult to directly design a conventional circuit.",
            "Sigmoid neurons"
        ],
        "Sigmoid neurons": [
            "Learning algorithms sound terrific.  But how can we devise such\ralgorithms for a neural network?  Suppose we have a network of\rperceptrons that we'd like to use to learn to solve some problem.  For\rexample, the inputs to the network might be the raw pixel data from a\rscanned, handwritten image of a digit.  And we'd like the network to\rlearn weights and biases so that the output from the network correctly\rclassifies the digit.  To see how learning might work, suppose we make\ra small change in some weight (or bias) in the network.  What we'd\rlike is for this small change in weight to cause only a small\rcorresponding change in the output from the network.  As we'll see in\ra moment, this property will make learning possible.  Schematically,\rhere's what we want (obviously this network is too simple to do\rhandwriting recognition!):",
            "",
            "If it were true that a small change in a weight (or bias) causes only\ra small change in output, then we could use this fact to modify the\rweights and biases to get our network to behave more in the manner we\rwant.  For example, suppose the network was mistakenly classifying an\rimage as an \"8\" when it should be a \"9\".  We could figure out how\rto make a small change in the weights and biases so the network gets a\rlittle closer to classifying the image as a \"9\".  And then we'd\rrepeat this, changing the weights and biases over and over to produce\rbetter and better output.  The network would be learning.",
            "The problem is that this isn't what happens when our network contains\rperceptrons.  In fact, a small change in the weights or bias of any\rsingle perceptron in the network can sometimes cause the output of\rthat perceptron to completely flip, say from $0$ to $1$.  That flip\rmay then cause the behaviour of the rest of the network to completely\rchange in some very complicated way.  So while your \"9\" might now be\rclassified correctly, the behaviour of the network on all the other\rimages is likely to have completely changed in some hard-to-control\rway.  That makes it difficult to see how to gradually modify the\rweights and biases so that the network gets closer to the desired\rbehaviour.  Perhaps there's some clever way of getting around this\rproblem.  But it's not immediately obvious how we can get a network of\rperceptrons to learn.",
            "We can overcome this problem by introducing a new type of artificial\rneuron called asigmoidneuron.\rSigmoid neurons are similar to perceptrons, but modified so that small\rchanges in their weights and bias cause only a small change in their\routput.  That's the crucial fact which will allow a network of sigmoid\rneurons to learn.",
            "Okay, let me describe the sigmoid neuron.  We'll depict sigmoid\rneurons in the same way we depicted perceptrons:Just like a perceptron, the sigmoid neuron has inputs, $x_1, x_2,\r\\ldots$.  But instead of being just $0$ or $1$, these inputs can also\rtake on any valuesbetween$0$ and $1$.  So, for instance,\r$0.638\\ldots$ is a valid input for a sigmoid neuron. Also just like a\rperceptron, the sigmoid neuron has weights for each input, $w_1, w_2,\r\\ldots$, and an overall bias, $b$.  But the output is not $0$ or $1$.\rInstead, it's $\\sigma(w \\cdot x+b)$, where $\\sigma$ is called thesigmoid function**Incidentally, $\\sigma$ is sometimes\r  called thelogistic\r    function, and this\r  new class of neurons calledlogistic\r    neurons.  It's useful\r  to remember this terminology, since these terms are used by many\r  people working with neural nets.  However, we'll stick with the\r  sigmoid terminology., and is defined\rby:\\begin{eqnarray} \r  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\r\\tag{3}\\end{eqnarray}\rTo put it all a little more explicitly, the output of a sigmoid neuron\rwith inputs $x_1,x_2,\\ldots$, weights $w_1,w_2,\\ldots$, and bias $b$ is\\begin{eqnarray} \r  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\r\\tag{4}\\end{eqnarray}",
            "At first sight, sigmoid neurons appear very different to perceptrons.\rThe algebraic form of the sigmoid function may seem opaque and\rforbidding if you're not already familiar with it.  In fact, there are\rmany similarities between perceptrons and sigmoid neurons, and the\ralgebraic form of the sigmoid function turns out to be more of a\rtechnical detail than a true barrier to understanding.",
            "To understand the similarity to the perceptron model, suppose $z\r\\equiv w \\cdot x + b$ is a large positive number.  Then $e^{-z}\r\\approx 0$ and so $\\sigma(z) \\approx 1$.  In other words, when $z = w\r\\cdot x+b$ is large and positive, the output from the sigmoid neuron\ris approximately $1$, just as it would have been for a perceptron.\rSuppose on the other hand that $z = w \\cdot x+b$ is very negative.\rThen $e^{-z} \\rightarrow \\infty$, and $\\sigma(z) \\approx 0$.  So when\r$z = w \\cdot x +b$ is very negative, the behaviour of a sigmoid neuron\ralso closely approximates a perceptron.  It's only when $w \\cdot x+b$\ris of modest size that there's much deviation from the perceptron\rmodel.",
            "What about the algebraic form of $\\sigma$?  How can we understand\rthat?  In fact, the exact form of $\\sigma$ isn't so important - what\rreally matters is the shape of the function when plotted.  Here's the\rshape:",
            "",
            "This shape is a smoothed out version of a step function:",
            "",
            "If $\\sigma$ had in fact been a step function, then the sigmoid neuron\rwouldbea perceptron, since the output would be $1$ or $0$\rdepending on whether $w\\cdot x+b$ was positive or\rnegative**Actually, when $w \\cdot x +b = 0$ the perceptron\r  outputs $0$, while the step function outputs $1$.  So, strictly\r  speaking, we'd need to modify the step function at that one point.\r  But you get the idea..  By using the actual $\\sigma$ function we\rget, as already implied above, a smoothed out perceptron.  Indeed,\rit's the smoothness of the $\\sigma$ function that is the crucial fact,\rnot its detailed form.  The smoothness of $\\sigma$ means that small\rchanges $\\Delta w_j$ in the weights and $\\Delta b$ in the bias will\rproduce a small change $\\Delta \\mbox{output}$ in the output from the\rneuron.  In fact, calculus tells us that $\\Delta \\mbox{output}$ is\rwell approximated by\\begin{eqnarray} \r  \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\r  \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\r\\tag{5}\\end{eqnarray}\rwhere the sum is over all the weights, $w_j$, and $\\partial \\,\r\\mbox{output} / \\partial w_j$ and $\\partial \\, \\mbox{output} /\\partial\rb$ denote partial derivatives of the $\\mbox{output}$ with respect to\r$w_j$ and $b$, respectively.  Don't panic if you're not comfortable\rwith partial derivatives!  While the expression above looks\rcomplicated, with all the partial derivatives, it's actually saying\rsomething very simple (and which is very good news): $\\Delta\r\\mbox{output}$ is alinear functionof the changes $\\Delta w_j$\rand $\\Delta b$ in the weights and bias.  This linearity makes it easy\rto choose small changes in the weights and biases to achieve any\rdesired small change in the output.  So while sigmoid neurons have\rmuch of the same qualitative behaviour as perceptrons, they make it\rmuch easier to figure out how changing the weights and biases will\rchange the output.",
            "If it's the shape of $\\sigma$ which really matters, and not its exact\rform, then why use the particular form used for $\\sigma$ in\rEquation(3)\\begin{eqnarray} \r  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}} \\nonumber\\end{eqnarray}?  In fact, later in the book we will\roccasionally consider neurons where the output is $f(w \\cdot x + b)$\rfor some otheractivation function$f(\\cdot)$.  The main thing\rthat changes when we use a different activation function is that the\rparticular values for the partial derivatives in\rEquation(5)\\begin{eqnarray} \r  \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\r  \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b \\nonumber\\end{eqnarray}change.  It turns out that when we\rcompute those partial derivatives later, using $\\sigma$ will simplify\rthe algebra, simply because exponentials have lovely properties when\rdifferentiated.  In any case, $\\sigma$ is commonly-used in work on\rneural nets, and is the activation function we'll use most often in\rthis book.",
            "How should we interpret the output from a sigmoid neuron?  Obviously,\rone big difference between perceptrons and sigmoid neurons is that\rsigmoid neurons don't just output $0$ or $1$.  They can have as output\rany real number between $0$ and $1$, so values such as $0.173\\ldots$\rand $0.689\\ldots$ are legitimate outputs.  This can be useful, for\rexample, if we want to use the output value to represent the average\rintensity of the pixels in an image input to a neural network.  But\rsometimes it can be a nuisance.  Suppose we want the output from the\rnetwork to indicate either \"the input image is a 9\" or \"the input\rimage is not a 9\".  Obviously, it'd be easiest to do this if the\routput was a $0$ or a $1$, as in a perceptron.  But in practice we can\rset up a convention to deal with this, for example, by deciding to\rinterpret any output of at least $0.5$ as indicating a \"9\", and any\routput less than $0.5$ as indicating \"not a 9\".  I'll always\rexplicitly state when we're using such a convention, so it shouldn't\rcause any confusion.",
            "ExercisesSigmoid neurons simulating perceptrons, part I$\\mbox{}$Suppose we take all the weights and biases in a network of\r  perceptrons, and multiply them by a positive constant, $c > 0$.\r  Show that the behaviour of the network doesn't change.",
            "Sigmoid neurons simulating perceptrons, part II$\\mbox{}$Suppose we have the same setup as the last problem - a\r  network of perceptrons.  Suppose also that the overall input to the\r  network of perceptrons has been chosen.  We won't need the actual\r  input value, we just need the input to have been fixed.  Suppose the\r  weights and biases are such that $w \\cdot x + b \\neq 0$ for the\r  input $x$ to any particular perceptron in the network.  Now replace\r  all the perceptrons in the network by sigmoid neurons, and multiply\r  the weights and biases by a positive constant $c > 0$. Show that in\r  the limit as $c \\rightarrow \\infty$ the behaviour of this network of\r  sigmoid neurons is exactly the same as the network of perceptrons.\r  How can this fail when $w \\cdot x + b = 0$ for one of the\r  perceptrons?",
            "The architecture of neural networks"
        ],
        "The architecture of neural networks": [
            "In the next section I'll introduce a neural network that can do a\rpretty good job classifying handwritten digits.  In preparation for\rthat, it helps to explain some terminology that lets us name different\rparts of a network.  Suppose we have the network:As mentioned earlier, the leftmost layer in this network is called the\rinput layer, and the neurons within the\rlayer are calledinput neurons.\rThe rightmost oroutputlayer\rcontains theoutput neurons, or,\ras in this case, a single output neuron.  The middle layer is called ahidden layer, since the neurons in\rthis layer are neither inputs nor outputs.  The term \"hidden\"\rperhaps sounds a little mysterious - the first time I heard the term\rI thought it must have some deep philosophical or mathematical\rsignificance - but it really means nothing more than \"not an input\ror an output\".  The network above has just a single hidden layer, but\rsome networks have multiple hidden layers.  For example, the following\rfour-layer network has two hidden layers:Somewhat confusingly, and for historical reasons, such multiple layer\rnetworks are sometimes calledmultilayer perceptronsorMLPs, despite being made up of sigmoid neurons, not\rperceptrons.  I'm not going to use the MLP terminology in this book,\rsince I think it's confusing, but wanted to warn you of its existence.",
            "The design of the input and output layers in a network is often\rstraightforward.  For example, suppose we're trying to determine\rwhether a handwritten image depicts a \"9\" or not.  A natural way to\rdesign the network is to encode the intensities of the image pixels\rinto the input neurons. If the image is a $64$ by $64$ greyscale\rimage, then we'd have $4,096 = 64 \\times 64$ input neurons, with the\rintensities scaled appropriately between $0$ and $1$.  The output\rlayer will contain just a single neuron, with output values of less\rthan $0.5$ indicating \"input image is not a 9\", and values greater\rthan $0.5$ indicating \"input image is a 9 \".",
            "",
            "",
            "While the design of the input and output layers of a neural network is\roften straightforward, there can be quite an art to the design of the\rhidden layers.  In particular, it's not possible to sum up the design\rprocess for the hidden layers with a few simple rules of thumb.\rInstead, neural networks researchers have developed many design\rheuristics for the hidden layers, which help people get the behaviour\rthey want out of their nets.  For example, such heuristics can be used\rto help determine how to trade off the number of hidden layers against\rthe time required to train the network.  We'll meet several such\rdesign heuristics later in this book.",
            "Up to now, we've been discussing neural networks where the output from\rone layer is used as input to the next layer.  Such networks are\rcalledfeedforwardneural networks.  This means there are no loops in the network -\rinformation is always fed forward, never fed back.  If we did have\rloops, we'd end up with situations where the input to the $\\sigma$\rfunction depended on the output.  That'd be hard to make sense of, and\rso we don't allow such loops.",
            "However, there are other models of artificial neural networks in which\rfeedback loops are possible.  These models are calledrecurrent\r  neural networks. The idea in these models is to have neurons which\rfire for some limited duration of time, before becoming quiescent.\rThat firing can stimulate other neurons, which may fire a little while\rlater, also for a limited duration.  That causes still more neurons to\rfire, and so over time we get a cascade of neurons firing.  Loops\rdon't cause problems in such a model, since a neuron's output only\raffects its input at some later time, not instantaneously.",
            "",
            "Recurrent neural nets have been less influential than feedforward\rnetworks, in part because the learning algorithms for recurrent nets\rare (at least to date) less powerful.  But recurrent networks are\rstill extremely interesting.  They're much closer in spirit to how our\rbrains work than feedforward networks.  And it's possible that\rrecurrent networks can solve important problems which can only be\rsolved with great difficulty by feedforward networks.  However, to\rlimit our scope, in this book we're going to concentrate on the more\rwidely-used feedforward networks.",
            "A simple network to classify handwritten digits"
        ],
        "A simple network to classify handwritten digits": [
            "Having defined neural networks, let's return to handwriting\rrecognition.  We can split the problem of recognizing handwritten\rdigits into two sub-problems.  First, we'd like a way of breaking an\rimage containing many digits into a sequence of separate images, each\rcontaining a single digit.  For example, we'd like to break the image",
            "",
            "into six separate images,",
            "",
            "We humans solve thissegmentation\r  problemwith ease, but it's challenging\rfor a computer program to correctly break up the image.  Once the\rimage has been segmented, the program then needs to classify each\rindividual digit.  So, for instance, we'd like our program to\rrecognize that the first digit above,",
            "",
            "is a 5.",
            "We'll focus on writing a program to solve the second problem, that is,\rclassifying individual digits.  We do this because it turns out that\rthe segmentation problem is not so difficult to solve, once you have a\rgood way of classifying individual digits.  There are many approaches\rto solving the segmentation problem.  One approach is to trial many\rdifferent ways of segmenting the image, using the individual digit\rclassifier to score each trial segmentation.  A trial segmentation\rgets a high score if the individual digit classifier is confident of\rits classification in all segments, and a low score if the classifier\ris having a lot of trouble in one or more segments.  The idea is that\rif the classifier is having trouble somewhere, then it's probably\rhaving trouble because the segmentation has been chosen incorrectly.\rThis idea and other variations can be used to solve the segmentation\rproblem quite well.  So instead of worrying about segmentation we'll\rconcentrate on developing a neural network which can solve the more\rinteresting and difficult problem, namely, recognizing individual\rhandwritten digits.",
            "To recognize individual digits we will use a three-layer neural\rnetwork:",
            "",
            "The input layer of the network contains neurons encoding the values of\rthe input pixels.  As discussed in the next section, our training data\rfor the network will consist of many $28$ by $28$ pixel images of\rscanned handwritten digits, and so the input layer contains $784 = 28\r\\times 28$ neurons.  For simplicity I've omitted most of the $784$\rinput neurons in the diagram above.  The input pixels are greyscale,\rwith a value of $0.0$ representing white, a value of $1.0$\rrepresenting black, and in between values representing gradually\rdarkening shades of grey.",
            "The second layer of the network is a hidden layer.  We denote the\rnumber of neurons in this hidden layer by $n$, and we'll experiment\rwith different values for $n$.  The example shown illustrates a small\rhidden layer, containing just $n = 15$ neurons.",
            "The output layer of the network contains 10 neurons.  If the first\rneuron fires, i.e., has an output $\\approx 1$, then that will indicate\rthat the network thinks the digit is a $0$.  If the second neuron\rfires then that will indicate that the network thinks the digit is a\r$1$.  And so on.  A little more precisely, we number the output\rneurons from $0$ through $9$, and figure out which neuron has the\rhighest activation value.  If that neuron is, say, neuron number $6$,\rthen our network will guess that the input digit was a $6$.  And so on\rfor the other output neurons.",
            "You might wonder why we use $10$ output neurons.  After all, the goal\rof the network is to tell us which digit ($0, 1, 2, \\ldots, 9$)\rcorresponds to the input image.  A seemingly natural way of doing that\ris to use just $4$ output neurons, treating each neuron as taking on a\rbinary value, depending on whether the neuron's output is closer to\r$0$ or to $1$.  Four neurons are enough to encode the answer, since\r$2^4 = 16$ is more than the 10 possible values for the input digit.\rWhy should our network use $10$ neurons instead?  Isn't that\rinefficient?  The ultimate justification is empirical: we can try out\rboth network designs, and it turns out that, for this particular\rproblem, the network with $10$ output neurons learns to recognize\rdigits better than the network with $4$ output neurons.  But that\rleaves us wonderingwhyusing $10$ output neurons works better.\rIs there some heuristic that would tell us in advance that we should\ruse the $10$-output encoding instead of the $4$-output encoding?",
            "To understand why we do this, it helps to think about what the neural\rnetwork is doing from first principles.  Consider first the case where\rwe use $10$ output neurons.  Let's concentrate on the first output\rneuron, the one that's trying to decide whether or not the digit is a\r$0$.  It does this by weighing up evidence from the hidden layer of\rneurons.  What are those hidden neurons doing?  Well, just suppose for\rthe sake of argument that the first neuron in the hidden layer detects\rwhether or not an image like the following is present:",
            "",
            "It can do this by heavily weighting input pixels which overlap with\rthe image, and only lightly weighting the other inputs.  In a similar\rway, let's suppose for the sake of argument that the second, third,\rand fourth neurons in the hidden layer detect whether or not the\rfollowing images are present:",
            "",
            "As you may have guessed, these four images together make up the $0$\rimage that we saw in the line of digits shownearlier:",
            "",
            "So if all four of these hidden neurons are firing then we can conclude\rthat the digit is a $0$.  Of course, that's not theonlysort\rof evidence we can use to conclude that the image was a $0$ - we\rcould legitimately get a $0$ in many other ways (say, through\rtranslations of the above images, or slight distortions).  But it\rseems safe to say that at least in this case we'd conclude that the\rinput was a $0$.",
            "",
            "",
            "",
            "Supposing the neural network functions in this way, we can give a\rplausible explanation for why it's better to have $10$ outputs from\rthe network, rather than $4$.  If we had $4$ outputs, then the first\routput neuron would be trying to decide what the most significant bit\rof the digit was.  And there's no easy way to relate that most\rsignificant bit to simple shapes like those shown above.  It's hard to\rimagine that there's any good historical reason the component shapes\rof the digit will be closely related to (say) the most significant bit\rin the output.",
            "Now, with all that said, this is all just a heuristic.  Nothing says\rthat the three-layer neural network has to operate in the way I\rdescribed, with the hidden neurons detecting simple component shapes.\rMaybe a clever learning algorithm will find some assignment of weights\rthat lets us use only $4$ output neurons.  But as a heuristic the way\rof thinking I've described works pretty well, and can save you a lot\rof time in designing good neural network architectures.",
            "ExerciseThere is a way of determining the bitwise representation of a\r  digit by adding an extra layer to the three-layer network above.\r  The extra layer converts the output from the previous layer into a\r  binary representation, as illustrated in the figure below.  Find a\r  set of weights and biases for the new output layer.  Assume that the\r  first $3$ layers of neurons are such that the correct output in the\r  third layer (i.e., the old output layer) has activation at least\r  $0.99$, and incorrect outputs have activation less than $0.01$.",
            "",
            "",
            "",
            "",
            "Learning with gradient descent"
        ],
        "Learning with gradient descent": [
            "",
            "Now that we have a design for our neural network, how can it learn to\rrecognize digits?  The first thing we'll need is a data set to learn\rfrom - a so-called training data set.  We'll use theMNIST\r  data set, which contains tens of thousands of scanned images of\rhandwritten digits, together with their correct classifications.\rMNIST's name comes from the fact that it is a modified subset of two\rdata sets collected byNIST,\rthe United States' National Institute of Standards and\rTechnology. Here's a few images from MNIST:",
            "",
            "As you can see, these digits are, in fact, the same as those shown\rat thebeginning of this chapteras a challenge\rto recognize.  Of course, when testing our network we'll ask it to\rrecognize images which aren't in the training set!",
            "The MNIST data comes in two parts.  The first part contains 60,000\rimages to be used as training data.  These images are scanned\rhandwriting samples from 250 people, half of whom were US Census\rBureau employees, and half of whom were high school students.  The\rimages are greyscale and 28 by 28 pixels in size.  The second part of\rthe MNIST data set is 10,000 images to be used as test data.  Again,\rthese are 28 by 28 greyscale images.  We'll use the test data to\revaluate how well our neural network has learned to recognize digits.\rTo make this a good test of performance, the test data was taken from\radifferentset of 250 people than the original training data\r(albeit still a group split between Census Bureau employees and high\rschool students).  This helps give us confidence that our system can\rrecognize digits from people whose writing it didn't see during\rtraining.",
            "We'll use the notation $x$ to denote a training input.  It'll be\rconvenient to regard each training input $x$ as a $28 \\times 28 =\r784$-dimensional vector.  Each entry in the vector represents the grey\rvalue for a single pixel in the image.  We'll denote the corresponding\rdesired output by $y = y(x)$, where $y$ is a $10$-dimensional vector.\rFor example, if a particular training image, $x$, depicts a $6$, then\r$y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$ is the desired output from\rthe network.  Note that $T$ here is the transpose operation, turning a\rrow vector into an ordinary (column) vector.",
            "What we'd like is an algorithm which lets us find weights and biases\rso that the output from the network approximates $y(x)$ for all\rtraining inputs $x$.  To quantify how well we're achieving this goal\rwe define acost function**Sometimes referred to as alossorobjectivefunction.  We use the term cost\r  function throughout this book, but you should note the other\r  terminology, since it's often used in research papers and other\r  discussions of neural networks.:\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\r\\tag{6}\\end{eqnarray}\rHere, $w$ denotes the collection of all weights in the network, $b$\rall the biases, $n$ is the total number of training inputs, $a$ is the\rvector of outputs from the network when $x$ is input, and the sum is\rover all training inputs, $x$.  Of course, the output $a$ depends on\r$x$, $w$ and $b$, but to keep the notation simple I haven't explicitly\rindicated this dependence.  The notation $\\| v \\|$ just denotes the\rusual length function for a vector $v$.  We'll call $C$ thequadraticcost function; it's also\rsometimes known as themean squared erroror justMSE.\rInspecting the form of the quadratic cost function, we see that\r$C(w,b)$ is non-negative, since every term in the sum is non-negative.\rFurthermore, the cost $C(w,b)$ becomes small, i.e., $C(w,b) \\approx\r0$, precisely when $y(x)$ is approximately equal to the output, $a$,\rfor all training inputs, $x$.  So our training algorithm has done a\rgood job if it can find weights and biases so that $C(w,b) \\approx 0$.\rBy contrast, it's not doing so well when $C(w,b)$ is large - that\rwould mean that $y(x)$ is not close to the output $a$ for a large\rnumber of inputs.  So the aim of our training algorithm will be to\rminimize the cost $C(w,b)$ as a function of the weights and biases.\rIn other words, we want to find a set of weights and biases which make\rthe cost as small as possible.  We'll do that using an algorithm known\rasgradient descent.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "Why introduce the quadratic cost?  After all, aren't we primarily\rinterested in the number of images correctly classified by the\rnetwork?  Why not try to maximize that number directly, rather than\rminimizing a proxy measure like the quadratic cost?  The problem with\rthat is that the number of images correctly classified is not a smooth\rfunction of the weights and biases in the network.  For the most part,\rmaking small changes to the weights and biases won't cause any change\rat all in the number of training images classified correctly.  That\rmakes it difficult to figure out how to change the weights and biases\rto get improved performance.  If we instead use a smooth cost function\rlike the quadratic cost it turns out to be easy to figure out how to\rmake small changes in the weights and biases so as to get an\rimprovement in the cost.  That's why we focus first on minimizing the\rquadratic cost, and only after that will we examine the classification\raccuracy.",
            "",
            "Even given that we want to use a smooth cost function, you may still\rwonder why we choose the quadratic function used in\rEquation(6)\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 \\nonumber\\end{eqnarray}.  Isn't this a ratherad\r  hocchoice?  Perhaps if we chose a different cost function we'd get\ra totally different set of minimizing weights and biases?  This is a\rvalid concern, and later we'll revisit the cost function, and make\rsome modifications.  However, the quadratic cost function of\rEquation(6)\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 \\nonumber\\end{eqnarray}works perfectly well for\runderstanding the basics of learning in neural networks, so we'll\rstick with it for now.",
            "Recapping, our goal in training a neural network is to find weights\rand biases which minimize the quadratic cost function $C(w, b)$.  This\ris a well-posed problem, but it's got a lot of distracting structure\ras currently posed - the interpretation of $w$ and $b$ as weights\rand biases, the $\\sigma$ function lurking in the background, the\rchoice of network architecture, MNIST, and so on.  It turns out that\rwe can understand a tremendous amount by ignoring most of that\rstructure, and just concentrating on the minimization aspect.  So for\rnow we're going to forget all about the specific form of the cost\rfunction, the connection to neural networks, and so on.  Instead,\rwe're going to imagine that we've simply been given a function of many\rvariables and we want to minimize that function.  We're going to\rdevelop a technique calledgradient descentwhich can be used\rto solve such minimization problems.  Then we'll come back to the\rspecific function we want to minimize for neural networks.",
            "Okay, let's suppose we're trying to minimize some function, $C(v)$.\rThis could be any real-valued function of many variables, $v = v_1,\rv_2, \\ldots$.  Note that I've replaced the $w$ and $b$ notation by $v$\rto emphasize that this could be any function - we're not\rspecifically thinking in the neural networks context any more.  To\rminimize $C(v)$ it helps to imagine $C$ as a function of just two\rvariables, which we'll call $v_1$ and $v_2$:",
            "",
            "What we'd like is to find where $C$ achieves its global minimum.  Now,\rof course, for the function plotted above, we can eyeball the graph\rand find the minimum.  In that sense, I've perhaps shown slightlytoosimple a function! A general function, $C$, may be a\rcomplicated function of many variables, and it won't usually be\rpossible to just eyeball the graph to find the minimum.",
            "One way of attacking the problem is to use calculus to try to find the\rminimum analytically.  We could compute derivatives and then try using\rthem to find places where $C$ is an extremum.  With some luck that\rmight work when $C$ is a function of just one or a few variables.  But\rit'll turn into a nightmare when we have many more variables.  And for\rneural networks we'll often wantfarmore variables - the\rbiggest neural networks have cost functions which depend on billions\rof weights and biases in an extremely complicated way.  Using calculus\rto minimize that just won't work!",
            "(After asserting that we'll gain insight by imagining $C$ as a\rfunction of just two variables, I've turned around twice in two\rparagraphs and said, \"hey, but what if it's a function of many more\rthan two variables?\"  Sorry about that.  Please believe me when I say\rthat it really does help to imagine $C$ as a function of two\rvariables.  It just happens that sometimes that picture breaks down,\rand the last two paragraphs were dealing with such breakdowns.  Good\rthinking about mathematics often involves juggling multiple intuitive\rpictures, learning when it's appropriate to use each picture, and when\rit's not.)",
            "",
            "Okay, so calculus doesn't work.  Fortunately, there is a beautiful\ranalogy which suggests an algorithm which works pretty well.  We start\rby thinking of our function as a kind of a valley.  If you squint just\ra little at the plot above, that shouldn't be too hard.  And we\rimagine a ball rolling down the slope of the valley.  Our everyday\rexperience tells us that the ball will eventually roll to the bottom\rof the valley.  Perhaps we can use this idea as a way to find a\rminimum for the function?  We'd randomly choose a starting point for\ran (imaginary) ball, and then simulate the motion of the ball as it\rrolled down to the bottom of the valley.  We could do this simulation\rsimply by computing derivatives (and perhaps some second derivatives)\rof $C$ - those derivatives would tell us everything we need to know\rabout the local \"shape\" of the valley, and therefore how our ball\rshould roll.",
            "Based on what I've just written, you might suppose that we'll be\rtrying to write down Newton's equations of motion for the ball,\rconsidering the effects of friction and gravity, and so on.  Actually,\rwe're not going to take the ball-rolling analogy quite that seriously\r- we're devising an algorithm to minimize $C$, not developing an\raccurate simulation of the laws of physics!  The ball's-eye view is\rmeant to stimulate our imagination, not constrain our thinking.  So\rrather than get into all the messy details of physics, let's simply\rask ourselves: if we were declared God for a day, and could make up\rour own laws of physics, dictating to the ball how it should roll,\rwhat law or laws of motion could we pick that would make it so the\rball always rolled to the bottom of the valley?",
            "To make this question more precise, let's think about what happens\rwhen we move the ball a small amount $\\Delta v_1$ in the $v_1$\rdirection, and a small amount $\\Delta v_2$ in the $v_2$ direction.\rCalculus tells us that $C$ changes as follows:\\begin{eqnarray} \r  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\r  \\frac{\\partial C}{\\partial v_2} \\Delta v_2.\r\\tag{7}\\end{eqnarray}\rWe're going to find a way of choosing $\\Delta v_1$ and $\\Delta v_2$ so\ras to make $\\Delta C$ negative; i.e., we'll choose them so the ball is\rrolling down into the valley.  To figure out how to make such a choice\rit helps to define $\\Delta v$ to be the vector of changes in $v$,\r$\\Delta v \\equiv (\\Delta v_1, \\Delta v_2)^T$, where $T$ is again the\rtranspose operation, turning row vectors into column vectors.  We'll\ralso define thegradientof $C$\rto be the vector of partial derivatives, $\\left(\\frac{\\partial\r    C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2}\\right)^T$.  We\rdenote the gradient vector by $\\nabla C$, i.e.:\\begin{eqnarray} \r  \\nabla C \\equiv \\left( \\frac{\\partial C}{\\partial v_1}, \r  \\frac{\\partial C}{\\partial v_2} \\right)^T.\r\\tag{8}\\end{eqnarray}\rIn a moment we'll rewrite the change $\\Delta C$ in terms of $\\Delta v$\rand the gradient, $\\nabla C$.  Before getting to that, though, I want\rto clarify something that sometimes gets people hung up on the\rgradient.  When meeting the $\\nabla C$ notation for the first time,\rpeople sometimes wonder how they should think about the $\\nabla$\rsymbol.  What, exactly, does $\\nabla$ mean?  In fact, it's perfectly\rfine to think of $\\nabla C$ as a single mathematical object - the\rvector defined above - which happens to be written using two\rsymbols.  In this point of view, $\\nabla$ is just a piece of\rnotational flag-waving, telling you \"hey, $\\nabla C$ is a gradient\rvector\".  There are more advanced points of view where $\\nabla$ can\rbe viewed as an independent mathematical entity in its own right (for\rexample, as a differential operator), but we won't need such points of\rview.",
            "With these definitions, the expression(7)\\begin{eqnarray} \r  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\r  \\frac{\\partial C}{\\partial v_2} \\Delta v_2 \\nonumber\\end{eqnarray}for\r$\\Delta C$ can be rewritten as\\begin{eqnarray} \r  \\Delta C \\approx \\nabla C \\cdot \\Delta v.\r\\tag{9}\\end{eqnarray}\rThis equation helps explain why $\\nabla C$ is called the gradient\rvector: $\\nabla C$ relates changes in $v$ to changes in $C$, just as\rwe'd expect something called a gradient to do.  But what's really\rexciting about the equation is that it lets us see how to choose\r$\\Delta v$ so as to make $\\Delta C$ negative.  In particular, suppose\rwe choose\\begin{eqnarray} \r  \\Delta v = -\\eta \\nabla C,\r\\tag{10}\\end{eqnarray}\rwhere $\\eta$ is a small, positive parameter (known as thelearning rate).\rThen Equation(9)\\begin{eqnarray} \r  \\Delta C \\approx \\nabla C \\cdot \\Delta v \\nonumber\\end{eqnarray}tells us that $\\Delta C \\approx -\\eta\r\\nabla C \\cdot \\nabla C = -\\eta \\|\\nabla C\\|^2$.  Because $\\| \\nabla C\r\\|^2 \\geq 0$, this guarantees that $\\Delta C \\leq 0$, i.e., $C$ will\ralways decrease, never increase, if we change $v$ according to the\rprescription in(10)\\begin{eqnarray} \r  \\Delta v = -\\eta \\nabla C \\nonumber\\end{eqnarray}.  (Within, of course, the\rlimits of the approximation in Equation(9)\\begin{eqnarray} \r  \\Delta C \\approx \\nabla C \\cdot \\Delta v \\nonumber\\end{eqnarray}).  This is\rexactly the property we wanted!  And so we'll take\rEquation(10)\\begin{eqnarray} \r  \\Delta v = -\\eta \\nabla C \\nonumber\\end{eqnarray}to define the \"law of motion\"\rfor the ball in our gradient descent algorithm.  That is, we'll use\rEquation(10)\\begin{eqnarray} \r  \\Delta v = -\\eta \\nabla C \\nonumber\\end{eqnarray}to compute a value for $\\Delta\rv$, then move the ball's position $v$ by that amount:\\begin{eqnarray}\r  v \\rightarrow v' = v -\\eta \\nabla C.\r\\tag{11}\\end{eqnarray}\rThen we'll use this update rule again, to make another move.  If we\rkeep doing this, over and over, we'll keep decreasing $C$ until - we\rhope - we reach a global minimum.",
            "Summing up, the way the gradient descent algorithm works is to\rrepeatedly compute the gradient $\\nabla C$, and then to move in theoppositedirection, \"falling down\" the slope of the valley.\rWe can visualize it like this:",
            "",
            "Notice that with this rule gradient descent doesn't reproduce real\rphysical motion.  In real life a ball has momentum, and that momentum\rmay allow it to roll across the slope, or even (momentarily) roll\ruphill.  It's only after the effects of friction set in that the ball\ris guaranteed to roll down into the valley.  By contrast, our rule for\rchoosing $\\Delta v$ just says \"go down, right now\".  That's still a\rpretty good rule for finding the minimum!",
            "To make gradient descent work correctly, we need to choose the\rlearning rate $\\eta$ to be small\renough that Equation(9)\\begin{eqnarray} \r  \\Delta C \\approx \\nabla C \\cdot \\Delta v \\nonumber\\end{eqnarray}is a good approximation.  If\rwe don't, we might end up with $\\Delta C > 0$, which obviously would\rnot be good!  At the same time, we don't want $\\eta$ to be too small,\rsince that will make the changes $\\Delta v$ tiny, and thus the\rgradient descent algorithm will work very slowly.  In practical\rimplementations, $\\eta$ is often varied so that\rEquation(9)\\begin{eqnarray} \r  \\Delta C \\approx \\nabla C \\cdot \\Delta v \\nonumber\\end{eqnarray}remains a good approximation, but the\ralgorithm isn't too slow.  We'll see later how this\rworks.",
            "I've explained gradient descent when $C$ is a function of just two\rvariables.  But, in fact, everything works just as well even when $C$\ris a function of many more variables.  Suppose in particular that $C$\ris a function of $m$ variables, $v_1,\\ldots,v_m$.  Then the change\r$\\Delta C$ in $C$ produced by a small change $\\Delta v = (\\Delta v_1,\r\\ldots, \\Delta v_m)^T$ is\\begin{eqnarray} \r  \\Delta C \\approx \\nabla C \\cdot \\Delta v,\r\\tag{12}\\end{eqnarray}\rwhere the gradient $\\nabla C$ is the vector\\begin{eqnarray}\r  \\nabla C \\equiv \\left(\\frac{\\partial C}{\\partial v_1}, \\ldots, \r  \\frac{\\partial C}{\\partial v_m}\\right)^T.\r\\tag{13}\\end{eqnarray}\rJust as for the two variable case, we can\rchoose\\begin{eqnarray}\r  \\Delta v = -\\eta \\nabla C,\r\\tag{14}\\end{eqnarray}\rand we're guaranteed that our (approximate)\rexpression(12)\\begin{eqnarray} \r  \\Delta C \\approx \\nabla C \\cdot \\Delta v \\nonumber\\end{eqnarray}for $\\Delta C$ will be negative.\rThis gives us a way of following the gradient to a minimum, even when\r$C$ is a function of many variables, by repeatedly applying the update\rrule\\begin{eqnarray}\r  v \\rightarrow v' = v-\\eta \\nabla C.\r\\tag{15}\\end{eqnarray}\rYou can think of this update rule asdefiningthe gradient\rdescent algorithm.  It gives us a way of repeatedly changing the\rposition $v$ in order to find a minimum of the function $C$.  The rule\rdoesn't always work - several things can go wrong and prevent\rgradient descent from finding the global minimum of $C$, a point we'll\rreturn to explore in later chapters.  But, in practice gradient\rdescent often works extremely well, and in neural networks we'll find\rthat it's a powerful way of minimizing the cost function, and so\rhelping the net learn.",
            "",
            "",
            "Indeed, there's even a sense in which gradient descent is the optimal\rstrategy for searching for a minimum.  Let's suppose that we're trying\rto make a move $\\Delta v$ in position so as to decrease $C$ as much as\rpossible.  This is equivalent to minimizing $\\Delta C \\approx \\nabla C\r\\cdot \\Delta v$.  We'll constrain the size of the move so that $\\|\r\\Delta v \\| = \\epsilon$ for some small fixed $\\epsilon > 0$.  In other\rwords, we want a move that is a small step of a fixed size, and we're\rtrying to find the movement direction which decreases $C$ as much as\rpossible.  It can be proved that the choice of $\\Delta v$ which\rminimizes $\\nabla C \\cdot \\Delta v$ is $\\Delta v = - \\eta \\nabla C$,\rwhere $\\eta = \\epsilon / \\|\\nabla C\\|$ is determined by the size\rconstraint $\\|\\Delta v\\| = \\epsilon$.  So gradient descent can be\rviewed as a way of taking small steps in the direction which does the\rmost to immediately decrease $C$.",
            "ExercisesProve the assertion of the last paragraph.Hint:If\r    you're not already familiar with theCauchy-Schwarz\r      inequality, you may find it helpful to familiarize yourself\r    with it.",
            "I explained gradient descent when $C$ is a function of two\r  variables, and when it's a function of more than two variables.\r  What happens when $C$ is a function of just one variable?  Can you\r  provide a geometric interpretation of what gradient descent is doing\r  in the one-dimensional case?",
            "",
            "People have investigated many variations of gradient descent,\rincluding variations that more closely mimic a real physical ball.\rThese ball-mimicking variations have some advantages, but also have a\rmajor disadvantage: it turns out to be necessary to compute second\rpartial derivatives of $C$, and this can be quite costly.  To see why\rit's costly, suppose we want to compute all the second partial\rderivatives $\\partial^2 C/ \\partial v_j \\partial v_k$.  If there are a\rmillion such $v_j$ variables then we'd need to compute something like\ra trillion (i.e., a million squared) second partial\rderivatives**Actually, more like half a trillion, since\r  $\\partial^2 C/ \\partial v_j \\partial v_k = \\partial^2 C/ \\partial\r  v_k \\partial v_j$.  Still, you get the point.!  That's going to be\rcomputationally costly.  With that said, there are tricks for avoiding\rthis kind of problem, and finding alternatives to gradient descent is\ran active area of investigation.  But in this book we'll use gradient\rdescent (and variations) as our main approach to learning in neural\rnetworks.",
            "How can we apply gradient descent to learn in a neural network?  The\ridea is to use gradient descent to find the weights $w_k$ and biases\r$b_l$ which minimize the cost in\rEquation(6)\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 \\nonumber\\end{eqnarray}.  To see how this works, let's\rrestate the gradient descent update rule, with the weights and biases\rreplacing the variables $v_j$.  In other words, our \"position\" now\rhas components $w_k$ and $b_l$, and the gradient vector $\\nabla C$ has\rcorresponding components $\\partial C / \\partial w_k$ and $\\partial C\r/ \\partial b_l$.  Writing out the gradient descent update rule in\rterms of components, we have\\begin{eqnarray}\r  w_k & \\rightarrow & w_k' = w_k-\\eta \\frac{\\partial C}{\\partial w_k} \\tag{16}\\\\\r  b_l & \\rightarrow & b_l' = b_l-\\eta \\frac{\\partial C}{\\partial b_l}.\r\\tag{17}\\end{eqnarray}\rBy repeatedly applying this update rule we can \"roll down the hill\",\rand hopefully find a minimum of the cost function.  In other words,\rthis is a rule which can be used to learn in a neural network.",
            "There are a number of challenges in applying the gradient descent\rrule.  We'll look into those in depth in later chapters.  But for now\rI just want to mention one problem.  To understand what the problem\ris, let's look back at the quadratic cost in\rEquation(6)\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 \\nonumber\\end{eqnarray}.  Notice that this cost\rfunction has the form $C = \\frac{1}{n} \\sum_x C_x$, that is, it's an\raverage over costs $C_x \\equiv \\frac{\\|y(x)-a\\|^2}{2}$ for individual\rtraining examples.  In practice, to compute the gradient $\\nabla C$ we\rneed to compute the gradients $\\nabla C_x$ separately for each\rtraining input, $x$, and then average them, $\\nabla C = \\frac{1}{n}\r\\sum_x \\nabla C_x$.  Unfortunately, when the number of training inputs\ris very large this can take a long time, and learning thus occurs\rslowly.",
            "An idea calledstochastic gradient descentcan be used to speed\rup learning.  The idea is to estimate the gradient $\\nabla C$ by\rcomputing $\\nabla C_x$ for a small sample of randomly chosen training\rinputs.  By averaging over this small sample it turns out that we can\rquickly get a good estimate of the true gradient $\\nabla C$, and this\rhelps speed up gradient descent, and thus learning.",
            "To make these ideas more precise, stochastic gradient descent works by\rrandomly picking out a small number $m$ of randomly chosen training\rinputs.  We'll label those random training inputs $X_1, X_2, \\ldots,\rX_m$, and refer to them as amini-batch.  Provided the sample\rsize $m$ is large enough we expect that the average value of the\r$\\nabla C_{X_j}$ will be roughly equal to the average over all $\\nabla\rC_x$, that is,\\begin{eqnarray}\r  \\frac{\\sum_{j=1}^m \\nabla C_{X_{j}}}{m} \\approx \\frac{\\sum_x \\nabla C_x}{n} = \\nabla C,\r\\tag{18}\\end{eqnarray}\rwhere the second sum is over the entire set of training data.\rSwapping sides we get\\begin{eqnarray}\r  \\nabla C \\approx \\frac{1}{m} \\sum_{j=1}^m \\nabla C_{X_{j}},\r\\tag{19}\\end{eqnarray}\rconfirming that we can estimate the overall gradient by computing\rgradients just for the randomly chosen mini-batch.",
            "To connect this explicitly to learning in neural networks, suppose\r$w_k$ and $b_l$ denote the weights and biases in our neural network.\rThen stochastic gradient descent works by picking out a randomly\rchosen mini-batch of training inputs, and training with those,\\begin{eqnarray} \r  w_k & \\rightarrow & w_k' = w_k-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k} \\tag{20}\\\\\r  \r  b_l & \\rightarrow & b_l' = b_l-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l},\r\\tag{21}\\end{eqnarray}\rwhere the sums are over all the training examples $X_j$ in the current\rmini-batch.  Then we pick out another randomly chosen mini-batch and\rtrain with those.  And so on, until we've exhausted the training\rinputs, which is said to complete anepochof training.  At that point\rwe start over with a new training epoch.",
            "Incidentally, it's worth noting that conventions vary about scaling of\rthe cost function and of mini-batch updates to the weights and biases.\rIn Equation(6)\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 \\nonumber\\end{eqnarray}we scaled the overall cost\rfunction by a factor $\\frac{1}{n}$.  People sometimes omit the\r$\\frac{1}{n}$, summing over the costs of individual training examples\rinstead of averaging.  This is particularly useful when the total\rnumber of training examples isn't known in advance.  This can occur if\rmore training data is being generated in real time, for instance.\rAnd, in a similar way, the mini-batch update rules(20)\\begin{eqnarray} \r  w_k & \\rightarrow & w_k' = w_k-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k}  \\nonumber\\end{eqnarray}and(21)\\begin{eqnarray}  \r  b_l & \\rightarrow & b_l' = b_l-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l} \\nonumber\\end{eqnarray}sometimes omit the $\\frac{1}{m}$ term out the\rfront of the sums.  Conceptually this makes little difference, since\rit's equivalent to rescaling the learning rate $\\eta$.  But when doing\rdetailed comparisons of different work it's worth watching out for.",
            "We can think of stochastic gradient descent as being like political\rpolling: it's much easier to sample a small mini-batch than it is to\rapply gradient descent to the full batch, just as carrying out a poll\ris easier than running a full election.  For example, if we have a\rtraining set of size $n = 60,000$, as in MNIST, and choose a\rmini-batch size of (say) $m = 10$, this means we'll get a factor of\r$6,000$ speedup in estimating the gradient!  Of course, the estimate\rwon't be perfect - there will be statistical fluctuations - but it\rdoesn't need to be perfect: all we really care about is moving in a\rgeneral direction that will help decrease $C$, and that means we don't\rneed an exact computation of the gradient.  In practice, stochastic\rgradient descent is a commonly used and powerful technique for\rlearning in neural networks, and it's the basis for most of the\rlearning techniques we'll develop in this book.",
            "",
            "",
            "",
            "",
            "",
            "ExerciseAn extreme version of gradient descent is to use a mini-batch\r  size of just 1.  That is, given a training input, $x$, we update our\r  weights and biases according to the rules $w_k \\rightarrow w_k' =\r  w_k - \\eta \\partial C_x / \\partial w_k$ and $b_l \\rightarrow b_l' =\r  b_l - \\eta \\partial C_x / \\partial b_l$.  Then we choose another\r  training input, and update the weights and biases again.  And so on,\r  repeatedly.  This procedure is known asonline,on-line, orincrementallearning.  In online learning,\r  a neural network learns from just one training input at a time (just\r  as human beings do).  Name one advantage and one disadvantage of\r  online learning, compared to stochastic gradient descent with a\r  mini-batch size of, say, $20$.",
            "Let me conclude this section by discussing a point that sometimes bugs\rpeople new to gradient descent.  In neural networks the cost $C$ is,\rof course, a function of many variables - all the weights and biases\r- and so in some sense defines a surface in a very high-dimensional\rspace.  Some people get hung up thinking: \"Hey, I have to be able to\rvisualize all these extra dimensions\".  And they may start to worry:\r\"I can't think in four dimensions, let alone five (or five\rmillion)\".  Is there some special ability they're missing, some\rability that \"real\" supermathematicians have?  Of course, the answer\ris no.  Even most professional mathematicians can't visualize four\rdimensions especially well, if at all.  The trick they use, instead,\ris to develop other ways of representing what's going on.  That's\rexactly what we did above: we used an algebraic (rather than visual)\rrepresentation of $\\Delta C$ to figure out how to move so as to\rdecrease $C$.  People who are good at thinking in high dimensions have\ra mental library containing many different techniques along these\rlines; our algebraic trick is just one example.  Those techniques may\rnot have the simplicity we're accustomed to when visualizing three\rdimensions, but once you build up a library of such techniques, you\rcan get pretty good at thinking in high dimensions.  I won't go into\rmore detail here, but if you're interested then you may enjoy readingthis\r  discussionof some of the techniques professional mathematicians\ruse to think in high dimensions.  While some of the techniques\rdiscussed are quite complex, much of the best content is intuitive and\raccessible, and could be mastered by anyone.",
            "",
            "Implementing our network to classify digits"
        ],
        "Implementing our network to classify digits": [
            "Alright, let's write a program that learns how to recognize\rhandwritten digits, using stochastic gradient descent and the MNIST\rtraining data.  We'll do this with a short Python (2.7) program, just\r74 lines of code!  The first thing we need is to get the MNIST data.\rIf you're agituser then you can obtain the data by cloning\rthe code repository for this book,",
            "git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git",
            "If you don't usegitthen you can download the data and codehere.",
            "Incidentally, when I described the MNIST data earlier, I said it was\rsplit into 60,000 training images, and 10,000 test images.  That's the\rofficial MNIST description.  Actually, we're going to split the data a\rlittle differently.  We'll leave the test images as is, but split the\r60,000-image MNIST training set into two parts: a set of 50,000\rimages, which we'll use to train our neural network, and a separate\r10,000 imagevalidation set.  We won't\ruse the validation data in this chapter, but later in the book we'll\rfind it useful in figuring out how to set certainhyper-parametersof the neural network - things like the\rlearning rate, and so on, which aren't directly selected by our\rlearning algorithm.  Although the validation data isn't part of the\roriginal MNIST specification, many people use MNIST in this fashion,\rand the use of validation data is common in neural networks.  When I\rrefer to the \"MNIST training data\" from now on, I'll be referring to\rour 50,000 image data set, not the original 60,000 image data\rset**As noted earlier, the MNIST data set is based on two data\r  sets collected by NIST, the United States' National Institute of\r  Standards and Technology.  To construct MNIST the NIST data sets\r  were stripped down and put into a more convenient format by Yann\r  LeCun, Corinna Cortes, and Christopher J. C. Burges.  Seethis linkfor more\r  details.  The data set in my repository is in a form that makes it\r  easy to load and manipulate the MNIST data in Python.  I obtained\r  this particular form of the data from the LISA machine learning\r  laboratory at the University of Montreal\r  (link)..",
            "",
            "Apart from the MNIST data we also need a Python library calledNumpy, for doing fast linear algebra.  If you\rdon't already have Numpy installed, you can get ithere.",
            "Let me explain the core features of the neural networks code, before\rgiving a full listing, below.  The centerpiece is aNetworkclass, which we use to represent a neural network.  Here's the code we\ruse to initialize aNetworkobject:",
            "classNetwork(object):def__init__(self,sizes):self.num_layers=len(sizes)self.sizes=sizesself.biases=[np.random.randn(y,1)foryinsizes[1:]]self.weights=[np.random.randn(y,x)forx,yinzip(sizes[:-1],sizes[1:])]",
            "In this code, the listsizescontains the number of neurons in\rthe respective layers.  So, for example, if we want to create aNetworkobject with 2 neurons in the first layer, 3 neurons in\rthe second layer, and 1 neuron in the final layer, we'd do this with\rthe code:net=Network([2,3,1])The biases\rand weights in theNetworkobject are all initialized randomly,\rusing the Numpynp.random.randnfunction to generate Gaussian\rdistributions with mean $0$ and standard deviation $1$.  This random\rinitialization gives our stochastic gradient descent algorithm a place\rto start from.  In later chapters we'll find better ways of\rinitializing the weights and biases, but this will do for now.  Note\rthat theNetworkinitialization code assumes that the first\rlayer of neurons is an input layer, and omits to set any biases for\rthose neurons, since biases are only ever used in computing the\routputs from later layers.",
            "Note also that the biases and weights are stored as lists of Numpy\rmatrices.  So, for examplenet.weights[1]is a Numpy matrix\rstoring the weights connecting the second and third layers of neurons.\r(It's not the first and second layers, since Python's list indexing\rstarts at0.)  Sincenet.weights[1]is rather verbose,\rlet's just denote that matrix $w$.  It's a matrix such that $w_{jk}$\ris the weight for the connection between the $k^{\\rm th}$ neuron in the\rsecond layer, and the $j^{\\rm th}$ neuron in the third layer.  This ordering\rof the $j$ and $k$ indices may seem strange - surely it'd make more\rsense to swap the $j$ and $k$ indices around?  The big advantage of\rusing this ordering is that it means that the vector of activations of\rthe third layer of neurons is:\\begin{eqnarray} \r  a' = \\sigma(w a + b).\r\\tag{22}\\end{eqnarray}\rThere's quite a bit going on in this equation, so let's unpack it\rpiece by piece.  $a$ is the vector of activations of the second layer\rof neurons. To obtain $a'$ we multiply $a$ by the weight matrix $w$,\rand add the vector $b$ of biases.  We then apply the function $\\sigma$\relementwise to every entry in the vector $w a +b$.  (This is calledvectorizingthe function\r$\\sigma$.) It's easy to verify that\rEquation(22)\\begin{eqnarray} \r  a' = \\sigma(w a + b) \\nonumber\\end{eqnarray}gives the same result as our\rearlier rule, Equation(4)\\begin{eqnarray} \r  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)} \\nonumber\\end{eqnarray}, for\rcomputing the output of a sigmoid neuron.",
            "ExerciseWrite out Equation(22)\\begin{eqnarray} \r  a' = \\sigma(w a + b) \\nonumber\\end{eqnarray}in component\r  form, and verify that it gives the same result as the\r  rule(4)\\begin{eqnarray} \r  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)} \\nonumber\\end{eqnarray}for computing the output\r  of a sigmoid neuron.",
            "With all this in mind, it's easy to write code computing the output\rfrom aNetworkinstance.  We begin by defining the sigmoid\rfunction:defsigmoid(z):return1.0/(1.0+np.exp(-z))Note that when the inputzis a vector or Numpy array, Numpy\rautomatically applies the functionsigmoidelementwise, that\ris, in vectorized form.",
            "We then add afeedforwardmethod to theNetworkclass,\rwhich, given an inputafor the network, returns the\rcorresponding output**It is assumed that the inputais\r  an(n, 1)Numpy ndarray, not a(n,)vector.  Here,nis the number of inputs to the network.  If you try to use\r  an(n,)vector as input you'll get strange results.  Although\r  using an(n,)vector appears the more natural choice, using\r  an(n, 1)ndarray makes it particularly easy to modify the\r  code to feedforward multiple inputs at once, and that is sometimes\r  convenient..  All the method does is applies\rEquation(22)\\begin{eqnarray} \r  a' = \\sigma(w a + b) \\nonumber\\end{eqnarray}for each layer:deffeedforward(self,a):\"\"\"Return the output of the network if \"a\" is input.\"\"\"forb,winzip(self.biases,self.weights):a=sigmoid(np.dot(w,a)+b)returna",
            "Of course, the main thing we want ourNetworkobjects to do is\rto learn.  To that end we'll give them anSGDmethod which\rimplements stochastic gradient descent.  Here's the code.  It's a\rlittle mysterious in a few places, but I'll break it down below, after\rthe listing.",
            "defSGD(self,training_data,epochs,mini_batch_size,eta,test_data=None):\"\"\"Train the neural network using mini-batch stochasticgradient descent.  The \"training_data\" is a list of tuples\"(x, y)\" representing the training inputs and the desiredoutputs.  The other non-optional parameters areself-explanatory.  If \"test_data\" is provided then thenetwork will be evaluated against the test data after eachepoch, and partial progress printed out.  This is useful fortracking progress, but slows things down substantially.\"\"\"iftest_data:n_test=len(test_data)n=len(training_data)forjinxrange(epochs):random.shuffle(training_data)mini_batches=[training_data[k:k+mini_batch_size]forkinxrange(0,n,mini_batch_size)]formini_batchinmini_batches:self.update_mini_batch(mini_batch,eta)iftest_data:print\"Epoch {0}: {1} / {2}\".format(j,self.evaluate(test_data),n_test)else:print\"Epoch {0} complete\".format(j)",
            "Thetraining_datais a list of tuples(x, y)representing the training inputs and corresponding desired outputs.\rThe variablesepochsandmini_batch_sizeare what you'd\rexpect - the number of epochs to train for, and the size of the\rmini-batches to use when sampling.etais the learning rate,\r$\\eta$.  If the optional argumenttest_datais supplied, then\rthe program will evaluate the network after each epoch of training,\rand print out partial progress.  This is useful for tracking progress,\rbut slows things down substantially.",
            "The code works as follows.  In each epoch, it starts by randomly\rshuffling the training data, and then partitions it into mini-batches\rof the appropriate size.  This is an easy way of sampling randomly\rfrom the training data.  Then for eachmini_batchwe apply a\rsingle step of gradient descent.  This is done by the codeself.update_mini_batch(mini_batch, eta), which updates the\rnetwork weights and biases according to a single iteration of gradient\rdescent, using just the training data inmini_batch.  Here's\rthe code for theupdate_mini_batchmethod:defupdate_mini_batch(self,mini_batch,eta):\"\"\"Update the network's weights and biases by applyinggradient descent using backpropagation to a single mini batch.The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"is the learning rate.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]forx,yinmini_batch:delta_nabla_b,delta_nabla_w=self.backprop(x,y)nabla_b=[nb+dnbfornb,dnbinzip(nabla_b,delta_nabla_b)]nabla_w=[nw+dnwfornw,dnwinzip(nabla_w,delta_nabla_w)]self.weights=[w-(eta/len(mini_batch))*nwforw,nwinzip(self.weights,nabla_w)]self.biases=[b-(eta/len(mini_batch))*nbforb,nbinzip(self.biases,nabla_b)]Most of the work is done by the linedelta_nabla_b,delta_nabla_w=self.backprop(x,y)This invokes something called thebackpropagationalgorithm,\rwhich is a fast way of computing the gradient of the cost function.\rSoupdate_mini_batchworks simply by computing these gradients\rfor every training example in themini_batch, and then updatingself.weightsandself.biasesappropriately.",
            "I'm not going to show the code forself.backpropright now.\rWe'll study how backpropagation works in the next chapter, including\rthe code forself.backprop.  For now, just assume that it\rbehaves as claimed, returning the appropriate gradient for the cost\rassociated to the training examplex.",
            "Let's look at the full program, including the documentation strings,\rwhich I omitted above.  Apart fromself.backpropthe program is\rself-explanatory - all the heavy lifting is done inself.SGDandself.update_mini_batch, which we've already discussed.  Theself.backpropmethod makes use of a few extra functions to help\rin computing the gradient, namelysigmoid_prime, which computes\rthe derivative of the $\\sigma$ function, andself.cost_derivative, which I won't describe here.  You can get\rthe gist of these (and perhaps the details) just by looking at the\rcode and documentation strings.  We'll look at them in detail in the\rnext chapter. \rNote that while the program appears lengthy, much of the code is\rdocumentation strings intended to make the code easy to understand.\rIn fact, the program contains just 74 lines of non-whitespace,\rnon-comment code.  All the code may be found on GitHubhere.",
            "",
            "\"\"\"network.py~~~~~~~~~~A module to implement the stochastic gradient descent learningalgorithm for a feedforward neural network.  Gradients are calculatedusing backpropagation.  Note that I have focused on making the codesimple, easily readable, and easily modifiable.  It is not optimized,and omits many desirable features.\"\"\"#### Libraries# Standard libraryimportrandom# Third-party librariesimportnumpyasnpclassNetwork(object):def__init__(self,sizes):\"\"\"The list ``sizes`` contains the number of neurons in therespective layers of the network.  For example, if the listwas [2, 3, 1] then it would be a three-layer network, with thefirst layer containing 2 neurons, the second layer 3 neurons,and the third layer 1 neuron.  The biases and weights for thenetwork are initialized randomly, using a Gaussiandistribution with mean 0, and variance 1.  Note that the firstlayer is assumed to be an input layer, and by convention wewon't set any biases for those neurons, since biases are onlyever used in computing the outputs from later layers.\"\"\"self.num_layers=len(sizes)self.sizes=sizesself.biases=[np.random.randn(y,1)foryinsizes[1:]]self.weights=[np.random.randn(y,x)forx,yinzip(sizes[:-1],sizes[1:])]deffeedforward(self,a):\"\"\"Return the output of the network if ``a`` is input.\"\"\"forb,winzip(self.biases,self.weights):a=sigmoid(np.dot(w,a)+b)returnadefSGD(self,training_data,epochs,mini_batch_size,eta,test_data=None):\"\"\"Train the neural network using mini-batch stochasticgradient descent.  The ``training_data`` is a list of tuples``(x, y)`` representing the training inputs and the desiredoutputs.  The other non-optional parameters areself-explanatory.  If ``test_data`` is provided then thenetwork will be evaluated against the test data after eachepoch, and partial progress printed out.  This is useful fortracking progress, but slows things down substantially.\"\"\"iftest_data:n_test=len(test_data)n=len(training_data)forjinxrange(epochs):random.shuffle(training_data)mini_batches=[training_data[k:k+mini_batch_size]forkinxrange(0,n,mini_batch_size)]formini_batchinmini_batches:self.update_mini_batch(mini_batch,eta)iftest_data:print\"Epoch {0}: {1} / {2}\".format(j,self.evaluate(test_data),n_test)else:print\"Epoch {0} complete\".format(j)defupdate_mini_batch(self,mini_batch,eta):\"\"\"Update the network's weights and biases by applyinggradient descent using backpropagation to a single mini batch.The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``is the learning rate.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]forx,yinmini_batch:delta_nabla_b,delta_nabla_w=self.backprop(x,y)nabla_b=[nb+dnbfornb,dnbinzip(nabla_b,delta_nabla_b)]nabla_w=[nw+dnwfornw,dnwinzip(nabla_w,delta_nabla_w)]self.weights=[w-(eta/len(mini_batch))*nwforw,nwinzip(self.weights,nabla_w)]self.biases=[b-(eta/len(mini_batch))*nbforb,nbinzip(self.biases,nabla_b)]defbackprop(self,x,y):\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing thegradient for the cost function C_x.  ``nabla_b`` and``nabla_w`` are layer-by-layer lists of numpy arrays, similarto ``self.biases`` and ``self.weights``.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]# feedforwardactivation=xactivations=[x]# list to store all the activations, layer by layerzs=[]# list to store all the z vectors, layer by layerforb,winzip(self.biases,self.weights):z=np.dot(w,activation)+bzs.append(z)activation=sigmoid(z)activations.append(activation)# backward passdelta=self.cost_derivative(activations[-1],y)*\\sigmoid_prime(zs[-1])nabla_b[-1]=deltanabla_w[-1]=np.dot(delta,activations[-2].transpose())# Note that the variable l in the loop below is used a little# differently to the notation in Chapter 2 of the book.  Here,# l = 1 means the last layer of neurons, l = 2 is the# second-last layer, and so on.  It's a renumbering of the# scheme in the book, used here to take advantage of the fact# that Python can use negative indices in lists.forlinxrange(2,self.num_layers):z=zs[-l]sp=sigmoid_prime(z)delta=np.dot(self.weights[-l+1].transpose(),delta)*spnabla_b[-l]=deltanabla_w[-l]=np.dot(delta,activations[-l-1].transpose())return(nabla_b,nabla_w)defevaluate(self,test_data):\"\"\"Return the number of test inputs for which the neuralnetwork outputs the correct result. Note that the neuralnetwork's output is assumed to be the index of whicheverneuron in the final layer has the highest activation.\"\"\"test_results=[(np.argmax(self.feedforward(x)),y)for(x,y)intest_data]returnsum(int(x==y)for(x,y)intest_results)defcost_derivative(self,output_activations,y):\"\"\"Return the vector of partial derivatives \\partial C_x /\\partial a for the output activations.\"\"\"return(output_activations-y)#### Miscellaneous functionsdefsigmoid(z):\"\"\"The sigmoid function.\"\"\"return1.0/(1.0+np.exp(-z))defsigmoid_prime(z):\"\"\"Derivative of the sigmoid function.\"\"\"returnsigmoid(z)*(1-sigmoid(z))",
            "How well does the program recognize handwritten digits?  Well, let's\rstart by loading in the MNIST data.  I'll do this using a little\rhelper program,mnist_loader.py, to be described below.  We\rexecute the following commands in a Python shell,",
            ">>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()",
            "Of course, this could also be done in a separate Python program, but\rif you're following along it's probably easiest to do in a Python\rshell.",
            "After loading the MNIST data, we'll set up aNetworkwith $30$\rhidden neurons.  We do this after importing the Python program listed\rabove, which is namednetwork,",
            ">>>importnetwork>>>net=network.Network([784,30,10])",
            "Finally, we'll use stochastic gradient descent to learn from the MNISTtraining_dataover 30 epochs, with a mini-batch size of 10, and a\rlearning rate of $\\eta = 3.0$,",
            ">>>net.SGD(training_data,30,10,3.0,test_data=test_data)",
            "Note that if you're running the code as you read along, it will take\rsome time to execute - for a typical machine (as of 2015) it will\rlikely take a few minutes to run.  I suggest you set things running,\rcontinue to read, and periodically check the output from the code.  If\ryou're in a rush you can speed things up by decreasing the number of\repochs, by decreasing the number of hidden neurons, or by using only\rpart of the training data.  Note that production code would be much,\rmuch faster: these Python scripts are intended to help you understand\rhow neural nets work, not to be high-performance code!  And, of\rcourse, once we've trained a network it can be run very quickly\rindeed, on almost any computing platform. For example, once we've\rlearned a good set of weights and biases for a network, it can easily\rbe ported to run in Javascript in a web browser, or as a native app on\ra mobile device.  In any case, here is a partial transcript of the\routput of one training run of the neural network.  The transcript\rshows the number of test images correctly recognized by the neural\rnetwork after each epoch of training.  As you can see, after just a\rsingle epoch this has reached 9,129 out of 10,000, and the number\rcontinues to grow,",
            "Epoch 0: 9129 / 10000\nEpoch 1: 9295 / 10000\nEpoch 2: 9348 / 10000\n...\nEpoch 27: 9528 / 10000\nEpoch 28: 9542 / 10000\nEpoch 29: 9534 / 10000",
            "That is, the trained network gives us a classification rate of about\r$95$ percent - $95.42$ percent at its peak (\"Epoch 28\")!  That's\rquite encouraging as a first attempt.  I should warn you, however,\rthat if you run the code then your results are not necessarily going\rto be quite the same as mine, since we'll be initializing our network\rusing (different) random weights and biases.  To generate results in\rthis chapter I've taken best-of-three runs.",
            "Let's rerun the above experiment, changing the number of hidden\rneurons to $100$.  As was the case earlier, if you're running the code\ras you read along, you should be warned that it takes quite a while to\rexecute (on my machine this experiment takes tens of seconds for each\rtraining epoch), so it's wise to continue reading in parallel while\rthe code executes.",
            ">>>net=network.Network([784,100,10])>>>net.SGD(training_data,30,10,3.0,test_data=test_data)",
            "Sure enough, this improves the results to $96.59$ percent.  At least\rin this case, using more hidden neurons helps us get better\rresults**Reader feedback indicates quite some variation in\r  results for this experiment, and some training runs give results\r  quite a bit worse.  Using the techniques introduced in chapter 3\r  will greatly reduce the variation in performance across different\r  training runs for our networks..",
            "Of course, to obtain these accuracies I had to make specific choices\rfor the number of epochs of training, the mini-batch size, and the\rlearning rate, $\\eta$.  As I mentioned above, these are known as\rhyper-parameters for our neural network, in order to distinguish them\rfrom the parameters (weights and biases) learnt by our learning\ralgorithm.  If we choose our hyper-parameters poorly, we can get bad\rresults.  Suppose, for example, that we'd chosen the learning rate to\rbe $\\eta = 0.001$,",
            ">>>net=network.Network([784,100,10])>>>net.SGD(training_data,30,10,0.001,test_data=test_data)",
            "The results are much less encouraging,Epoch 0: 1139 / 10000\nEpoch 1: 1136 / 10000\nEpoch 2: 1135 / 10000\n...\nEpoch 27: 2101 / 10000\nEpoch 28: 2123 / 10000\nEpoch 29: 2142 / 10000However, you can see that the performance of the network is getting\rslowly better over time.  That suggests increasing the learning rate,\rsay to $\\eta = 0.01$.  If we do that, we get better results, which\rsuggests increasing the learning rate again.  (If making a change\rimproves things, try doing more!)  If we do that several times over,\rwe'll end up with a learning rate of something like $\\eta = 1.0$ (and\rperhaps fine tune to $3.0$), which is close to our earlier\rexperiments.  So even though we initially made a poor choice of\rhyper-parameters, we at least got enough information to help us\rimprove our choice of hyper-parameters.",
            "In general, debugging a neural network can be challenging.  This is\respecially true when the initial choice of hyper-parameters produces\rresults no better than random noise.  Suppose we try the successful 30\rhidden neuron network architecture from earlier, but with the learning\rrate changed to $\\eta = 100.0$:>>>net=network.Network([784,30,10])>>>net.SGD(training_data,30,10,100.0,test_data=test_data)At this point we've actually gone too far, and the learning rate is\rtoo high:Epoch0:1009/10000Epoch1:1009/10000Epoch2:1009/10000Epoch3:1009/10000...Epoch27:982/10000Epoch28:982/10000Epoch29:982/10000Now imagine that we were coming to this problem for the first time.\rOf course, weknowfrom our earlier experiments that the right\rthing to do is to decrease the learning rate.  But if we were coming\rto this problem for the first time then there wouldn't be much in the\routput to guide us on what to do.  We might worry not only about the\rlearning rate, but about every other aspect of our neural network.  We\rmight wonder if we've initialized the weights and biases in a way that\rmakes it hard for the network to learn?  Or maybe we don't have enough\rtraining data to get meaningful learning?  Perhaps we haven't run for\renough epochs?  Or maybe it's impossible for a neural network with\rthis architecture to learn to recognize handwritten digits?  Maybe the\rlearning rate is toolow?  Or, maybe, the learning rate is too\rhigh?  When you're coming to a problem for the first time, you're not\ralways sure.",
            "The lesson to take away from this is that debugging a neural network\ris not trivial, and, just as for ordinary programming, there is an art\rto it.  You need to learn that art of debugging in order to get good\rresults from neural networks.  More generally, we need to develop\rheuristics for choosing good hyper-parameters and a good architecture.\rWe'll discuss all these at length through the book, including how I\rchose the hyper-parameters above.",
            "Exercise",
            "Try creating a network with just two layers - an input and an\r  output layer, no hidden layer - with 784 and 10 neurons,\r  respectively.  Train the network using stochastic gradient descent.\r  What classification accuracy can you achieve?",
            "",
            "Earlier, I skipped over the details of how the MNIST data is loaded.\rIt's pretty straightforward.  For completeness, here's the code.  The\rdata structures used to store the MNIST data are described in the\rdocumentation strings - it's straightforward stuff, tuples and lists\rof Numpyndarrayobjects (think of them as vectors if you're\rnot familiar withndarrays):",
            "\"\"\"mnist_loader~~~~~~~~~~~~A library to load the MNIST image data.  For details of the datastructures that are returned, see the doc strings for ``load_data``and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is thefunction usually called by our neural network code.\"\"\"#### Libraries# Standard libraryimportcPickleimportgzip# Third-party librariesimportnumpyasnpdefload_data():\"\"\"Return the MNIST data as a tuple containing the training data,the validation data, and the test data.The ``training_data`` is returned as a tuple with two entries.The first entry contains the actual training images.  This is anumpy ndarray with 50,000 entries.  Each entry is, in turn, anumpy ndarray with 784 values, representing the 28 * 28 = 784pixels in a single MNIST image.The second entry in the ``training_data`` tuple is a numpy ndarraycontaining 50,000 entries.  Those entries are just the digitvalues (0...9) for the corresponding images contained in the firstentry of the tuple.The ``validation_data`` and ``test_data`` are similar, excepteach contains only 10,000 images.This is a nice data format, but for use in neural networks it'shelpful to modify the format of the ``training_data`` a little.That's done in the wrapper function ``load_data_wrapper()``, seebelow.\"\"\"f=gzip.open('../data/mnist.pkl.gz','rb')training_data,validation_data,test_data=cPickle.load(f)f.close()return(training_data,validation_data,test_data)defload_data_wrapper():\"\"\"Return a tuple containing ``(training_data, validation_data,test_data)``. Based on ``load_data``, but the format is moreconvenient for use in our implementation of neural networks.In particular, ``training_data`` is a list containing 50,0002-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarraycontaining the input image.  ``y`` is a 10-dimensionalnumpy.ndarray representing the unit vector corresponding to thecorrect digit for ``x``.``validation_data`` and ``test_data`` are lists containing 10,0002-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensionalnumpy.ndarry containing the input image, and ``y`` is thecorresponding classification, i.e., the digit values (integers)corresponding to ``x``.Obviously, this means we're using slightly different formats forthe training data and the validation / test data.  These formatsturn out to be the most convenient for use in our neural networkcode.\"\"\"tr_d,va_d,te_d=load_data()training_inputs=[np.reshape(x,(784,1))forxintr_d[0]]training_results=[vectorized_result(y)foryintr_d[1]]training_data=zip(training_inputs,training_results)validation_inputs=[np.reshape(x,(784,1))forxinva_d[0]]validation_data=zip(validation_inputs,va_d[1])test_inputs=[np.reshape(x,(784,1))forxinte_d[0]]test_data=zip(test_inputs,te_d[1])return(training_data,validation_data,test_data)defvectorized_result(j):\"\"\"Return a 10-dimensional unit vector with a 1.0 in the jthposition and zeroes elsewhere.  This is used to convert a digit(0...9) into a corresponding desired output from the neuralnetwork.\"\"\"e=np.zeros((10,1))e[j]=1.0returne",
            "I said above that our program gets pretty good results.  What does\rthat mean?  Good compared to what?  It's informative to have some\rsimple (non-neural-network) baseline tests to compare against, to\runderstand what it means to perform well.  The simplest baseline of\rall, of course, is to randomly guess the digit.  That'll be right\rabout ten percent of the time.  We're doing much better than that!",
            "What about a less trivial baseline?  Let's try an extremely simple\ridea: we'll look at howdarkan image is.  For instance, an\rimage of a $2$ will typically be quite a bit darker than an image of a\r$1$, just because more pixels are blackened out, as the following\rexamples illustrate:",
            "",
            "This suggests using the training data to compute average darknesses\rfor each digit, $0, 1, 2,\\ldots, 9$.  When presented with a new image,\rwe compute how dark the image is, and then guess that it's whichever\rdigit has the closest average darkness.  This is a simple procedure,\rand is easy to code up, so I won't explicitly write out the code -\rif you're interested it's in theGitHub\r  repository.  But it's a big improvement over random guessing,\rgetting $2,225$ of the $10,000$ test images correct, i.e., $22.25$\rpercent accuracy.",
            "",
            "It's not difficult to find other ideas which achieve accuracies in the\r$20$ to $50$ percent range.  If you work a bit harder you can get up\rover $50$ percent.  But to get much higher accuracies it helps to use\restablished machine learning algorithms.  Let's try using one of the\rbest known algorithms, thesupport vector\r  machineorSVM.  If you're not\rfamiliar with SVMs, not to worry, we're not going to need to\runderstand the details of how SVMs work.  Instead, we'll use a Python\rlibrary calledscikit-learn,\rwhich provides a simple Python interface to a fast C-based library for\rSVMs known asLIBSVM.",
            "If we run scikit-learn's SVM classifier using the default settings,\rthen it gets 9,435 of 10,000 test images correct.  (The code is\ravailablehere.)\rThat's a big improvement over our naive approach of classifying an\rimage based on how dark it is.  Indeed, it means that the SVM is\rperforming roughly as well as our neural networks, just a little\rworse.  In later chapters we'll introduce new techniques that enable\rus to improve our neural networks so that they perform much better\rthan the SVM.",
            "That's not the end of the story, however.  The 9,435 of 10,000 result\ris for scikit-learn's default settings for SVMs.  SVMs have a number\rof tunable parameters, and it's possible to search for parameters\rwhich improve this out-of-the-box performance.  I won't explicitly do\rthis search, but instead refer you tothis\r  blog postbyAndreas\r  Muellerif you'd like to know more.  Mueller shows that with some\rwork optimizing the SVM's parameters it's possible to get the\rperformance up above 98.5 percent accuracy.  In other words, a\rwell-tuned SVM only makes an error on about one digit in 70.  That's\rpretty good!  Can neural networks do better?",
            "In fact, they can.  At present, well-designed neural networks\routperform every other technique for solving MNIST, including SVMs.\rThe current (2013) record is classifying 9,979 of 10,000 images\rcorrectly.  This was done byLi\r  Wan,Matthew Zeiler, Sixin\rZhang,Yann LeCun, andRob Fergus.\rWe'll see most of the techniques they used later in the book.  At that\rlevel the performance is close to human-equivalent, and is arguably\rbetter, since quite a few of the MNIST images are difficult even for\rhumans to recognize with confidence, for example:",
            "",
            "I trust you'll agree that those are tough to classify!  With images\rlike these in the MNIST data set it's remarkable that neural networks\rcan accurately classify all but 21 of the 10,000 test images.\rUsually, when programming we believe that solving a complicated\rproblem like recognizing the MNIST digits requires a sophisticated\ralgorithm.  But even the neural networks in the Wanet alpaper\rjust mentioned involve quite simple algorithms, variations on the\ralgorithm we've seen in this chapter.  All the complexity is learned,\rautomatically, from the training data. In some sense, the moral of\rboth our results and those in more sophisticated papers, is that for\rsome problems:sophisticated algorithm $\\leq$ simple learning algorithm + good\r  training data.",
            "Toward deep learning"
        ],
        "Toward deep learning": [
            "While our neural network gives impressive performance, that\rperformance is somewhat mysterious.  The weights and biases in the\rnetwork were discovered automatically.  And that means we don't\rimmediately have an explanation of how the network does what it does.\rCan we find some way to understand the principles by which our network\ris classifying handwritten digits?  And, given such principles, can we\rdo better?",
            "To put these questions more starkly, suppose that a few decades hence\rneural networks lead to artificial intelligence (AI).  Will we\runderstand how such intelligent networks work?  Perhaps the networks\rwill be opaque to us, with weights and biases we don't understand,\rbecause they've been learned automatically.  In the early days of AI\rresearch people hoped that the effort to build an AI would also help\rus understand the principles behind intelligence and, maybe, the\rfunctioning of the human brain.  But perhaps the outcome will be that\rwe end up understanding neither the brain nor how artificial\rintelligence works!",
            "To address these questions, let's think back to the interpretation of\rartificial neurons that I gave at the start of the chapter, as a means\rof weighing evidence.  Suppose we want to determine whether an image\rshows a human face or not:",
            "",
            "Credits: 1.Ester Inbar. 2.\r  Unknown. 3. NASA, ESA, G. Illingworth, D. Magee, and P. Oesch\r  (University of California, Santa Cruz), R. Bouwens (Leiden\r  University), and the HUDF09 Team.  Click on the images for more\r  details.",
            "",
            "We could attack this problem the same way we attacked handwriting\rrecognition - by using the pixels in the image as input to a neural\rnetwork, with the output from the network a single neuron indicating\reither \"Yes, it's a face\" or \"No, it's not a face\".",
            "Let's suppose we do this, but that we're not using a learning\ralgorithm.  Instead, we're going to try to design a network by hand,\rchoosing appropriate weights and biases.  How might we go about it?\rForgetting neural networks entirely for the moment, a heuristic we\rcould use is to decompose the problem into sub-problems: does the\rimage have an eye in the top left?  Does it have an eye in the top\rright?  Does it have a nose in the middle?  Does it have a mouth in\rthe bottom middle?  Is there hair on top?  And so on.",
            "If the answers to several of these questions are \"yes\", or even just\r\"probably yes\", then we'd conclude that the image is likely to be a\rface.  Conversely, if the answers to most of the questions are \"no\",\rthen the image probably isn't a face.",
            "Of course, this is just a rough heuristic, and it suffers from many\rdeficiencies.  Maybe the person is bald, so they have no hair.  Maybe\rwe can only see part of the face, or the face is at an angle, so some\rof the facial features are obscured.  Still, the heuristic suggests\rthat if we can solve the sub-problems using neural networks, then\rperhaps we can build a neural network for face-detection, by combining\rthe networks for the sub-problems.  Here's a possible architecture,\rwith rectangles denoting the sub-networks.  Note that this isn't\rintended as a realistic approach to solving the face-detection\rproblem; rather, it's to help us build intuition about how networks\rfunction.  Here's the architecture:",
            "",
            "It's also plausible that the sub-networks can be decomposed.  Suppose\rwe're considering the question: \"Is there an eye in the top left?\"\rThis can be decomposed into questions such as: \"Is there an\reyebrow?\"; \"Are there eyelashes?\"; \"Is there an iris?\"; and so\ron.  Of course, these questions should really include positional\rinformation, as well - \"Is the eyebrow in the top left, and above\rthe iris?\", that kind of thing - but let's keep it simple.  The\rnetwork to answer the question \"Is there an eye in the top left?\"\rcan now be decomposed:",
            "",
            "Those questions too can be broken down, further and further through\rmultiple layers.  Ultimately, we'll be working with sub-networks that\ranswer questions so simple they can easily be answered at the level of\rsingle pixels.  Those questions might, for example, be about the\rpresence or absence of very simple shapes at particular points in the\rimage.  Such questions can be answered by single neurons connected to\rthe raw pixels in the image.",
            "The end result is a network which breaks down a very complicated\rquestion - does this image show a face or not - into very simple\rquestions answerable at the level of single pixels.  It does this\rthrough a series of many layers, with early layers answering very\rsimple and specific questions about the input image, and later layers\rbuilding up a hierarchy of ever more complex and abstract concepts.\rNetworks with this kind of many-layer structure - two or more hidden\rlayers - are calleddeep neural networks.",
            "",
            "",
            "Of course, I haven't said how to do this recursive decomposition into\rsub-networks.  It certainly isn't practical to hand-design the weights\rand biases in the network.  Instead, we'd like to use learning\ralgorithms so that the network can automatically learn the weights and\rbiases - and thus, the hierarchy of concepts - from training data.\rResearchers in the 1980s and 1990s tried using stochastic gradient\rdescent and backpropagation to train deep networks.  Unfortunately,\rexcept for a few special architectures, they didn't have much luck.\rThe networks would learn, but very slowly, and in practice often too\rslowly to be useful.",
            "Since 2006, a set of techniques has been developed that enable\rlearning in deep neural nets.  These deep learning techniques are\rbased on stochastic gradient descent and backpropagation, but also\rintroduce new ideas.  These techniques have enabled much deeper (and\rlarger) networks to be trained - people now routinely train networks\rwith 5 to 10 hidden layers.  And, it turns out that these perform far\rbetter on many problems than shallow neural networks, i.e., networks\rwith just a single hidden layer.  The reason, of course, is the\rability of deep nets to build up a complex hierarchy of concepts.\rIt's a bit like the way conventional programming languages use modular\rdesign and ideas about abstraction to enable the creation of complex\rcomputer programs.  Comparing a deep network to a shallow network is a\rbit like comparing a programming language with the ability to make\rfunction calls to a stripped down language with no ability to make\rsuch calls.  Abstraction takes a different form in neural networks\rthan it does in conventional programming, but it's just as important.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ]
    },
    "CHAPTER 2": {
        "Warm up: a fast matrix-based approach to computing the output\r  from a neural network": [
            "Before discussing backpropagation, let's warm up with a fast\rmatrix-based algorithm to compute the output from a neural network.\rWe actually already briefly saw this algorithmnear\r  the end of the last chapter, but I described it quickly, so it's\rworth revisiting in detail.  In particular, this is a good way of\rgetting comfortable with the notation used in backpropagation, in a\rfamiliar context.",
            "Let's begin with a notation which lets us refer to weights in the\rnetwork in an unambiguous way.  We'll use $w^l_{jk}$ to denote the\rweight for the connection from the $k^{\\rm th}$ neuron in the\r$(l-1)^{\\rm th}$ layer to the $j^{\\rm th}$ neuron in the $l^{\\rm th}$\rlayer.  So, for example, the diagram below shows the weight on a\rconnection from the fourth neuron in the second layer to the second\rneuron in the third layer of a network:This notation is cumbersome at first, and it does take some work to\rmaster.  But with a little effort you'll find the notation becomes\reasy and natural.  One quirk of the notation is the ordering of the\r$j$ and $k$ indices.  You might think that it makes more sense to use\r$j$ to refer to the input neuron, and $k$ to the output neuron, not\rvice versa, as is actually done.  I'll explain the reason for this\rquirk below.",
            "We use a similar notation for the network's biases and activations.\rExplicitly, we use $b^l_j$ for the bias of the $j^{\\rm th}$ neuron in\rthe $l^{\\rm th}$ layer.  And we use $a^l_j$ for the activation of the\r$j^{\\rm th}$ neuron in the $l^{\\rm th}$ layer.  The following diagram\rshows examples of these notations in use:With these notations, the activation $a^{l}_j$ of the $j^{\\rm th}$\rneuron in the $l^{\\rm th}$ layer is related to the activations in the\r$(l-1)^{\\rm th}$ layer by the equation (compare\rEquation(4)\\begin{eqnarray} \r  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)} \\nonumber\\end{eqnarray}and surrounding\rdiscussion in the last chapter)\\begin{eqnarray} \r  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\r\\tag{23}\\end{eqnarray}\rwhere the sum is over all neurons $k$ in the $(l-1)^{\\rm th}$ layer.  To\rrewrite this expression in a matrix form we define aweight\r  matrix$w^l$ for each layer, $l$.  The entries of the weight matrix\r$w^l$ are just the weights connecting to the $l^{\\rm th}$ layer of neurons,\rthat is, the entry in the $j^{\\rm th}$ row and $k^{\\rm th}$ column is $w^l_{jk}$.\rSimilarly, for each layer $l$ we define abias vector, $b^l$.\rYou can probably guess how this works - the components of the bias\rvector are just the values $b^l_j$, one component for each neuron in\rthe $l^{\\rm th}$ layer.  And finally, we define an activation vector $a^l$\rwhose components are the activations $a^l_j$.",
            "The last ingredient we need to rewrite(23)\\begin{eqnarray} \r  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right) \\nonumber\\end{eqnarray}in a\rmatrix form is the idea of vectorizing a function such as $\\sigma$.\rWe met vectorization briefly in the last chapter, but to recap, the\ridea is that we want to apply a function such as $\\sigma$ to every\relement in a vector $v$.  We use the obvious notation $\\sigma(v)$ to\rdenote this kind of elementwise application of a function.  That is,\rthe components of $\\sigma(v)$ are just $\\sigma(v)_j = \\sigma(v_j)$.\rAs an example, if we have the function $f(x) = x^2$ then the\rvectorized form of $f$ has the effect\\begin{eqnarray}\r  f\\left(\\left[ \\begin{array}{c} 2 \\\\ 3 \\end{array} \\right] \\right)\r  = \\left[ \\begin{array}{c} f(2) \\\\ f(3) \\end{array} \\right]\r  = \\left[ \\begin{array}{c} 4 \\\\ 9 \\end{array} \\right],\r\\tag{24}\\end{eqnarray}\rthat is, the vectorized $f$ just squares every element of the vector.",
            "With these notations in mind, Equation(23)\\begin{eqnarray} \r  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right) \\nonumber\\end{eqnarray}can\rbe rewritten in the beautiful and compact vectorized form\\begin{eqnarray} \r  a^{l} = \\sigma(w^l a^{l-1}+b^l).\r\\tag{25}\\end{eqnarray}\rThis expression gives us a much more global way of thinking about how\rthe activations in one layer relate to activations in the previous\rlayer: we just apply the weight matrix to the activations, then add\rthe bias vector, and finally apply the $\\sigma$ function**By the way, it's this expression that\r  motivates the quirk in the $w^l_{jk}$ notation mentioned earlier.\r  If we used $j$ to index the input neuron, and $k$ to index the\r  output neuron, then we'd need to replace the weight matrix in\r  Equation(25)\\begin{eqnarray} \r  a^{l} = \\sigma(w^l a^{l-1}+b^l) \\nonumber\\end{eqnarray}by the transpose of the\r  weight matrix.  That's a small change, but annoying, and we'd lose\r  the easy simplicity of saying (and thinking) \"apply the weight\r  matrix to the activations\"..  That global view is often easier and\rmore succinct (and involves fewer indices!) than the neuron-by-neuron\rview we've taken to now.  Think of it as a way of escaping index hell,\rwhile remaining precise about what's going on.  The expression is also\ruseful in practice, because most matrix libraries provide fast ways of\rimplementing matrix multiplication, vector addition, and\rvectorization.  Indeed, thecodein the last chapter made implicit use of this expression to compute\rthe behaviour of the network.",
            "When using Equation(25)\\begin{eqnarray} \r  a^{l} = \\sigma(w^l a^{l-1}+b^l) \\nonumber\\end{eqnarray}to compute $a^l$,\rwe compute the intermediate quantity $z^l \\equiv w^l a^{l-1}+b^l$\ralong the way.  This quantity turns out to be useful enough to be\rworth naming: we call $z^l$ theweighted inputto the neurons\rin layer $l$.  We'll make considerable use of the weighted input $z^l$\rlater in the chapter.  Equation(25)\\begin{eqnarray} \r  a^{l} = \\sigma(w^l a^{l-1}+b^l) \\nonumber\\end{eqnarray}is\rsometimes written in terms of the weighted input, as $a^l =\r\\sigma(z^l)$.  It's also worth noting that $z^l$ has components $z^l_j\r= \\sum_k w^l_{jk} a^{l-1}_k+b^l_j$, that is, $z^l_j$ is just the\rweighted input to the activation function for neuron $j$ in layer $l$.",
            "The two assumptions we need about the cost function"
        ],
        "The two assumptions we need about the cost function": [
            "The goal of backpropagation is to compute the partial derivatives\r$\\partial C / \\partial w$ and $\\partial C / \\partial b$ of the cost\rfunction $C$ with respect to any weight $w$ or bias $b$ in the\rnetwork.  For backpropagation to work we need to make two main\rassumptions about the form of the cost function.  Before stating those\rassumptions, though, it's useful to have an example cost function in\rmind.  We'll use the quadratic cost function from last chapter\r(c.f. Equation(6)\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 \\nonumber\\end{eqnarray}).  In the notation of\rthe last section, the quadratic cost has the form\\begin{eqnarray}\r  C = \\frac{1}{2n} \\sum_x \\|y(x)-a^L(x)\\|^2,\r\\tag{26}\\end{eqnarray}\rwhere: $n$ is the total number of training examples; the sum is over\rindividual training examples, $x$; $y = y(x)$ is the corresponding\rdesired output; $L$ denotes the number of layers in the network; and\r$a^L = a^L(x)$ is the vector of activations output from the network\rwhen $x$ is input.",
            "Okay, so what assumptions do we need to make about our cost function,\r$C$, in order that backpropagation can be applied?  The first\rassumption we need is that the cost function can be written as an\raverage $C = \\frac{1}{n} \\sum_x C_x$ over cost functions $C_x$ for\rindividual training examples, $x$.  This is the case for the quadratic\rcost function, where the cost for a single training example is $C_x =\r\\frac{1}{2} \\|y-a^L \\|^2$.  This assumption will also hold true for\rall the other cost functions we'll meet in this book.",
            "The reason we need this assumption is because what backpropagation\ractually lets us do is compute the partial derivatives $\\partial C_x\r/ \\partial w$ and $\\partial C_x / \\partial b$ for a single training\rexample.  We then recover $\\partial C / \\partial w$ and $\\partial C\r/ \\partial b$ by averaging over training examples.  In fact, with this\rassumption in mind, we'll suppose the training example $x$ has been\rfixed, and drop the $x$ subscript, writing the cost $C_x$ as $C$.\rWe'll eventually put the $x$ back in, but for now it's a notational\rnuisance that is better left implicit.",
            "The second assumption we make about the cost is that it can be written\ras a function of the outputs from the neural network:For example, the quadratic cost function satisfies this requirement,\rsince the quadratic cost for a single training example $x$ may be\rwritten as\\begin{eqnarray}\r  C = \\frac{1}{2} \\|y-a^L\\|^2 = \\frac{1}{2} \\sum_j (y_j-a^L_j)^2,\r\\tag{27}\\end{eqnarray}\rand thus is a function of the output activations.  Of course, this\rcost function also depends on the desired output $y$, and you may\rwonder why we're not regarding the cost also as a function of $y$.\rRemember, though, that the input training example $x$ is fixed, and so\rthe output $y$ is also a fixed parameter.  In particular, it's not\rsomething we can modify by changing the weights and biases in any way,\ri.e., it's not something which the neural network learns.  And so it\rmakes sense to regard $C$ as a function of the output activations\r$a^L$ alone, with $y$ merely a parameter that helps define that\rfunction.",
            "",
            "",
            "",
            "The Hadamard product, $s \\odot t$"
        ],
        "The Hadamard product, $s \\odot t$": [
            "The backpropagation algorithm is based on common linear algebraic\roperations - things like vector addition, multiplying a vector by a\rmatrix, and so on.  But one of the operations is a little less\rcommonly used.  In particular, suppose $s$ and $t$ are two vectors of\rthe same dimension.  Then we use $s \\odot t$ to denote theelementwiseproduct of the two vectors.  Thus the components of\r$s \\odot t$ are just $(s \\odot t)_j = s_j t_j$.  As an example,\\begin{eqnarray}\r\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \r  \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]\r= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]\r= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\r\\tag{28}\\end{eqnarray}\rThis kind of elementwise multiplication is sometimes called theHadamard productorSchur product.  We'll refer to it as\rthe Hadamard product.  Good matrix libraries usually provide fast\rimplementations of the Hadamard product, and that comes in handy when\rimplementing backpropagation.",
            "The four fundamental equations behind backpropagation"
        ],
        "The four fundamental equations behind backpropagation": [
            "Backpropagation is about understanding how changing the weights and\rbiases in a network changes the cost function.  Ultimately, this means\rcomputing the partial derivatives $\\partial C / \\partial w^l_{jk}$ and\r$\\partial C / \\partial b^l_j$.  But to compute those, we first\rintroduce an intermediate quantity, $\\delta^l_j$, which we call theerrorin the $j^{\\rm th}$ neuron in the $l^{\\rm th}$ layer.\rBackpropagation will give us a procedure to compute the error\r$\\delta^l_j$, and then will relate $\\delta^l_j$ to $\\partial C\r/ \\partial w^l_{jk}$ and $\\partial C / \\partial b^l_j$.",
            "To understand how the error is defined, imagine there is a demon in\rour neural network:The demon sits at the $j^{\\rm th}$ neuron in layer $l$.  As the input to the\rneuron comes in, the demon messes with the neuron's operation.  It\radds a little change $\\Delta z^l_j$ to the neuron's weighted input, so\rthat instead of outputting $\\sigma(z^l_j)$, the neuron instead outputs\r$\\sigma(z^l_j+\\Delta z^l_j)$.  This change propagates through later\rlayers in the network, finally causing the overall cost to change by\ran amount $\\frac{\\partial C}{\\partial z^l_j} \\Delta z^l_j$.",
            "Now, this demon is a good demon, and is trying to help you improve the\rcost, i.e., they're trying to find a $\\Delta z^l_j$ which makes the\rcost smaller.  Suppose $\\frac{\\partial C}{\\partial z^l_j}$ has a large\rvalue (either positive or negative).  Then the demon can lower the\rcost quite a bit by choosing $\\Delta z^l_j$ to have the opposite sign\rto $\\frac{\\partial C}{\\partial z^l_j}$.  By contrast, if\r$\\frac{\\partial C}{\\partial z^l_j}$ is close to zero, then the demon\rcan't improve the cost much at all by perturbing the weighted input\r$z^l_j$.  So far as the demon can tell, the neuron is already pretty\rnear optimal**This is only the case for small changes $\\Delta\r  z^l_j$, of course. We'll assume that the demon is constrained to\r  make such small changes..  And so there's a heuristic sense in\rwhich $\\frac{\\partial C}{\\partial z^l_j}$ is a measure of the error in\rthe neuron.",
            "Motivated by this story, we define the error $\\delta^l_j$ of neuron\r$j$ in layer $l$ by\\begin{eqnarray} \r  \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}.\r\\tag{29}\\end{eqnarray}\rAs per our usual conventions, we use $\\delta^l$ to denote the vector\rof errors associated with layer $l$.  Backpropagation will give us a\rway of computing $\\delta^l$ for every layer, and then relating those\rerrors to the quantities of real interest, $\\partial C / \\partial\rw^l_{jk}$ and $\\partial C / \\partial b^l_j$.",
            "You might wonder why the demon is changing the weighted input $z^l_j$.\rSurely it'd be more natural to imagine the demon changing the output\ractivation $a^l_j$, with the result that we'd be using $\\frac{\\partial\r  C}{\\partial a^l_j}$ as our measure of error.  In fact, if you do\rthis things work out quite similarly to the discussion below.  But it\rturns out to make the presentation of backpropagation a little more\ralgebraically complicated.  So we'll stick with $\\delta^l_j =\r\\frac{\\partial C}{\\partial z^l_j}$ as our measure of error**In\r  classification problems like MNIST the term \"error\" is sometimes\r  used to mean the classification failure rate.  E.g., if the neural\r  net correctly classifies 96.0 percent of the digits, then the error\r  is 4.0 percent.  Obviously, this has quite a different meaning from\r  our $\\delta$ vectors.  In practice, you shouldn't have trouble\r  telling which meaning is intended in any given usage..",
            "Plan of attack:Backpropagation is based around four\rfundamental equations.  Together, those equations give us a way of\rcomputing both the error $\\delta^l$ and the gradient of the cost\rfunction.  I state the four equations below.  Be warned, though: you\rshouldn't expect to instantaneously assimilate the equations.  Such an\rexpectation will lead to disappointment.  In fact, the backpropagation\requations are so rich that understanding them well requires\rconsiderable time and patience as you gradually delve deeper into the\requations.  The good news is that such patience is repaid many times\rover.  And so the discussion in this section is merely a beginning,\rhelping you on the way to a thorough understanding of the equations.",
            "Here's a preview of the ways we'll delve more deeply into the\requations later in the chapter: I'llgive\r  a short proof of the equations, which helps explain why they are\rtrue; we'llrestate\r  the equationsin algorithmic form as pseudocode, andsee howthe\rpseudocode can be implemented as real, running Python code; and, inthe final\r  section of the chapter, we'll develop an intuitive picture of what\rthe backpropagation equations mean, and how someone might discover\rthem from scratch.  Along the way we'll return repeatedly to the four\rfundamental equations, and as you deepen your understanding those\requations will come to seem comfortable and, perhaps, even beautiful\rand natural.",
            "An equation for the error in the output layer, $\\delta^L$:The components of $\\delta^L$ are given by\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\r\\tag{BP1}\\end{eqnarray}\rThis is a very natural expression.  The first term on the right,\r$\\partial C / \\partial a^L_j$, just measures how fast the cost is\rchanging as a function of the $j^{\\rm th}$ output activation.  If, for\rexample, $C$ doesn't depend much on a particular output neuron, $j$,\rthen $\\delta^L_j$ will be small, which is what we'd expect.  The\rsecond term on the right, $\\sigma'(z^L_j)$, measures how fast the\ractivation function $\\sigma$ is changing at $z^L_j$.",
            "Notice that everything in(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}is easily computed.  In\rparticular, we compute $z^L_j$ while computing the behaviour of the\rnetwork, and it's only a small additional overhead to compute\r$\\sigma'(z^L_j)$.  The exact form of $\\partial C / \\partial a^L_j$\rwill, of course, depend on the form of the cost function.  However,\rprovided the cost function is known there should be little trouble\rcomputing $\\partial C / \\partial a^L_j$.  For example, if we're using\rthe quadratic cost function then $C = \\frac{1}{2} \\sum_j\r(y_j-a^L_j)^2$, and so $\\partial C / \\partial a^L_j = (a_j^L-y_j)$,\rwhich obviously is easily computable.",
            "Equation(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}is a componentwise expression for $\\delta^L$.\rIt's a perfectly good expression, but not the matrix-based form we\rwant for backpropagation. However, it's easy to rewrite the equation\rin a matrix-based form, as\\begin{eqnarray} \r  \\delta^L = \\nabla_a C \\odot \\sigma'(z^L).\r\\tag{BP1a}\\end{eqnarray}\rHere, $\\nabla_a C$ is defined to be a vector whose components are the\rpartial derivatives $\\partial C / \\partial a^L_j$.  You can think of\r$\\nabla_a C$ as expressing the rate of change of $C$ with respect to\rthe output activations.  It's easy to see that Equations(BP1a)\\begin{eqnarray} \r  \\delta^L = \\nabla_a C \\odot \\sigma'(z^L) \\nonumber\\end{eqnarray}and(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}are equivalent, and for that reason from now on we'll\ruse(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}interchangeably to refer to both equations.  As an\rexample, in the case of the quadratic cost we have $\\nabla_a C =\r(a^L-y)$, and so the fully matrix-based form of(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}becomes\\begin{eqnarray} \r  \\delta^L = (a^L-y) \\odot \\sigma'(z^L).\r\\tag{30}\\end{eqnarray}\rAs you can see, everything in this expression has a nice vector form,\rand is easily computed using a library such as Numpy.",
            "An equation for the error $\\delta^l$ in terms of the error in\r  the next layer, $\\delta^{l+1}$:In particular\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\r\\tag{BP2}\\end{eqnarray}\rwhere $(w^{l+1})^T$ is the transpose of the weight matrix $w^{l+1}$ for\rthe $(l+1)^{\\rm th}$ layer.  This equation appears complicated, but\reach element has a nice interpretation.  Suppose we know the error\r$\\delta^{l+1}$ at the $l+1^{\\rm th}$ layer.  When we apply the\rtranspose weight matrix, $(w^{l+1})^T$, we can think intuitively of\rthis as moving the errorbackwardthrough the network, giving\rus some sort of measure of the error at the output of the $l^{\\rm th}$\rlayer.  We then take the Hadamard product $\\odot \\sigma'(z^l)$.  This\rmoves the error backward through the activation function in layer $l$,\rgiving us the error $\\delta^l$ in the weighted input to layer $l$.",
            "By combining(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}with(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}we can compute the error\r$\\delta^l$ for any layer in the network.  We start by\rusing(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}to compute $\\delta^L$, then apply\rEquation(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}to compute $\\delta^{L-1}$, then\rEquation(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}again to compute $\\delta^{L-2}$, and so on, all\rthe way back through the network.",
            "An equation for the rate of change of the cost with respect to\r  any bias in the network:In particular:\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\r  \\delta^l_j.\r\\tag{BP3}\\end{eqnarray}\rThat is, the error $\\delta^l_j$ isexactly equalto the rate of\rchange $\\partial C / \\partial b^l_j$.  This is great news, since(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}and(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}have already told us how to compute\r$\\delta^l_j$.  We can rewrite(BP3)\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\r  \\delta^l_j \\nonumber\\end{eqnarray}in shorthand as\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial b} = \\delta,\r\\tag{31}\\end{eqnarray}\rwhere it is understood that $\\delta$ is being evaluated at the same\rneuron as the bias $b$.",
            "An equation for the rate of change of the cost with respect to\r  any weight in the network:In particular:\\begin{eqnarray}\r  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\r\\tag{BP4}\\end{eqnarray}\rThis tells us how to compute the partial derivatives $\\partial C\r/ \\partial w^l_{jk}$ in terms of the quantities $\\delta^l$ and\r$a^{l-1}$, which we already know how to compute.  The equation can be\rrewritten in a less index-heavy notation as\\begin{eqnarray}  \\frac{\\partial\r    C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out},\r\\tag{32}\\end{eqnarray}\rwhere it's understood that $a_{\\rm in}$ is the activation of the\rneuron input to the weight $w$, and $\\delta_{\\rm out}$ is the error of\rthe neuron output from the weight $w$.  Zooming in to look at just the\rweight $w$, and the two neurons connected by that weight, we can\rdepict this as:A nice consequence of Equation(32)\\begin{eqnarray}  \\frac{\\partial\r    C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out} \\nonumber\\end{eqnarray}is\rthat when the activation $a_{\\rm in}$ is small, $a_{\\rm in} \\approx\r0$, the gradient term $\\partial C / \\partial w$ will also tend to be\rsmall.  In this case, we'll say the weightlearns slowly,\rmeaning that it's not changing much during gradient descent.  In other\rwords, one consequence of(BP4)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\nonumber\\end{eqnarray}is that weights output from\rlow-activation neurons learn slowly.",
            "",
            "There are other insights along these lines which can be obtained\rfrom(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}-(BP4)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\nonumber\\end{eqnarray}.  Let's start by looking at the output\rlayer.  Consider the term $\\sigma'(z^L_j)$ in(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}.  Recall\rfrom thegraph of the sigmoid\r  function in the last chapterthat the $\\sigma$ function becomes\rvery flat when $\\sigma(z^L_j)$ is approximately $0$ or $1$.  When this\roccurs we will have $\\sigma'(z^L_j) \\approx 0$.  And so the lesson is\rthat a weight in the final layer will learn slowly if the output\rneuron is either low activation ($\\approx 0$) or high activation\r($\\approx 1$).  In this case it's common to say the output neuron hassaturatedand, as a result, the weight has stopped learning (or\ris learning slowly).  Similar remarks hold also for the biases of\routput neuron.",
            "We can obtain similar insights for earlier layers.  In particular,\rnote the $\\sigma'(z^l)$ term in(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}.  This means that\r$\\delta^l_j$ is likely to get small if the neuron is near saturation.\rAnd this, in turn, means that any weights input to a saturated neuron\rwill learn slowly**This reasoning won't hold if ${w^{l+1}}^T\r  \\delta^{l+1}$ has large enough entries to compensate for the\r  smallness of $\\sigma'(z^l_j)$.  But I'm speaking of the general\r  tendency..",
            "Summing up, we've learnt that a weight will learn slowly if either the\rinput neuron is low-activation, or if the output neuron has saturated,\ri.e., is either high- or low-activation.",
            "None of these observations is too greatly surprising.  Still, they\rhelp improve our mental model of what's going on as a neural network\rlearns.  Furthermore, we can turn this type of reasoning around.  The\rfour fundamental equations turn out to hold for any activation\rfunction, not just the standard sigmoid function (that's because, as\rwe'll see in a moment, the proofs don't use any special properties of\r$\\sigma$).  And so we can use these equations todesignactivation functions which have particular desired learning\rproperties.  As an example to give you the idea, suppose we were to\rchoose a (non-sigmoid) activation function $\\sigma$ so that $\\sigma'$\ris always positive, and never gets close to zero.  That would prevent\rthe slow-down of learning that occurs when ordinary sigmoid neurons\rsaturate.  Later in the book we'll see examples where this kind of\rmodification is made to the activation function.  Keeping the four\requations(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}-(BP4)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\nonumber\\end{eqnarray}in mind can help explain why such\rmodifications are tried, and what impact they can have.",
            "",
            "",
            "",
            "ProblemAlternate presentation of the equations of backpropagation:I've stated the equations of backpropagation (notably(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}and(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}) using the Hadamard product.  This presentation may\r  be disconcerting if you're unused to the Hadamard product.  There's\r  an alternative approach, based on conventional matrix\r  multiplication, which some readers may find enlightening.  (1) Show\r  that(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}may be rewritten as\\begin{eqnarray}\r    \\delta^L = \\Sigma'(z^L) \\nabla_a C,\r  \\tag{33}\\end{eqnarray}\r  where $\\Sigma'(z^L)$ is a square matrix whose diagonal entries are\r  the values $\\sigma'(z^L_j)$, and whose off-diagonal entries are\r  zero.  Note that this matrix acts on $\\nabla_a C$ by conventional\r  matrix multiplication.  (2) Show that(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}may be rewritten\r  as\\begin{eqnarray}\r    \\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\delta^{l+1}.\r  \\tag{34}\\end{eqnarray}\r  (3) By combining observations (1) and (2) show that\\begin{eqnarray}\r    \\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\ldots \\Sigma'(z^{L-1}) (w^L)^T \r    \\Sigma'(z^L) \\nabla_a C\r  \\tag{35}\\end{eqnarray}\r  For readers comfortable with matrix multiplication this equation may\r  be easier to understand than(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}and(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}.  The\r  reason I've focused on(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}and(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}is because that\r  approach turns out to be faster to implement numerically.",
            "Proof of the four fundamental equations (optional)"
        ],
        "Proof of the four fundamental equations (optional)": [
            "We'll now prove the four fundamental\requations(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}-(BP4)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\nonumber\\end{eqnarray}.  All four are consequences of the\rchain rule from multivariable calculus.  If you're comfortable with\rthe chain rule, then I strongly encourage you to attempt the\rderivation yourself before reading on.",
            "Let's begin with Equation(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}, which gives an expression for\rthe output error, $\\delta^L$.  To prove this equation, recall that by\rdefinition\\begin{eqnarray}\r  \\delta^L_j = \\frac{\\partial C}{\\partial z^L_j}.\r\\tag{36}\\end{eqnarray}\rApplying the chain rule, we can re-express the partial derivative\rabove in terms of partial derivatives with respect to the output\ractivations,\\begin{eqnarray}\r  \\delta^L_j = \\sum_k \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_j},\r\\tag{37}\\end{eqnarray}\rwhere the sum is over all neurons $k$ in the output layer.  Of course,\rthe output activation $a^L_k$ of the $k^{\\rm th}$ neuron depends only\ron the weighted input $z^L_j$ for the $j^{\\rm th}$ neuron when $k =\rj$.  And so $\\partial a^L_k / \\partial z^L_j$ vanishes when $k \\neq\rj$.  As a result we can simplify the previous equation to\\begin{eqnarray}\r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\frac{\\partial a^L_j}{\\partial z^L_j}.\r\\tag{38}\\end{eqnarray}\rRecalling that $a^L_j = \\sigma(z^L_j)$ the second term on the right\rcan be written as $\\sigma'(z^L_j)$, and the equation becomes\\begin{eqnarray}\r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j),\r\\tag{39}\\end{eqnarray}\rwhich is just(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}, in component form.",
            "Next, we'll prove(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}, which gives an equation for the error\r$\\delta^l$ in terms of the error in the next layer, $\\delta^{l+1}$.\rTo do this, we want to rewrite $\\delta^l_j = \\partial C / \\partial\rz^l_j$ in terms of $\\delta^{l+1}_k = \\partial C / \\partial z^{l+1}_k$.\rWe can do this using the chain rule,\\begin{eqnarray}\r  \\delta^l_j & = & \\frac{\\partial C}{\\partial z^l_j} \\tag{40}\\\\\r  & = & \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k} \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\tag{41}\\\\ \r  & = & \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\delta^{l+1}_k,\r\\tag{42}\\end{eqnarray}\rwhere in the last line we have interchanged the two terms on the\rright-hand side, and substituted the definition of $\\delta^{l+1}_k$.\rTo evaluate the first term on the last line, note that\\begin{eqnarray}\r  z^{l+1}_k = \\sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \\sum_j w^{l+1}_{kj} \\sigma(z^l_j) +b^{l+1}_k.\r\\tag{43}\\end{eqnarray}\rDifferentiating, we obtain\\begin{eqnarray}\r  \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj} \\sigma'(z^l_j).\r\\tag{44}\\end{eqnarray}\rSubstituting back into(42)\\begin{eqnarray} \r  & = & \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\delta^{l+1}_k \\nonumber\\end{eqnarray}we obtain\\begin{eqnarray}\r  \\delta^l_j = \\sum_k w^{l+1}_{kj}  \\delta^{l+1}_k \\sigma'(z^l_j).\r\\tag{45}\\end{eqnarray}\rThis is just(BP2)\\begin{eqnarray} \r  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\nonumber\\end{eqnarray}written in component form.",
            "The final two equations we want to prove are(BP3)\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\r  \\delta^l_j \\nonumber\\end{eqnarray}and(BP4)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\nonumber\\end{eqnarray}.  These also follow from the chain rule, in a manner\rsimilar to the proofs of the two equations above.  I leave them to you\ras an exercise.",
            "ExerciseProve Equations(BP3)\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\r  \\delta^l_j \\nonumber\\end{eqnarray}and(BP4)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\nonumber\\end{eqnarray}.",
            "That completes the proof of the four fundamental equations of\rbackpropagation.  The proof may seem complicated.  But it's really\rjust the outcome of carefully applying the chain rule.  A little less\rsuccinctly, we can think of backpropagation as a way of computing the\rgradient of the cost function by systematically applying the chain\rrule from multi-variable calculus.  That's all there really is to\rbackpropagation - the rest is details.",
            "The backpropagation algorithm"
        ],
        "The backpropagation algorithm": [
            "The backpropagation equations provide us with a way of computing the\rgradient of the cost function.  Let's explicitly write this out in the\rform of an algorithm:Input $x$:Set the corresponding activation $a^{1}$ for\r  the input layer.",
            "Feedforward:For each $l = 2, 3, \\ldots, L$ compute\r  $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \\sigma(z^{l})$.",
            "Output error $\\delta^L$:Compute the vector $\\delta^{L}\r  = \\nabla_a C \\odot \\sigma'(z^L)$.",
            "Backpropagate the error:For each $l = L-1, L-2,\r  \\ldots, 2$ compute $\\delta^{l} = ((w^{l+1})^T \\delta^{l+1}) \\odot\r  \\sigma'(z^{l})$.",
            "Output:The gradient of the cost function is given by\r  $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$ and\r  $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$.",
            "Examining the algorithm you can see why it's calledbackpropagation.  We compute the error vectors $\\delta^l$\rbackward, starting from the final layer.  It may seem peculiar that\rwe're going through the network backward.  But if you think about the\rproof of backpropagation, the backward movement is a consequence of\rthe fact that the cost is a function of outputs from the network.  To\runderstand how the cost varies with earlier weights and biases we need\rto repeatedly apply the chain rule, working backward through the\rlayers to obtain usable expressions.",
            "ExercisesBackpropagation with a single modified neuronSuppose we modify\r  a single neuron in a feedforward network so that the output from the\r  neuron is given by $f(\\sum_j w_j x_j + b)$, where $f$ is some\r  function other than the sigmoid.  How should we modify the\r  backpropagation algorithm in this case?",
            "Backpropagation with linear neuronsSuppose we replace the\r  usual non-linear $\\sigma$ function with $\\sigma(z) = z$ throughout\r  the network.  Rewrite the backpropagation algorithm for this case.",
            "As I've described it above, the backpropagation algorithm computes the\rgradient of the cost function for a single training example, $C =\rC_x$.  In practice, it's common to combine backpropagation with a\rlearning algorithm such as stochastic gradient descent, in which we\rcompute the gradient for many training examples.  In particular, given\ra mini-batch of $m$ training examples, the following algorithm applies\ra gradient descent learning step based on that mini-batch:Input a set of training examples",
            "For each training example $x$:Set the corresponding\r  input activation $a^{x,1}$, and perform the following steps:",
            "Feedforward:For each $l = 2, 3, \\ldots, L$ compute\r  $z^{x,l} = w^l a^{x,l-1}+b^l$ and $a^{x,l} = \\sigma(z^{x,l})$.",
            "Output error $\\delta^{x,L}$:Compute the vector\r  $\\delta^{x,L} = \\nabla_a C_x \\odot \\sigma'(z^{x,L})$.",
            "Backpropagate the error:For each $l = L-1, L-2,\r  \\ldots, 2$ compute $\\delta^{x,l} = ((w^{l+1})^T \\delta^{x,l+1})\r  \\odot \\sigma'(z^{x,l})$.",
            "Gradient descent:For each $l = L, L-1, \\ldots, 2$\r  update the weights according to the rule $w^l \\rightarrow\r  w^l-\\frac{\\eta}{m} \\sum_x \\delta^{x,l} (a^{x,l-1})^T$, and the\r  biases according to the rule $b^l \\rightarrow b^l-\\frac{\\eta}{m}\r  \\sum_x \\delta^{x,l}$.",
            "Of course, to implement stochastic gradient descent in practice you\ralso need an outer loop generating mini-batches of training examples,\rand an outer loop stepping through multiple epochs of training.  I've\romitted those for simplicity.",
            "",
            "The code for backpropagation"
        ],
        "The code for backpropagation": [
            "Having understood backpropagation in the abstract, we can now\runderstand the code used in the last chapter to implement\rbackpropagation.  Recall fromthat\r  chapterthat the code was contained in theupdate_mini_batchandbackpropmethods of theNetworkclass.  The code for\rthese methods is a direct translation of the algorithm described\rabove.  In particular, theupdate_mini_batchmethod updates theNetwork's weights and biases by computing the gradient for the\rcurrentmini_batchof training examples:classNetwork(object):...defupdate_mini_batch(self,mini_batch,eta):\"\"\"Update the network's weights and biases by applyinggradient descent using backpropagation to a single mini batch.The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"is the learning rate.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]forx,yinmini_batch:delta_nabla_b,delta_nabla_w=self.backprop(x,y)nabla_b=[nb+dnbfornb,dnbinzip(nabla_b,delta_nabla_b)]nabla_w=[nw+dnwfornw,dnwinzip(nabla_w,delta_nabla_w)]self.weights=[w-(eta/len(mini_batch))*nwforw,nwinzip(self.weights,nabla_w)]self.biases=[b-(eta/len(mini_batch))*nbforb,nbinzip(self.biases,nabla_b)]Most of the work is done by the linedelta_nabla_b, delta_nabla_w = self.backprop(x, y)which uses\rthebackpropmethod to figure out the partial derivatives\r$\\partial C_x / \\partial b^l_j$ and $\\partial C_x / \\partial\rw^l_{jk}$.  Thebackpropmethod follows the algorithm in the\rlast section closely.  There is one small change - we use a slightly\rdifferent approach to indexing the layers.  This change is made to\rtake advantage of a feature of Python, namely the use of negative list\rindices to count backward from the end of a list, so, e.g.,l[-3]is the third last entry in a listl.  The code forbackpropis below, together with a few helper functions, which\rare used to compute the $\\sigma$ function, the derivative $\\sigma'$,\rand the derivative of the cost function.  With these inclusions you\rshould be able to understand the code in a self-contained way.  If\rsomething's tripping you up, you may find it helpful to consultthe\r  original description (and complete listing) of the code.classNetwork(object):...defbackprop(self,x,y):\"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing thegradient for the cost function C_x.  \"nabla_b\" and\"nabla_w\" are layer-by-layer lists of numpy arrays, similarto \"self.biases\" and \"self.weights\".\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]# feedforwardactivation=xactivations=[x]# list to store all the activations, layer by layerzs=[]# list to store all the z vectors, layer by layerforb,winzip(self.biases,self.weights):z=np.dot(w,activation)+bzs.append(z)activation=sigmoid(z)activations.append(activation)# backward passdelta=self.cost_derivative(activations[-1],y)*\\sigmoid_prime(zs[-1])nabla_b[-1]=deltanabla_w[-1]=np.dot(delta,activations[-2].transpose())# Note that the variable l in the loop below is used a little# differently to the notation in Chapter 2 of the book.  Here,# l = 1 means the last layer of neurons, l = 2 is the# second-last layer, and so on.  It's a renumbering of the# scheme in the book, used here to take advantage of the fact# that Python can use negative indices in lists.forlinxrange(2,self.num_layers):z=zs[-l]sp=sigmoid_prime(z)delta=np.dot(self.weights[-l+1].transpose(),delta)*spnabla_b[-l]=deltanabla_w[-l]=np.dot(delta,activations[-l-1].transpose())return(nabla_b,nabla_w)...defcost_derivative(self,output_activations,y):\"\"\"Return the vector of partial derivatives \\partial C_x /\\partial a for the output activations.\"\"\"return(output_activations-y)defsigmoid(z):\"\"\"The sigmoid function.\"\"\"return1.0/(1.0+np.exp(-z))defsigmoid_prime(z):\"\"\"Derivative of the sigmoid function.\"\"\"returnsigmoid(z)*(1-sigmoid(z))",
            "ProblemFully matrix-based approach to backpropagation over a\r  mini-batchOur implementation of stochastic gradient descent loops\r  over training examples in a mini-batch.  It's possible to modify the\r  backpropagation algorithm so that it computes the gradients for all\r  training examples in a mini-batch simultaneously.  The idea is that\r  instead of beginning with a single input vector, $x$, we can begin\r  with a matrix $X = [x_1 x_2 \\ldots x_m]$ whose columns are the\r  vectors in the mini-batch.  We forward-propagate by multiplying by\r  the weight matrices, adding a suitable matrix for the bias terms,\r  and applying the sigmoid function everywhere. We backpropagate along\r  similar lines.  Explicitly write out pseudocode for this approach to\r  the backpropagation algorithm.  Modifynetwork.pyso that it\r  uses this fully matrix-based approach.  The advantage of this\r  approach is that it takes full advantage of modern libraries for\r  linear algebra.  As a result it can be quite a bit faster than\r  looping over the mini-batch.  (On my laptop, for example, the\r  speedup is about a factor of two when run on MNIST classification\r  problems like those we considered in the last chapter.)  In\r  practice, all serious libraries for backpropagation use this fully\r  matrix-based approach or some variant.",
            "In what sense is backpropagation a fast algorithm?"
        ],
        "In what sense is backpropagation a fast algorithm?": [
            "In what sense is backpropagation a fast algorithm?  To answer this\rquestion, let's consider another approach to computing the gradient.\rImagine it's the early days of neural networks research.  Maybe it's\rthe 1950s or 1960s, and you're the first person in the world to think\rof using gradient descent to learn!  But to make the idea work you\rneed a way of computing the gradient of the cost function.  You think\rback to your knowledge of calculus, and decide to see if you can use\rthe chain rule to compute the gradient.  But after playing around a\rbit, the algebra looks complicated, and you get discouraged.  So you\rtry to find another approach.  You decide to regard the cost as a\rfunction of the weights $C = C(w)$ alone (we'll get back to the biases\rin a moment).  You number the weights $w_1, w_2, \\ldots$, and want to\rcompute $\\partial C / \\partial w_j$ for some particular weight $w_j$.\rAn obvious way of doing that is to use the approximation\\begin{eqnarray}  \\frac{\\partial\r    C}{\\partial w_{j}} \\approx \\frac{C(w+\\epsilon\r    e_j)-C(w)}{\\epsilon},\r\\tag{46}\\end{eqnarray}\rwhere $\\epsilon > 0$ is a small positive number, and $e_j$ is the unit\rvector in the $j^{\\rm th}$ direction.  In other words, we can estimate\r$\\partial C / \\partial w_j$ by computing the cost $C$ for two slightly\rdifferent values of $w_j$, and then applying\rEquation(46)\\begin{eqnarray}  \\frac{\\partial\r    C}{\\partial w_{j}} \\approx \\frac{C(w+\\epsilon\r    e_j)-C(w)}{\\epsilon} \\nonumber\\end{eqnarray}.  The same idea will let us\rcompute the partial derivatives $\\partial C / \\partial b$ with respect\rto the biases.",
            "This approach looks very promising.  It's simple conceptually, and\rextremely easy to implement, using just a few lines of code.\rCertainly, it looks much more promising than the idea of using the\rchain rule to compute the gradient!",
            "Unfortunately, while this approach appears promising, when you\rimplement the code it turns out to be extremely slow.  To understand\rwhy, imagine we have a million weights in our network.  Then for each\rdistinct weight $w_j$ we need to compute $C(w+\\epsilon e_j)$ in order\rto compute $\\partial C / \\partial w_j$.  That means that to compute\rthe gradient we need to compute the cost function a million different\rtimes, requiring a million forward passes through the network (per\rtraining example).  We need to compute $C(w)$ as well, so that's a\rtotal of a million and one passes through the network.",
            "What's clever about backpropagation is that it enables us to\rsimultaneously computeallthe partial derivatives $\\partial C\r/ \\partial w_j$ using just one forward pass through the network,\rfollowed by one backward pass through the network.  Roughly speaking,\rthe computational cost of the backward pass is about the same as the\rforward pass**This should be plausible, but it requires some\r  analysis to make a careful statement.  It's plausible because the\r  dominant computational cost in the forward pass is multiplying by\r  the weight matrices, while in the backward pass it's multiplying by\r  the transposes of the weight matrices.  These operations obviously\r  have similar computational cost..  And so the total cost of\rbackpropagation is roughly the same as making just two forward passes\rthrough the network.  Compare that to the million and one forward\rpasses we needed for the approach based\ron(46)\\begin{eqnarray}  \\frac{\\partial\r    C}{\\partial w_{j}} \\approx \\frac{C(w+\\epsilon\r    e_j)-C(w)}{\\epsilon} \\nonumber\\end{eqnarray}!  And so even though backpropagation\rappears superficially more complex than the approach based\ron(46)\\begin{eqnarray}  \\frac{\\partial\r    C}{\\partial w_{j}} \\approx \\frac{C(w+\\epsilon\r    e_j)-C(w)}{\\epsilon} \\nonumber\\end{eqnarray}, it's actually much, much faster.",
            "This speedup was first fully appreciated in 1986, and it greatly\rexpanded the range of problems that neural networks could solve.\rThat, in turn, caused a rush of people using neural networks.  Of\rcourse, backpropagation is not a panacea.  Even in the late 1980s\rpeople ran up against limits, especially when attempting to use\rbackpropagation to train deep neural networks, i.e., networks with\rmany hidden layers.  Later in the book we'll see how modern computers\rand some clever new ideas now make it possible to use backpropagation\rto train such deep neural networks.",
            "Backpropagation: the big picture"
        ],
        "Backpropagation: the big picture": [
            "As I've explained it, backpropagation presents two mysteries.  First,\rwhat's the algorithm really doing?  We've developed a picture of the\rerror being backpropagated from the output.  But can we go any deeper,\rand build up more intuition about what is going on when we do all\rthese matrix and vector multiplications?  The second mystery is how\rsomeone could ever have discovered backpropagation in the first place?\rIt's one thing to follow the steps in an algorithm, or even to follow\rthe proof that the algorithm works.  But that doesn't mean you\runderstand the problem so well that you could have discovered the\ralgorithm in the first place.  Is there a plausible line of reasoning\rthat could have led you to discover the backpropagation algorithm?  In\rthis section I'll address both these mysteries.",
            "To improve our intuition about what the algorithm is doing, let's\rimagine that we've made a small change $\\Delta w^l_{jk}$ to some\rweight in the network, $w^l_{jk}$:That change in weight will cause a change in the output activation\rfrom the corresponding neuron:That, in turn, will cause a change inallthe activations in\rthe next layer:Those changes will in turn cause changes in the next layer, and then\rthe next, and so on all the way through to causing a change in the\rfinal layer, and then in the cost function:The change $\\Delta C$ in the cost is related to the change $\\Delta\rw^l_{jk}$ in the weight by the equation\\begin{eqnarray} \r  \\Delta C \\approx \\frac{\\partial C}{\\partial w^l_{jk}} \\Delta w^l_{jk}.\r\\tag{47}\\end{eqnarray}\rThis suggests that a possible approach to computing $\\frac{\\partial\r  C}{\\partial w^l_{jk}}$ is to carefully track how a small change in\r$w^l_{jk}$ propagates to cause a small change in $C$.  If we can do\rthat, being careful to express everything along the way in terms of\reasily computable quantities, then we should be able to compute\r$\\partial C / \\partial w^l_{jk}$.",
            "Let's try to carry this out.  The change $\\Delta w^l_{jk}$ causes a\rsmall change $\\Delta a^{l}_j$ in the activation of the $j^{\\rm th}$ neuron in\rthe $l^{\\rm th}$ layer.  This change is given by\\begin{eqnarray} \r  \\Delta a^l_j \\approx \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\Delta w^l_{jk}.\r\\tag{48}\\end{eqnarray}\rThe change in activation $\\Delta a^l_{j}$ will cause changes inallthe activations in the next layer, i.e., the $(l+1)^{\\rm\r  th}$ layer.  We'll concentrate on the way just a single one of those\ractivations is affected, say $a^{l+1}_q$,In fact, it'll cause the following change:\\begin{eqnarray}\r  \\Delta a^{l+1}_q \\approx \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \\Delta a^l_j.\r\\tag{49}\\end{eqnarray}\rSubstituting in the expression from Equation(48)\\begin{eqnarray} \r  \\Delta a^l_j \\approx \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\Delta w^l_{jk} \\nonumber\\end{eqnarray},\rwe get:\\begin{eqnarray}\r  \\Delta a^{l+1}_q \\approx \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\Delta w^l_{jk}.\r\\tag{50}\\end{eqnarray}\rOf course, the change $\\Delta a^{l+1}_q$ will, in turn, cause changes\rin the activations in the next layer.  In fact, we can imagine a path\rall the way through the network from $w^l_{jk}$ to $C$, with each\rchange in activation causing a change in the next activation, and,\rfinally, a change in the cost at the output.  If the path goes through\ractivations $a^l_j, a^{l+1}_q, \\ldots, a^{L-1}_n, a^L_m$ then the\rresulting expression is\\begin{eqnarray}\r  \\Delta C \\approx \\frac{\\partial C}{\\partial a^L_m} \r  \\frac{\\partial a^L_m}{\\partial a^{L-1}_n}\r  \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots\r  \\frac{\\partial a^{l+1}_q}{\\partial a^l_j}\r  \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\Delta w^l_{jk},\r\\tag{51}\\end{eqnarray}\rthat is, we've picked up a $\\partial a / \\partial a$ type term for\reach additional neuron we've passed through, as well as the $\\partial\rC/\\partial a^L_m$ term at the end.  This represents the change in $C$\rdue to changes in the activations along this particular path through\rthe network.  Of course, there's many paths by which a change in\r$w^l_{jk}$ can propagate to affect the cost, and we've been\rconsidering just a single path. To compute the total change in $C$ it\ris plausible that we should sum over all the possible paths between\rthe weight and the final cost, i.e.,\\begin{eqnarray} \r  \\Delta C \\approx \\sum_{mnp\\ldots q} \\frac{\\partial C}{\\partial a^L_m} \r  \\frac{\\partial a^L_m}{\\partial a^{L-1}_n}\r  \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots\r  \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \r  \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\Delta w^l_{jk},\r\\tag{52}\\end{eqnarray}\rwhere we've summed over all possible choices for the intermediate\rneurons along the path.  Comparing with(47)\\begin{eqnarray} \r  \\Delta C \\approx \\frac{\\partial C}{\\partial w^l_{jk}} \\Delta w^l_{jk} \\nonumber\\end{eqnarray}we\rsee that\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w^l_{jk}} = \\sum_{mnp\\ldots q} \\frac{\\partial C}{\\partial a^L_m} \r  \\frac{\\partial a^L_m}{\\partial a^{L-1}_n}\r  \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots\r  \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \r  \\frac{\\partial a^l_j}{\\partial w^l_{jk}}.\r\\tag{53}\\end{eqnarray}\rNow, Equation(53)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w^l_{jk}} = \\sum_{mnp\\ldots q} \\frac{\\partial C}{\\partial a^L_m} \r  \\frac{\\partial a^L_m}{\\partial a^{L-1}_n}\r  \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots\r  \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \r  \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\nonumber\\end{eqnarray}looks complicated.  However,\rit has a nice intuitive interpretation.  We're computing the rate of\rchange of $C$ with respect to a weight in the network.  What the\requation tells us is that every edge between two neurons in the\rnetwork is associated with a rate factor which is just the partial\rderivative of one neuron's activation with respect to the other\rneuron's activation.  The edge from the first weight to the first\rneuron has a rate factor $\\partial a^{l}_j / \\partial w^l_{jk}$.  The\rrate factor for a path is just the product of the rate factors along\rthe path.  And the total rate of change $\\partial C / \\partial\rw^l_{jk}$ is just the sum of the rate factors of all paths from the\rinitial weight to the final cost.  This procedure is illustrated here,\rfor a single path:",
            "What I've been providing up to now is a heuristic argument, a way of\rthinking about what's going on when you perturb a weight in a network.\rLet me sketch out a line of thinking you could use to further develop\rthis argument.  First, you could derive explicit expressions for all\rthe individual partial derivatives in\rEquation(53)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w^l_{jk}} = \\sum_{mnp\\ldots q} \\frac{\\partial C}{\\partial a^L_m} \r  \\frac{\\partial a^L_m}{\\partial a^{L-1}_n}\r  \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots\r  \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \r  \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\nonumber\\end{eqnarray}.  That's easy to do with a bit of\rcalculus.  Having done that, you could then try to figure out how to\rwrite all the sums over indices as matrix multiplications.  This turns\rout to be tedious, and requires some persistence, but not\rextraordinary insight.  After doing all this, and then simplifying as\rmuch as possible, what you discover is that you end up with exactly\rthe backpropagation algorithm!  And so you can think of the\rbackpropagation algorithm as providing a way of computing the sum over\rthe rate factor for all these paths.  Or, to put it slightly\rdifferently, the backpropagation algorithm is a clever way of keeping\rtrack of small perturbations to the weights (and biases) as they\rpropagate through the network, reach the output, and then affect the\rcost.",
            "Now, I'm not going to work through all this here.  It's messy and\rrequires considerable care to work through all the details.  If you're\rup for a challenge, you may enjoy attempting it.  And even if not, I\rhope this line of thinking gives you some insight into what\rbackpropagation is accomplishing.",
            "What about the other mystery - how backpropagation could have been\rdiscovered in the first place?  In fact, if you follow the approach I\rjust sketched you will discover a proof of backpropagation.\rUnfortunately, the proof is quite a bit longer and more complicated\rthan the one I described earlier in this chapter.  So how was that\rshort (but more mysterious) proof discovered?  What you find when you\rwrite out all the details of the long proof is that, after the fact,\rthere are several obvious simplifications staring you in the face.\rYou make those simplifications, get a shorter proof, and write that\rout.  And then several more obvious simplifications jump out at\ryou. So you repeat again.  The result after a few iterations is the\rproof we saw earlier**There is one clever step required.  In\r  Equation(53)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w^l_{jk}} = \\sum_{mnp\\ldots q} \\frac{\\partial C}{\\partial a^L_m} \r  \\frac{\\partial a^L_m}{\\partial a^{L-1}_n}\r  \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots\r  \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \r  \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\nonumber\\end{eqnarray}the intermediate variables are\r  activations like $a_q^{l+1}$.  The clever idea is to switch to using\r  weighted inputs, like $z^{l+1}_q$, as the intermediate variables. If\r  you don't have this idea, and instead continue using the activations\r  $a^{l+1}_q$, the proof you obtain turns out to be slightly more\r  complex than the proof given earlier in the chapter.- short, but\rsomewhat obscure, because all the signposts to its construction have\rbeen removed!  I am, of course, asking you to trust me on this, but\rthere really is no great mystery to the origin of the earlier proof.\rIt's just a lot of hard work simplifying the proof I've sketched in\rthis section.",
            "",
            ""
        ]
    },
    "CHAPTER 3": {
        "The cross-entropy cost function": [
            "Most of us find it unpleasant to be wrong.  Soon after beginning to\rlearn the piano I gave my first performance before an audience.  I was\rnervous, and began playing the piece an octave too low.  I got\rconfused, and couldn't continue until someone pointed out my error.  I\rwas very embarrassed.  Yet while unpleasant, we also learn quickly when\rwe're decisively wrong.  You can bet that the next time I played\rbefore an audience I played in the correct octave!  By contrast, we\rlearn more slowly when our errors are less well-defined.",
            "Ideally, we hope and expect that our neural networks will learn fast\rfrom their errors.  Is this what happens in practice?  To answer this\rquestion, let's look at a toy example.  The example involves a neuron\rwith just one input:",
            "",
            "We'll train this neuron to do something ridiculously easy: take the\rinput $1$ to the output $0$.  Of course, this is such a trivial task\rthat we could easily figure out an appropriate weight and bias by\rhand, without using a learning algorithm.  However, it turns out to be\rilluminating to use gradient descent to attempt to learn a weight and\rbias.  So let's take a look at how the neuron learns.",
            "To make things definite, I'll pick the initial weight to be $0.6$ and\rthe initial bias to be $0.9$.  These are generic choices used as a\rplace to begin learning, I wasn't picking them to be special in any\rway.  The initial output from the neuron is $0.82$, so quite a bit of\rlearning will be needed before our neuron gets near the desired\routput, $0.0$.  Click on \"Run\" in the bottom right corner below to\rsee how the neuron learns an output much closer to $0.0$.  Note that\rthis isn't a pre-recorded animation, your browser is actually\rcomputing the gradient, then using the gradient to update the weight\rand bias, and displaying the result.  The learning rate is $\\eta =\r0.15$, which turns out to be slow enough that we can follow what's\rhappening, but fast enough that we can get substantial learning in\rjust a few seconds.  The cost is the quadratic cost function, $C$,\rintroduced back in Chapter 1.  I'll remind you of the exact form of\rthe cost function shortly, so there's no need to go and dig up the\rdefinition.  Note that you can run the animation multiple times by\rclicking on \"Run\" again.",
            "",
            "As you can see, the neuron rapidly learns a weight and bias that\rdrives down the cost, and gives an output from the neuron of about\r$0.09$.  That's not quite the desired output, $0.0$, but it is pretty\rgood.  Suppose, however, that we instead choose both the starting\rweight and the starting bias to be $2.0$.  In this case the initial\routput is $0.98$, which is very badly wrong.  Let's look at how the\rneuron learns to output $0$ in this case.  Click on \"Run\" again:",
            "",
            "Although this example uses the same learning rate ($\\eta = 0.15$), we\rcan see that learning starts out much more slowly.  Indeed, for the\rfirst 150 or so learning epochs, the weights and biases don't change\rmuch at all.  Then the learning kicks in and, much as in our first\rexample, the neuron's output rapidly moves closer to $0.0$.",
            "This behaviour is strange when contrasted to human learning.  As I\rsaid at the beginning of this section, we often learn fastest when\rwe're badly wrong about something.  But we've just seen that our\rartificial neuron has a lot of difficulty learning when it's badly\rwrong - far more difficulty than when it's just a little wrong.\rWhat's more, it turns out that this behaviour occurs not just in this\rtoy model, but in more general networks.  Why is learning so slow?\rAnd can we find a way of avoiding this slowdown?",
            "To understand the origin of the problem, consider that our neuron\rlearns by changing the weight and bias at a rate determined by the\rpartial derivatives of the cost function, $\\partial C/\\partial w$ and\r$\\partial C / \\partial b$.  So saying \"learning is slow\" is really\rthe same as saying that those partial derivatives are small.  The\rchallenge is to understand why they are small.  To understand that,\rlet's compute the partial derivatives.  Recall that we're using the\rquadratic cost function, which, from\rEquation(6)\\begin{eqnarray}  C(w,b) \\equiv\r  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 \\nonumber\\end{eqnarray}, is given by\\begin{eqnarray}\r  C = \\frac{(y-a)^2}{2},\r\\tag{54}\\end{eqnarray}\rwhere $a$ is the neuron's output when the training input $x = 1$ is\rused, and $y = 0$ is the corresponding desired output.  To write this\rmore explicitly in terms of the weight and bias, recall that $a =\r\\sigma(z)$, where $z = wx+b$.  Using the chain rule to differentiate\rwith respect to the weight and bias we get\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z) \\tag{55}\\\\\r  \r  \\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z),\r\\tag{56}\\end{eqnarray}\rwhere I have substituted $x = 1$ and $y = 0$.  To understand the\rbehaviour of these expressions, let's look more closely at the\r$\\sigma'(z)$ term on the right-hand side.  Recall the shape of the\r$\\sigma$ function:",
            "",
            "We can see from this graph that when the neuron's output is close to\r$1$, the curve gets very flat, and so $\\sigma'(z)$ gets very small.\rEquations(55)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z)  \\nonumber\\end{eqnarray}and(56)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z) \\nonumber\\end{eqnarray}then tell us that\r$\\partial C / \\partial w$ and $\\partial C / \\partial b$ get very\rsmall.  This is the origin of the learning slowdown.  What's more, as\rwe shall see a little later, the learning slowdown occurs for\ressentially the same reason in more general neural networks, not just\rthe toy example we've been playing with.",
            "Introducing the cross-entropy cost function",
            "How can we address the learning slowdown?  It turns out that we can\rsolve the problem by replacing the quadratic cost with a different\rcost function, known as the cross-entropy.  To understand the\rcross-entropy, let's move a little away from our super-simple toy\rmodel.  We'll suppose instead that we're trying to train a neuron with\rseveral input variables, $x_1, x_2, \\ldots$, corresponding weights\r$w_1, w_2, \\ldots$, and a bias, $b$:The output from the neuron is, of course, $a = \\sigma(z)$, where $z =\r\\sum_j w_j x_j+b$ is the weighted sum of the inputs.  We define the\rcross-entropy cost function for this neuron by\\begin{eqnarray} \r  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right],\r\\tag{57}\\end{eqnarray}\rwhere $n$ is the total number of items of training data, the sum is\rover all training inputs, $x$, and $y$ is the corresponding desired\routput.",
            "It's not obvious that the expression(57)\\begin{eqnarray} \r  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right] \\nonumber\\end{eqnarray}fixes the learning slowdown problem.  In fact, frankly, it's not even\robvious that it makes sense to call this a cost function!  Before\raddressing the learning slowdown, let's see in what sense the\rcross-entropy can be interpreted as a cost function.",
            "Two properties in particular make it reasonable to interpret the\rcross-entropy as a cost function.  First, it's non-negative, that is,\r$C > 0$.  To see this, notice that: (a) all the individual terms in\rthe sum in(57)\\begin{eqnarray} \r  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right] \\nonumber\\end{eqnarray}are negative, since both\rlogarithms are of numbers in the range $0$ to $1$; and (b) there is a\rminus sign out the front of the sum.",
            "Second, if the neuron's actual output is close to the desired output\rfor all training inputs, $x$, then the cross-entropy will be close to\rzero**To prove this I will need to assume that the desired\r  outputs $y$ are all either $0$ or $1$.  This is usually the case\r  when solving classification problems, for example, or when computing\r  Boolean functions.  To understand what happens when we don't make\r  this assumption, see the exercises at the end of this section..  To\rsee this, suppose for example that $y = 0$ and $a \\approx 0$ for some\rinput $x$.  This is a case when the neuron is doing a good job on that\rinput.  We see that the first term in the\rexpression(57)\\begin{eqnarray} \r  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right] \\nonumber\\end{eqnarray}for the cost vanishes, since\r$y = 0$, while the second term is just $-\\ln (1-a) \\approx 0$.  A\rsimilar analysis holds when $y = 1$ and $a \\approx 1$.  And so the\rcontribution to the cost will be low provided the actual output is\rclose to the desired output.",
            "Summing up, the cross-entropy is positive, and tends toward zero as\rthe neuron gets better at computing the desired output, $y$, for all\rtraining inputs, $x$.  These are both properties we'd intuitively\rexpect for a cost function.  Indeed, both properties are also\rsatisfied by the quadratic cost. So that's good news for the\rcross-entropy.  But the cross-entropy cost function has the benefit\rthat, unlike the quadratic cost, it avoids the problem of learning\rslowing down.  To see this, let's compute the partial derivative of\rthe cross-entropy cost with respect to the weights.  We substitute $a\r= \\sigma(z)$ into(57)\\begin{eqnarray} \r  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right] \\nonumber\\end{eqnarray}, and apply the chain\rrule twice, obtaining:\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial w_j} & = & -\\frac{1}{n} \\sum_x \\left(\r    \\frac{y }{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\r  \\frac{\\partial \\sigma}{\\partial w_j} \\tag{58}\\\\\r & = & -\\frac{1}{n} \\sum_x \\left( \r    \\frac{y}{\\sigma(z)} \r    -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma'(z) x_j.\r\\tag{59}\\end{eqnarray}\rPutting everything over a common denominator and simplifying this\rbecomes:\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial w_j} & = & \\frac{1}{n}\r  \\sum_x \\frac{\\sigma'(z) x_j}{\\sigma(z) (1-\\sigma(z))}\r  (\\sigma(z)-y).\r\\tag{60}\\end{eqnarray}\rUsing the definition of the sigmoid function, $\\sigma(z) =\r1/(1+e^{-z})$, and a little algebra we can show that $\\sigma'(z) =\r\\sigma(z)(1-\\sigma(z))$.  I'll ask you to verify this in an exercise\rbelow, but for now let's accept it as given.  We see that the\r$\\sigma'(z)$ and $\\sigma(z)(1-\\sigma(z))$ terms cancel in the equation\rjust above, and it simplifies to become:\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w_j} =  \\frac{1}{n} \\sum_x x_j(\\sigma(z)-y).\r\\tag{61}\\end{eqnarray}\rThis is a beautiful expression.  It tells us that the rate at which\rthe weight learns is controlled by $\\sigma(z)-y$, i.e., by the error\rin the output.  The larger the error, the faster the neuron will\rlearn.  This is just what we'd intuitively expect.  In particular, it\ravoids the learning slowdown caused by the $\\sigma'(z)$ term in the\ranalogous equation for the quadratic cost, Equation(55)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z)  \\nonumber\\end{eqnarray}.\rWhen we use the cross-entropy, the $\\sigma'(z)$ term gets canceled\rout, and we no longer need worry about it being small.  This\rcancellation is the special miracle ensured by the cross-entropy cost\rfunction.  Actually, it's not really a miracle.  As we'll see later,\rthe cross-entropy was specially chosen to have just this property.",
            "In a similar way, we can compute the partial derivative for the bias.\rI won't go through all the details again, but you can easily verify\rthat\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial b} = \\frac{1}{n} \\sum_x (\\sigma(z)-y).\r\\tag{62}\\end{eqnarray}\rAgain, this avoids the learning slowdown caused by the $\\sigma'(z)$\rterm in the analogous equation for the quadratic cost,\rEquation(56)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z) \\nonumber\\end{eqnarray}.",
            "ExerciseVerify that $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$.",
            "",
            "Let's return to the toy example we played with earlier, and explore\rwhat happens when we use the cross-entropy instead of the quadratic\rcost.  To re-orient ourselves, we'll begin with the case where the\rquadratic cost did just fine, with starting weight $0.6$ and starting\rbias $0.9$.  Press \"Run\" to see what happens when we replace the\rquadratic cost by the cross-entropy:",
            "",
            "Unsurprisingly, the neuron learns perfectly well in this instance,\rjust as it did earlier.  And now let's look at the case where our\rneuron got stuck before (link, for\rcomparison), with the weight and bias both starting at $2.0$:",
            "",
            "Success!  This time the neuron learned quickly, just as we hoped.  If\ryou observe closely you can see that the slope of the cost curve was\rmuch steeper initially than the initial flat region on the\rcorresponding curve for the quadratic cost.  It's that steepness which\rthe cross-entropy buys us, preventing us from getting stuck just when\rwe'd expect our neuron to learn fastest, i.e., when the neuron starts\rout badly wrong.",
            "I didn't say what learning rate was used in the examples just\rillustrated.  Earlier, with the quadratic cost, we used $\\eta = 0.15$.\rShould we have used the same learning rate in the new examples?  In\rfact, with the change in cost function it's not possible to say\rprecisely what it means to use the \"same\" learning rate; it's an\rapples and oranges comparison.  For both cost functions I simply\rexperimented to find a learning rate that made it possible to see what\ris going on.  If you're still curious, despite my disavowal, here's\rthe lowdown: I used $\\eta = 0.005$ in the examples just given.",
            "You might object that the change in learning rate makes the graphs\rabove meaningless.  Who cares how fast the neuron learns, when our\rchoice of learning rate was arbitrary to begin with?!  That objection\rmisses the point.  The point of the graphs isn't about the absolute\rspeed of learning.  It's about how the speed of learning changes.  In\rparticular, when we use the quadratic cost learning isslowerwhen the neuron is unambiguously wrong than it is later on, as the\rneuron gets closer to the correct output; while with the cross-entropy\rlearning is faster when the neuron is unambiguously wrong.  Those\rstatements don't depend on how the learning rate is set.",
            "We've been studying the cross-entropy for a single neuron.  However,\rit's easy to generalize the cross-entropy to many-neuron multi-layer\rnetworks.  In particular, suppose $y = y_1, y_2, \\ldots$ are the\rdesired values at the output neurons, i.e., the neurons in the final\rlayer, while $a^L_1, a^L_2, \\ldots$ are the actual output values.\rThen we define the cross-entropy by\\begin{eqnarray}  C = -\\frac{1}{n} \\sum_x\r  \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right].\r\\tag{63}\\end{eqnarray}\rThis is the same as our earlier expression,\rEquation(57)\\begin{eqnarray} \r  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right] \\nonumber\\end{eqnarray}, except now we've got the\r$\\sum_j$ summing over all the output neurons.  I won't explicitly work\rthrough a derivation, but it should be plausible that using the\rexpression(63)\\begin{eqnarray}  C = -\\frac{1}{n} \\sum_x\r  \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right] \\nonumber\\end{eqnarray}avoids a learning slowdown in\rmany-neuron networks.  If you're interested, you can work through the\rderivation in the problem below.",
            "Incidentally, I'm using the term \"cross-entropy\" in a way that has\rconfused some early readers, since it superficially appears to\rconflict with other sources.  In particular, it's common to define the\rcross-entropy for two probability distributions, $p_j$ and $q_j$, as\r$\\sum_j p_j \\ln q_j$.  This definition may be connected\rto(57)\\begin{eqnarray} \r  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right] \\nonumber\\end{eqnarray}, if we treat a single sigmoid neuron\ras outputting a probability distribution consisting of the neuron's\ractivation $a$ and its complement $1-a$.",
            "However, when we have many sigmoid neurons in the final layer, the\rvector $a^L_j$ of activations don't usually form a probability\rdistribution.  As a result, a definition like $\\sum_j p_j \\ln q_j$\rdoesn't even make sense, since we're not working with probability\rdistributions.  Instead, you can think of(63)\\begin{eqnarray}  C = -\\frac{1}{n} \\sum_x\r  \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right] \\nonumber\\end{eqnarray}as\ra summed set of per-neuron cross-entropies, with the activation of\reach neuron being interpreted as part of a two-element probability\rdistribution**Of course, in our networks there are no\r  probabilistic elements, so they're not really probabilities..  In\rthis sense,(63)\\begin{eqnarray}  C = -\\frac{1}{n} \\sum_x\r  \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right] \\nonumber\\end{eqnarray}is a generalization of the\rcross-entropy for probability distributions.",
            "When should we use the cross-entropy instead of the quadratic cost?\rIn fact, the cross-entropy is nearly always the better choice,\rprovided the output neurons are sigmoid neurons.  To see why, consider\rthat when we're setting up the network we usually initialize the\rweights and biases using some sort of randomization.  It may happen\rthat those initial choices result in the network being decisively\rwrong for some training input - that is, an output neuron will have\rsaturated near $1$, when it should be $0$, or vice versa.  If we're\rusing the quadratic cost that will slow down learning.  It won't stop\rlearning completely, since the weights will continue learning from\rother training inputs, but it's obviously undesirable.",
            "ExercisesOne gotcha with the cross-entropy is that it can be difficult at\r  first to remember the respective roles of the $y$s and the $a$s.\r  It's easy to get confused about whether the right form is $-[y \\ln a\r  + (1-y) \\ln (1-a)]$ or $-[a \\ln y + (1-a) \\ln (1-y)]$.  What happens\r  to the second of these expressions when $y = 0$ or $1$?  Does this\r  problem afflict the first expression?  Why or why not?",
            "In the single-neuron discussion at the start of this section, I\r  argued that the cross-entropy is small if $\\sigma(z) \\approx y$ for\r  all training inputs.  The argument relied on $y$ being equal to\r  either $0$ or $1$.  This is usually true in classification problems,\r  but for other problems (e.g., regression problems) $y$ can sometimes\r  take values intermediate between $0$ and $1$.  Show that the\r  cross-entropy is still minimized when $\\sigma(z) = y$ for all\r  training inputs.  When this is the case the cross-entropy has the\r  value:\\begin{eqnarray}\r    C = -\\frac{1}{n} \\sum_x [y \\ln y+(1-y) \\ln(1-y)].\r  \\tag{64}\\end{eqnarray}\r  The quantity $-[y \\ln y+(1-y)\\ln(1-y)]$ is sometimes known as thebinary\r    entropy.",
            "",
            "ProblemsMany-layer multi-neuron networksIn the notation introduced in\r  thelast chapter, show that for the quadratic\r  cost the partial derivative with respect to weights in the output\r  layer is\\begin{eqnarray}\r      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n}\r      \\sum_x a^{L-1}_k  (a^L_j-y_j) \\sigma'(z^L_j).\r  \\tag{65}\\end{eqnarray}\r  The term $\\sigma'(z^L_j)$ causes a learning slowdown whenever an\r  output neuron saturates on the wrong value.  Show that for the\r  cross-entropy cost the output error $\\delta^L$ for a single training\r  example $x$ is given by\\begin{eqnarray} \r    \\delta^L = a^L-y.\r  \\tag{66}\\end{eqnarray}\r  Use this expression to show that the partial derivative with respect\r  to the weights in the output layer is given by\\begin{eqnarray} \r      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n} \\sum_x \r      a^{L-1}_k  (a^L_j-y_j).\r  \\tag{67}\\end{eqnarray}\r  The $\\sigma'(z^L_j)$ term has vanished, and so the cross-entropy\r  avoids the problem of learning slowdown, not just when used with a\r  single neuron, as we saw earlier, but also in many-layer\r  multi-neuron networks.  A simple variation on this analysis holds\r  also for the biases.  If this is not obvious to you, then you should\r  work through that analysis as well.",
            "Using the quadratic cost when we have linear neurons in the\r  output layerSuppose that we have a many-layer multi-neuron\r  network.  Suppose all the neurons in the final layer arelinear neurons, meaning that the sigmoid activation function\r  is not applied, and the outputs are simply $a^L_j = z^L_j$.  Show\r  that if we use the quadratic cost function then the output error\r  $\\delta^L$ for a single training example $x$ is given by\\begin{eqnarray}\r    \\delta^L = a^L-y.\r  \\tag{68}\\end{eqnarray}\r  Similarly to the previous problem, use this expression to show that\r  the partial derivatives with respect to the weights and biases in\r  the output layer are given by\\begin{eqnarray}\r      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n} \\sum_x \r      a^{L-1}_k  (a^L_j-y_j) \\tag{69}\\\\\r      \\frac{\\partial C}{\\partial b^L_{j}} & = & \\frac{1}{n} \\sum_x \r      (a^L_j-y_j).\r  \\tag{70}\\end{eqnarray}\r  This shows that if the output neurons are linear neurons then the\r  quadratic cost will not give rise to any problems with a learning\r  slowdown.  In this case the quadratic cost is, in fact, an\r  appropriate cost function to use.",
            "Using the cross-entropy to classify MNIST digits",
            "",
            "The cross-entropy is easy to implement as part of a program which\rlearns using gradient descent and backpropagation.  We'll do thatlater in the\r  chapter, developing an improved version of ourearlier\r  programfor classifying the MNIST handwritten digits,network.py.  The new program is callednetwork2.py, and\rincorporates not just the cross-entropy, but also several other\rtechniques developed in this chapter**The code is availableon\r    GitHub..  For now, let's look at how well our new program\rclassifies MNIST digits.  As was the case in Chapter 1, we'll use a\rnetwork with $30$ hidden neurons, and we'll use a mini-batch size of\r$10$.  We set the learning rate to $\\eta = 0.5$**In Chapter 1\r  we used the quadratic cost and a learning rate of $\\eta = 3.0$.  As\r  discussed above, it's not possible to say precisely what it means to\r  use the \"same\" learning rate when the cost function is changed.\r  For both cost functions I experimented to find a learning rate that\r  provides near-optimal performance, given the other hyper-parameter\r  choices.There is, incidentally, a very rough\r  general heuristic for relating the learning rate for the\r  cross-entropy and the quadratic cost.  As we saw earlier, the\r  gradient terms for the quadratic cost have an extra $\\sigma' =\r  \\sigma(1-\\sigma)$ term in them.  Suppose we average this over values\r  for $\\sigma$, $\\int_0^1 d\\sigma \\sigma(1-\\sigma) = 1/6$.  We see\r  that (very roughly) the quadratic cost learns an average of $6$\r  times slower, for the same learning rate.  This suggests that a\r  reasonable starting point is to divide the learning rate for the\r  quadratic cost by $6$.  Of course, this argument is far from\r  rigorous, and shouldn't be taken too seriously.  Still, it can\r  sometimes be a useful starting point.and we train for $30$ epochs.\rThe interface tonetwork2.pyis slightly different thannetwork.py, but it should still be clear what is going on.  You\rcan, by the way, get documentation aboutnetwork2.py's\rinterface by using commands such ashelp(network2.Network.SGD)in a Python shell.",
            ">>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data,30,10,0.5,evaluation_data=test_data,...monitor_evaluation_accuracy=True)",
            "Note, by the way, that thenet.large_weight_initializer()command is used to initialize the weights and biases in the same way\ras described in Chapter 1.  We need to run this command because later\rin this chapter we'll change the default weight initialization in our\rnetworks.  The result from running the above sequence of commands is a\rnetwork with $95.49$ percent accuracy.  This is pretty close to the\rresult we obtained in Chapter 1, $95.42$ percent, using the quadratic\rcost.",
            "Let's look also at the case where we use $100$ hidden neurons, the\rcross-entropy, and otherwise keep the parameters the same.  In this\rcase we obtain an accuracy of $96.82$ percent.  That's a substantial\rimprovement over the results from Chapter 1, where we obtained a\rclassification accuracy of $96.59$ percent, using the quadratic cost.\rThat may look like a small change, but consider that the error rate\rhas dropped from $3.41$ percent to $3.18$ percent.  That is, we've\reliminated about one in fourteen of the original errors.  That's quite\ra handy improvement.",
            "It's encouraging that the cross-entropy cost gives us similar or\rbetter results than the quadratic cost.  However, these results don't\rconclusively prove that the cross-entropy is a better choice.  The\rreason is that I've put only a little effort into choosing\rhyper-parameters such as learning rate, mini-batch size, and so on.\rFor the improvement to be really convincing we'd need to do a thorough\rjob optimizing such hyper-parameters.  Still, the results are\rencouraging, and reinforce our earlier theoretical argument that the\rcross-entropy is a better choice than the quadratic cost.",
            "This, by the way, is part of a general pattern that we'll see through\rthis chapter and, indeed, through much of the rest of the book.  We'll\rdevelop a new technique, we'll try it out, and we'll get \"improved\"\rresults.  It is, of course, nice that we see such improvements.  But\rthe interpretation of such improvements is always problematic.\rThey're only truly convincing if we see an improvement after putting\rtremendous effort into optimizing all the other hyper-parameters.\rThat's a great deal of work, requiring lots of computing power, and\rwe're not usually going to do such an exhaustive investigation.\rInstead, we'll proceed on the basis of informal tests like those done\rabove.  Still, you should keep in mind that such tests fall short of\rdefinitive proof, and remain alert to signs that the arguments are\rbreaking down.",
            "By now, we've discussed the cross-entropy at great length.  Why go to\rso much effort when it gives only a small improvement to our MNIST\rresults?  Later in the chapter we'll see other techniques - notably,regularization- which\rgive much bigger improvements.  So why so much focus on cross-entropy?\rPart of the reason is that the cross-entropy is a widely-used cost\rfunction, and so is worth understanding well.  But the more important\rreason is that neuron saturation is an important problem in neural\rnets, a problem we'll return to repeatedly throughout the book.  And\rso I've discussed the cross-entropy at length because it's a good\rlaboratory to begin understanding neuron saturation and how it may be\raddressed.",
            "What does the cross-entropy mean?  Where does it come from?",
            "Our discussion of the cross-entropy has focused on algebraic analysis\rand practical implementation.  That's useful, but it leaves unanswered\rbroader conceptual questions, like: what does the cross-entropy mean?\rIs there some intuitive way of thinking about the cross-entropy?  And\rhow could we have dreamed up the cross-entropy in the first place?",
            "Let's begin with the last of these questions: what could have\rmotivated us to think up the cross-entropy in the first place?\rSuppose we'd discovered the learning slowdown described earlier, and\runderstood that the origin was the $\\sigma'(z)$ terms in\rEquations(55)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z)  \\nonumber\\end{eqnarray}and(56)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z) \\nonumber\\end{eqnarray}.  After staring at\rthose equations for a bit, we might wonder if it's possible to choose\ra cost function so that the $\\sigma'(z)$ term disappeared.  In that\rcase, the cost $C = C_x$ for a single training example $x$ would\rsatisfy\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w_j} & = & x_j(a-y) \\tag{71}\\\\\r  \r  \\frac{\\partial C}{\\partial b } & = & (a-y).\r\\tag{72}\\end{eqnarray}\rIf we could choose the cost function to make these equations true,\rthen they would capture in a simple way the intuition that the greater\rthe initial error, the faster the neuron learns.  They'd also\reliminate the problem of a learning slowdown.  In fact, starting from\rthese equations we'll now show that it's possible to derive the form\rof the cross-entropy, simply by following our mathematical noses.  To\rsee this, note that from the chain rule we have\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a} \r  \\sigma'(z).\r\\tag{73}\\end{eqnarray}\rUsing $\\sigma'(z) = \\sigma(z)(1-\\sigma(z)) = a(1-a)$ the last equation\rbecomes\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a} \r  a(1-a).\r\\tag{74}\\end{eqnarray}\rComparing to Equation(72)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial b } & = & (a-y) \\nonumber\\end{eqnarray}we obtain\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial a} = \\frac{a-y}{a(1-a)}.\r\\tag{75}\\end{eqnarray}\rIntegrating this expression with respect to $a$ gives\\begin{eqnarray}\r  C = -[y \\ln a + (1-y) \\ln (1-a)]+ {\\rm constant},\r\\tag{76}\\end{eqnarray}\rfor some constant of integration.  This is the contribution to the\rcost from a single training example, $x$.  To get the full cost\rfunction we must average over training examples, obtaining\\begin{eqnarray}\r  C = -\\frac{1}{n} \\sum_x [y \\ln a +(1-y) \\ln(1-a)] + {\\rm constant},\r\\tag{77}\\end{eqnarray}\rwhere the constant here is the average of the individual constants for\reach training example.  And so we see that\rEquations(71)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w_j} & = & x_j(a-y)  \\nonumber\\end{eqnarray}and(72)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial b } & = & (a-y) \\nonumber\\end{eqnarray}uniquely determine the form\rof the cross-entropy, up to an overall constant term.  The\rcross-entropy isn't something that was miraculously pulled out of thin\rair.  Rather, it's something that we could have discovered in a simple\rand natural way.",
            "What about the intuitive meaning of the cross-entropy?  How should we\rthink about it?  Explaining this in depth would take us further afield\rthan I want to go.  However, it is worth mentioning that there is a\rstandard way of interpreting the cross-entropy that comes from the\rfield of information theory.  Roughly speaking, the idea is that the\rcross-entropy is a measure of surprise.  In particular, our neuron is\rtrying to compute the function $x \\rightarrow y = y(x)$.  But instead\rit computes the function $x \\rightarrow a = a(x)$.  Suppose we think\rof $a$ as our neuron's estimated probability that $y$ is $1$, and\r$1-a$ is the estimated probability that the right value for $y$ is\r$0$.  Then the cross-entropy measures how \"surprised\" we are, on\raverage, when we learn the true value for $y$.  We get low surprise if\rthe output is what we expect, and high surprise if the output is\runexpected.  Of course, I haven't said exactly what \"surprise\"\rmeans, and so this perhaps seems like empty verbiage.  But in fact\rthere is a precise information-theoretic way of saying what is meant\rby surprise.  Unfortunately, I don't know of a good, short,\rself-contained discussion of this subject that's available online.\rBut if you want to dig deeper, then Wikipedia contains abrief\r  summarythat will get you started down the right track.  And the\rdetails can be filled in by working through the materials about the\rKraft inequality in chapter 5 of the book about information theory byCover and Thomas.",
            "ProblemWe've discussed at length the learning slowdown that can occur\r  when output neurons saturate, in networks using the quadratic cost\r  to train.  Another factor that may inhibit learning is the presence\r  of the $x_j$ term in Equation(61)\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w_j} =  \\frac{1}{n} \\sum_x x_j(\\sigma(z)-y) \\nonumber\\end{eqnarray}.  Because of\r  this term, when an input $x_j$ is near to zero, the corresponding\r  weight $w_j$ will learn slowly.  Explain why it is not possible to\r  eliminate the $x_j$ term through a clever choice of cost function.",
            "Softmax",
            "In this chapter we'll mostly use the cross-entropy cost to address the\rproblem of learning slowdown.  However, I want to briefly describe\ranother approach to the problem, based on what are calledsoftmaxlayers of neurons. We're not actually going to use\rsoftmax layers in the remainder of the chapter, so if you're in a\rgreat hurry, you can skip to the next section.  However, softmax is\rstill worth understanding, in part because it's intrinsically\rinteresting, and in part because we'll use softmax layers inChapter 6, in our discussion of deep neural\rnetworks.",
            "The idea of softmax is to define a new type of output layer for our\rneural networks.  It begins in the same way as with a sigmoid layer,\rby forming the weighted inputs**In describing the softmax\r  we'll make frequent use of notation introduced in thelast chapter.  You may wish to revisit that\r  chapter if you need to refresh your memory about the meaning of the\r  notation.$z^L_j = \\sum_{k} w^L_{jk} a^{L-1}_k + b^L_j$.  However,\rwe don't apply the sigmoid function to get the output.  Instead, in a\rsoftmax layer we apply the so-calledsoftmax functionto the\r$z^L_j$.  According to this function, the activation $a^L_j$ of the\r$j$th output neuron is\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}},\r\\tag{78}\\end{eqnarray}\rwhere in the denominator we sum over all the output neurons.",
            "If you're not familiar with the softmax function,\rEquation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}may look pretty opaque.  It's certainly\rnot obvious why we'd want to use this function.  And it's also not\robvious that this will help us address the learning slowdown problem.\rTo better understand Equation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}, suppose we have a\rnetwork with four output neurons, and four corresponding weighted\rinputs, which we'll denote $z^L_1, z^L_2, z^L_3$, and $z^L_4$.  Shown\rbelow are adjustable sliders showing possible values for the weighted\rinputs, and a graph of the corresponding output activations.  A good\rplace to start exploration is by using the bottom slider to increase\r$z^L_4$:",
            "$z^L_1 = $$a^L_1 = $$z^L_2$ =$a^L_2 = $$z^L_3$ =$a^L_3 = $$z^L_4$ =$a^L_4 = $As you increase $z^L_4$, you'll see an increase in the corresponding\routput activation, $a^L_4$, and a decrease in the other output\ractivations.  Similarly, if you decrease $z^L_4$ then $a^L_4$ will\rdecrease, and all the other output activations will increase.  In\rfact, if you look closely, you'll see that in both cases the total\rchange in the other activations exactly compensates for the change in\r$a^L_4$.  The reason is that the output activations are guaranteed to\ralways sum up to $1$, as we can prove using\rEquation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}and a little algebra:\\begin{eqnarray}\r  \\sum_j a^L_j & = & \\frac{\\sum_j e^{z^L_j}}{\\sum_k e^{z^L_k}} = 1.\r\\tag{79}\\end{eqnarray}\rAs a result, if $a^L_4$ increases, then the other output activations\rmust decrease by the same total amount, to ensure the sum over all\ractivations remains $1$.  And, of course, similar statements hold for\rall the other activations.Equation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}also implies that the output activations\rare all positive, since the exponential function is positive.\rCombining this with the observation in the last paragraph, we see that\rthe output from the softmax layer is a set of positive numbers which\rsum up to $1$.  In other words, the output from the softmax layer can\rbe thought of as a probability distribution.The fact that a softmax layer outputs a probability distribution is\rrather pleasing.  In many problems it's convenient to be able to\rinterpret the output activation $a^L_j$ as the network's estimate of\rthe probability that the correct output is $j$.  So, for instance, in\rthe MNIST classification problem, we can interpret $a^L_j$ as the\rnetwork's estimated probability that the correct digit classification\ris $j$.By contrast, if the output layer was a sigmoid layer, then we\rcertainly couldn't assume that the activations formed a probability\rdistribution.  I won't explicitly prove it, but it should be plausible\rthat the activations from a sigmoid layer won't in general form a\rprobability distribution.  And so with a sigmoid output layer we don't\rhave such a simple interpretation of the output activations.ExerciseConstruct an example showing explicitly that in a network with a\r  sigmoid output layer, the output activations $a^L_j$ won't always\r  sum to $1$.We're starting to build up some feel for the softmax function and the\rway softmax layers behave.  Just to review where we're at: the\rexponentials in Equation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}ensure that all the output\ractivations are positive.  And the sum in the denominator of\rEquation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}ensures that the softmax outputs sum to\r$1$.  So that particular form no longer appears so mysterious: rather,\rit is a natural way to ensure that the output activations form a\rprobability distribution.  You can think of softmax as a way of\rrescaling the $z^L_j$, and then squishing them together to form a\rprobability distribution.ExercisesMonotonicity of softmaxShow that $\\partial a^L_j / \\partial\r  z^L_k$ is positive if $j = k$ and negative if $j \\neq k$.  As a\r  consequence, increasing $z^L_j$ is guaranteed to increase the\r  corresponding output activation, $a^L_j$, and will decrease all the\r  other output activations.  We already saw this empirically with the\r  sliders, but this is a rigorous proof.Non-locality of softmaxA nice thing about sigmoid layers is\r  that the output $a^L_j$ is a function of the corresponding weighted\r  input, $a^L_j = \\sigma(z^L_j)$.  Explain why this is not the case\r  for a softmax layer: any particular output activation $a^L_j$\r  depends onallthe weighted inputs.ProblemInverting the softmax layerSuppose we have a neural network\r  with a softmax output layer, and the activations $a^L_j$ are known.\r  Show that the corresponding weighted inputs have the form $z^L_j =\r  \\ln a^L_j + C$, for some constant $C$ that is independent of $j$.The learning slowdown problem:We've now built up\rconsiderable familiarity with softmax layers of neurons.  But we\rhaven't yet seen how a softmax layer lets us address the learning\rslowdown problem.  To understand that, let's define thelog-likelihoodcost function.  We'll use $x$ to denote a\rtraining input to the network, and $y$ to denote the corresponding\rdesired output.  Then the log-likelihood cost associated to this\rtraining input is\\begin{eqnarray}\r  C \\equiv -\\ln a^L_y.\r\\tag{80}\\end{eqnarray}\rSo, for instance, if we're training with MNIST images, and input an\rimage of a $7$, then the log-likelihood cost is $-\\ln a^L_7$.  To see\rthat this makes intuitive sense, consider the case when the network is\rdoing a good job, that is, it is confident the input is a $7$.  In\rthat case it will estimate a value for the corresponding probability\r$a^L_7$ which is close to $1$, and so the cost $-\\ln a^L_7$ will be\rsmall.  By contrast, when the network isn't doing such a good job, the\rprobability $a^L_7$ will be smaller, and the cost $-\\ln a^L_7$ will be\rlarger.  So the log-likelihood cost behaves as we'd expect a cost\rfunction to behave.What about the learning slowdown problem?  To analyze that, recall\rthat the key to the learning slowdown is the behaviour of the\rquantities $\\partial C / \\partial w^L_{jk}$ and $\\partial C / \\partial\rb^L_j$.  I won't go through the derivation explicitly - I'll ask you\rto do in the problems, below - but with a little algebra you can\rshow that**Note that I'm abusing notation here, using $y$ in a\r  slightly different way to last paragraph.  In the last paragraph we\r  used $y$ to denote the desired output from the network - e.g.,\r  output a \"$7$\" if an image of a $7$ was input.  But in the\r  equations which follow I'm using $y$ to denote the vector of output\r  activations which corresponds to $7$, that is, a vector which is all\r  $0$s, except for a $1$ in the $7$th location.\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial b^L_j} & = & a^L_j-y_j  \\tag{81}\\\\\r  \\frac{\\partial C}{\\partial w^L_{jk}} & = & a^{L-1}_k (a^L_j-y_j) \r\\tag{82}\\end{eqnarray}\rThese equations are the same as the analogous expressions obtained in\rour earlier analysis of the cross-entropy.  Compare, for example,\rEquation(82)\\begin{eqnarray}  \\frac{\\partial C}{\\partial w^L_{jk}} & = & a^{L-1}_k (a^L_j-y_j)  \\nonumber\\end{eqnarray}to Equation(67)\\begin{eqnarray} \r      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n} \\sum_x \r      a^{L-1}_k  (a^L_j-y_j).\r   \\nonumber\\end{eqnarray}.  It's the\rsame equation, albeit in the latter I've averaged over training\rinstances.  And, just as in the earlier analysis, these expressions\rensure that we will not encounter a learning slowdown.  In fact, it's\ruseful to think of a softmax output layer with log-likelihood cost as\rbeing quite similar to a sigmoid output layer with cross-entropy cost.Given this similarity, should you use a sigmoid output layer and\rcross-entropy, or a softmax output layer and log-likelihood?  In fact,\rin many situations both approaches work well.  Through the remainder\rof this chapter we'll use a sigmoid output layer, with the\rcross-entropy cost.  Later, inChapter 6, we'll\rsometimes use a softmax output layer, with log-likelihood cost.  The\rreason for the switch is to make some of our later networks more\rsimilar to networks found in certain influential academic papers.  As\ra more general point of principle, softmax plus log-likelihood is\rworth using whenever you want to interpret the output activations as\rprobabilities.  That's not always a concern, but can be useful with\rclassification problems (like MNIST) involving disjoint classes.ProblemsDerive Equations(81)\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^L_j} & = & a^L_j-y_j   \\nonumber\\end{eqnarray}and(82)\\begin{eqnarray}  \\frac{\\partial C}{\\partial w^L_{jk}} & = & a^{L-1}_k (a^L_j-y_j)  \\nonumber\\end{eqnarray}.Where does the \"softmax\" name come from?Suppose we change\r  the softmax function so the output activations are given by\\begin{eqnarray}\r  a^L_j = \\frac{e^{c z^L_j}}{\\sum_k e^{c z^L_k}},\r\\tag{83}\\end{eqnarray}\rwhere $c$ is a positive constant.  Note that $c = 1$ corresponds to\rthe standard softmax function.  But if we use a different value of $c$\rwe get a different function, which is nonetheless qualitatively rather\rsimilar to the softmax.  In particular, show that the output\ractivations form a probability distribution, just as for the usual\rsoftmax.  Suppose we allow $c$ to become large, i.e., $c \\rightarrow\r\\infty$.  What is the limiting value for the output activations\r$a^L_j$?  After solving this problem it should be clear to you why we\rthink of the $c = 1$ function as a \"softened\" version of the maximum\rfunction.  This is the origin of the term \"softmax\".Backpropagation with softmax and the log-likelihood costIn the\r  last chapter we derived the backpropagation algorithm for a network\r  containing sigmoid layers.  To apply the algorithm to a network with\r  a softmax layer we need to figure out an expression for the error\r  $\\delta^L_j \\equiv \\partial C / \\partial z^L_j$ in the final layer.\r  Show that a suitable expression is:\\begin{eqnarray}\r    \\delta^L_j = a^L_j -y_j.\r  \\tag{84}\\end{eqnarray}\r  Using this expression we can apply the backpropagation algorithm to\r  a network using a softmax output layer and the log-likelihood cost.Overfitting and regularizationThe Nobel prizewinning physicist Enrico Fermi was once asked his\ropinion of a mathematical model some colleagues had proposed as the\rsolution to an important unsolved physics problem.  The model gave\rexcellent agreement with experiment, but Fermi was skeptical.  He\rasked how many free parameters could be set in the model.  \"Four\"\rwas the answer.  Fermi replied**The quote comes from a\r  charming article byFreeman\r    Dyson, who is one of the people who proposed the flawed model. A\r  four-parameter elephant may be foundhere.:\r\"I remember my friend Johnny von Neumann used to say, with four\rparameters I can fit an elephant, and with five I can make him wiggle\rhis trunk.\".The point, of course, is that models with a large number of free\rparameters can describe an amazingly wide range of phenomena.  Even if\rsuch a model agrees well with the available data, that doesn't make it\ra good model.  It may just mean there's enough freedom in the model\rthat it can describe almost any data set of the given size, without\rcapturing any genuine insights into the underlying phenomenon.  When\rthat happens the model will work well for the existing data, but will\rfail to generalize to new situations.  The true test of a model is its\rability to make predictions in situations it hasn't been exposed to\rbefore.Fermi and von Neumann were suspicious of models with four parameters.\rOur 30 hidden neuron network for classifying MNIST digits has nearly\r24,000 parameters!  That's a lot of parameters.  Our 100 hidden neuron\rnetwork has nearly 80,000 parameters, and state-of-the-art deep neural\rnets sometimes contain millions or even billions of parameters.\rShould we trust the results?Let's sharpen this problem up by constructing a situation where our\rnetwork does a bad job generalizing to new situations.  We'll use our\r30 hidden neuron network, with its 23,860 parameters.  But we won't\rtrain the network using all 50,000 MNIST training images.  Instead,\rwe'll use just the first 1,000 training images.  Using that restricted\rset will make the problem with generalization much more evident.\rWe'll train in a similar way to before, using the cross-entropy cost\rfunction, with a learning rate of $\\eta = 0.5$ and a mini-batch size\rof $10$.  However, we'll train for 400 epochs, a somewhat larger\rnumber than before, because we're not using as many training examples.\rLet's usenetwork2to look at the way the cost function\rchanges:>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data[:1000],400,10,0.5,evaluation_data=test_data,...monitor_evaluation_accuracy=True,monitor_training_cost=True)Using the results we can plot the way the cost changes as the network\rlearns**This and the next four graphs were generated by the\r  programoverfitting.py.:This looks encouraging, showing a smooth decrease in the cost, just as\rwe expect.  Note that I've only shown training epochs 200 through 399.\rThis gives us a nice up-close view of the later stages of learning,\rwhich, as we'll see, turns out to be where the interesting action is.Let's now look at how the classification accuracy on the test data\rchanges over time:Again, I've zoomed in quite a bit.  In the first 200 epochs (not\rshown) the accuracy rises to just under 82 percent.  The learning then\rgradually slows down.  Finally, at around epoch 280 the classification\raccuracy pretty much stops improving.  Later epochs merely see small\rstochastic fluctuations near the value of the accuracy at epoch 280.\rContrast this with the earlier graph, where the cost associated to the\rtraining data continues to smoothly drop.  If we just look at that\rcost, it appears that our model is still getting \"better\".  But the\rtest accuracy results show the improvement is an illusion.  Just like\rthe model that Fermi disliked, what our network learns after epoch 280\rno longer generalizes to the test data.  And so it's not useful\rlearning.  We say the network isoverfittingorovertrainingbeyond epoch 280.You might wonder if the problem here is that I'm looking at thecoston the training data, as opposed to theclassification accuracyon the test data.  In other words,\rmaybe the problem is that we're making an apples and oranges\rcomparison.  What would happen if we compared the cost on the training\rdata with the cost on the test data, so we're comparing similar\rmeasures?  Or perhaps we could compare the classification accuracy on\rboth the training data and the test data?  In fact, essentially the\rsame phenomenon shows up no matter how we do the comparison.  The\rdetails do change, however. For instance, let's look at the cost on\rthe test data:We can see that the cost on the test data improves until around epoch\r15, but after that it actually starts to get worse, even though the\rcost on the training data is continuing to get better.  This is\ranother sign that our model is overfitting.  It poses a puzzle,\rthough, which is whether we should regard epoch 15 or epoch 280 as the\rpoint at which overfitting is coming to dominate learning?  From a\rpractical point of view, what we really care about is improving\rclassification accuracy on the test data, while the cost on the test\rdata is no more than a proxy for classification accuracy.  And so it\rmakes most sense to regard epoch 280 as the point beyond which\roverfitting is dominating learning in our neural network.Another sign of overfitting may be seen in the classification accuracy\ron the training data:The accuracy rises all the way up to $100$ percent.  That is, our\rnetwork correctly classifies all $1,000$ training images!  Meanwhile,\rour test accuracy tops out at just $82.27$ percent.  So our network\rreally is learning about peculiarities of the training set, not just\rrecognizing digits in general.  It's almost as though our network is\rmerely memorizing the training set, without understanding digits well\renough to generalize to the test set.Overfitting is a major problem in neural networks.  This is especially\rtrue in modern networks, which often have very large numbers of\rweights and biases.  To train effectively, we need a way of detecting\rwhen overfitting is going on, so we don't overtrain.  And we'd like to\rhave techniques for reducing the effects of overfitting.The obvious way to detect overfitting is to use the approach above,\rkeeping track of accuracy on the test data as our network trains.  If\rwe see that the accuracy on the test data is no longer improving, then\rwe should stop training.  Of course, strictly speaking, this is not\rnecessarily a sign of overfitting.  It might be that accuracy on the\rtest data and the training data both stop improving at the same time.\rStill, adopting this strategy will prevent overfitting.In fact, we'll use a variation on this strategy.  Recall that when we\rload in the MNIST data we load in three data sets:>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()Up to now we've been using thetraining_dataandtest_data, and ignoring thevalidation_data.  Thevalidation_datacontains $10,000$ images of digits, images\rwhich are different from the $50,000$ images in the MNIST training\rset, and the $10,000$ images in the MNIST test set.  Instead of using\rthetest_datato prevent overfitting, we will use thevalidation_data.  To do this, we'll use much the same strategy\ras was described above for thetest_data.  That is, we'll\rcompute the classification accuracy on thevalidation_dataat\rthe end of each epoch.  Once the classification accuracy on thevalidation_datahas saturated, we stop training.  This strategy\ris calledearly stopping.  Of course, in practice we won't\rimmediately know when the accuracy has saturated.  Instead, we\rcontinue training until we're confident that the accuracy has\rsaturated**It requires some judgment to determine when to\r  stop.  In my earlier graphs I identified epoch 280 as the place at\r  which accuracy saturated.  It's possible that was too pessimistic.\r  Neural networks sometimes plateau for a while in training, before\r  continuing to improve.  I wouldn't be surprised if more learning\r  could have occurred even after epoch 400, although the magnitude of\r  any further improvement would likely be small.  So it's possible to\r  adopt more or less aggressive strategies for early stopping..Why use thevalidation_datato prevent overfitting, rather than\rthetest_data?  In fact, this is part of a more general\rstrategy, which is to use thevalidation_datato evaluate\rdifferent trial choices of hyper-parameters such as the number of\repochs to train for, the learning rate, the best network architecture,\rand so on.  We use such evaluations to find and set good values for\rthe hyper-parameters.  Indeed, although I haven't mentioned it until\rnow, that is, in part, how I arrived at the hyper-parameter choices\rmade earlier in this book. (More on thislater.)Of course, that doesn't in any way answer the question of why we're\rusing thevalidation_datato prevent overfitting, rather than\rthetest_data.  Instead, it replaces it with a more general\rquestion, which is why we're using thevalidation_datarather\rthan thetest_datato set good hyper-parameters?  To understand\rwhy, consider that when setting hyper-parameters we're likely to try\rmany different choices for the hyper-parameters.  If we set the\rhyper-parameters based on evaluations of thetest_datait's\rpossible we'll end up overfitting our hyper-parameters to thetest_data.  That is, we may end up finding hyper-parameters\rwhich fit particular peculiarities of thetest_data, but where\rthe performance of the network won't generalize to other data sets.\rWe guard against that by figuring out the hyper-parameters using thevalidation_data.  Then, once we've got the hyper-parameters we\rwant, we do a final evaluation of accuracy using thetest_data.\rThat gives us confidence that our results on thetest_dataare\ra true measure of how well our neural network generalizes.  To put it\ranother way, you can think of the validation data as a type of\rtraining data that helps us learn good hyper-parameters.  This\rapproach to finding good hyper-parameters is sometimes known as thehold outmethod, since thevalidation_datais kept apart\ror \"held out\" from thetraining_data.Now, in practice, even after evaluating performance on thetest_datawe may change our minds and want to try another\rapproach - perhaps a different network architecture - which will\rinvolve finding a new set of hyper-parameters.  If we do this, isn't\rthere a danger we'll end up overfitting to thetest_dataas\rwell?  Do we need a potentially infinite regress of data sets, so we\rcan be confident our results will generalize?  Addressing this concern\rfully is a deep and difficult problem.  But for our practical\rpurposes, we're not going to worry too much about this question.\rInstead, we'll plunge ahead, using the basic hold out method, based on\rthetraining_data,validation_data, andtest_data, as described above.We've been looking so far at overfitting when we're just using 1,000\rtraining images.  What happens when we use the full training set of\r50,000 images?  We'll keep all the other parameters the same (30\rhidden neurons, learning rate 0.5, mini-batch size of 10), but train\rusing all 50,000 images for 30 epochs.  Here's a graph showing the\rresults for the classification accuracy on both the training data and\rthe test data.  Note that I've used the test data here, rather than\rthe validation data, in order to make the results more directly\rcomparable with the earlier graphs.As you can see, the accuracy on the test and training data remain much\rcloser together than when we were using 1,000 training examples.  In\rparticular, the best classification accuracy of $97.86$ percent on the\rtraining data is only $2.53$ percent higher than the $95.33$ percent\ron the test data.  That's compared to the $17.73$ percent gap we had\rearlier!  Overfitting is still going on, but it's been greatly\rreduced.  Our network is generalizing much better from the training\rdata to the test data.  In general, one of the best ways of reducing\roverfitting is to increase the size of the training data.  With enough\rtraining data it is difficult for even a very large network to\roverfit.  Unfortunately, training data can be expensive or difficult\rto acquire, so this is not always a practical option.RegularizationIncreasing the amount of training data is one way of reducing\roverfitting.  Are there other ways we can reduce the extent to which\roverfitting occurs?  One possible approach is to reduce the size of\rour network. However, large networks have the potential to be more\rpowerful than small networks, and so this is an option we'd only adopt\rreluctantly.Fortunately, there are other techniques which can reduce overfitting,\reven when we have a fixed network and fixed training data.  These are\rknown asregularizationtechniques.  In this section I describe\rone of the most commonly used regularization techniques, a technique\rsometimes known asweight decayorL2 regularization.\rThe idea of L2 regularization is to add an extra term to the cost\rfunction, a term called theregularization term.  Here's the\rregularized cross-entropy:\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\r(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\r\\tag{85}\\end{eqnarray}The first term is just the usual expression for the cross-entropy.\rBut we've added a second term, namely the sum of the squares of all\rthe weights in the network.  This is scaled by a factor $\\lambda /\r2n$, where $\\lambda > 0$ is known as theregularization\r  parameter, and $n$ is, as usual, the size of our training set.\rI'll discuss later how $\\lambda$ is chosen.  It's also worth noting\rthat the regularization termdoesn'tinclude the biases.  I'll\ralso come back to that below.Of course, it's possible to regularize other cost functions, such as\rthe quadratic cost.  This can be done in a similar way:\\begin{eqnarray} C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 +\r  \\frac{\\lambda}{2n} \\sum_w w^2.\r\\tag{86}\\end{eqnarray}In both cases we can write the regularized cost function as\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\r\\sum_w w^2,\r\\tag{87}\\end{eqnarray} where $C_0$ is the original, unregularized cost\rfunction.Intuitively, the effect of regularization is to make it so the network\rprefers to learn small weights, all other things being equal.  Large\rweights will only be allowed if they considerably improve the first\rpart of the cost function.  Put another way, regularization can be\rviewed as a way of compromising between finding small weights and\rminimizing the original cost function.  The relative importance of the\rtwo elements of the compromise depends on the value of $\\lambda$: when\r$\\lambda$ is small we prefer to minimize the original cost function,\rbut when $\\lambda$ is large we prefer small weights.Now, it's really not at all obvious why making this kind of compromise\rshould help reduce overfitting!  But it turns out that it does. We'll\raddress the question of why it helps in the next section.  But first,\rlet's work through an example showing that regularization really does\rreduce overfitting.To construct such an example, we first need to figure out how to apply\rour stochastic gradient descent learning algorithm in a regularized\rneural network.  In particular, we need to know how to compute the\rpartial derivatives $\\partial C / \\partial w$ and $\\partial C\r/ \\partial b$ for all the weights and biases in the network.  Taking\rthe partial derivatives of Equation(87)\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\r\\sum_w w^2 \\nonumber\\end{eqnarray}gives\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w} & = & \\frac{\\partial C_0}{\\partial w} + \r  \\frac{\\lambda}{n} w \\tag{88}\\\\ \r  \\frac{\\partial C}{\\partial b} & = & \\frac{\\partial C_0}{\\partial b}.\r\\tag{89}\\end{eqnarray}The $\\partial C_0 / \\partial w$ and $\\partial C_0 / \\partial b$ terms\rcan be computed using backpropagation, as described inthe last chapter.  And so we see that it's easy to\rcompute the gradient of the regularized cost function: just use\rbackpropagation, as usual, and then add $\\frac{\\lambda}{n} w$ to the\rpartial derivative of all the weight terms.  The partial derivatives\rwith respect to the biases are unchanged, and so the gradient descent\rlearning rule for the biases doesn't change from the usual rule:\\begin{eqnarray}\rb & \\rightarrow & b -\\eta \\frac{\\partial C_0}{\\partial b}.\r\\tag{90}\\end{eqnarray}The learning rule for the weights becomes:\\begin{eqnarray} \r  w & \\rightarrow & w-\\eta \\frac{\\partial C_0}{\\partial\r    w}-\\frac{\\eta \\lambda}{n} w \\tag{91}\\\\ \r  & = & \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial\r    C_0}{\\partial w}. \r\\tag{92}\\end{eqnarray}This is exactly the same as the usual gradient descent learning rule,\rexcept we first rescale the weight $w$ by a factor $1-\\frac{\\eta\r  \\lambda}{n}$.  This rescaling is sometimes referred to asweight decay, since it makes the weights smaller.  At first\rglance it looks as though this means the weights are being driven\runstoppably toward zero.  But that's not right, since the other term\rmay lead the weights to increase, if so doing causes a decrease in the\runregularized cost function.Okay, that's how gradient descent works.  What about stochastic\rgradient descent?  Well, just as in unregularized stochastic gradient\rdescent, we can estimate $\\partial C_0 / \\partial w$ by averaging over\ra mini-batch of $m$ training examples.  Thus the regularized learning\rrule for stochastic gradient descent becomes\r(c.f. Equation(20)\\begin{eqnarray} \r  w_k & \\rightarrow & w_k' = w_k-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k}  \\nonumber\\end{eqnarray})\\begin{eqnarray} \r  w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\r  \\sum_x \\frac{\\partial C_x}{\\partial w}, \r\\tag{93}\\end{eqnarray}where the sum is over training examples $x$ in the mini-batch, and\r$C_x$ is the (unregularized) cost for each training example.  This is\rexactly the same as the usual rule for stochastic gradient descent,\rexcept for the $1-\\frac{\\eta \\lambda}{n}$ weight decay factor.\rFinally, and for completeness, let me state the regularized learning\rrule for the biases.  This is, of course, exactly the same as in the\runregularized case (c.f. Equation(21)\\begin{eqnarray}  \r  b_l & \\rightarrow & b_l' = b_l-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l} \\nonumber\\end{eqnarray}),\\begin{eqnarray}\r  b \\rightarrow b - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\r\\tag{94}\\end{eqnarray}\rwhere the sum is over training examples $x$ in the mini-batch.Let's see how regularization changes the performance of our neural\rnetwork. We'll use a network with $30$ hidden neurons, a mini-batch\rsize of $10$, a learning rate of $0.5$, and the cross-entropy cost\rfunction.  However, this time we'll use a regularization parameter of\r$\\lambda = 0.1$.  Note that in the code, we use the variable namelmbda, becauselambdais a reserved word in Python, with\ran unrelated meaning.  I've also used thetest_dataagain, not\rthevalidation_data.  Strictly speaking, we should use thevalidation_data, for all the reasons we discussed earlier.  But\rI decided to use thetest_databecause it makes the results\rmore directly comparable with our earlier, unregularized results.  You\rcan easily change the code to use thevalidation_datainstead,\rand you'll find that it gives similar results.>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data[:1000],400,10,0.5,...evaluation_data=test_data,lmbda=0.1,...monitor_evaluation_cost=True,monitor_evaluation_accuracy=True,...monitor_training_cost=True,monitor_training_accuracy=True)The cost on the training data decreases over the whole time, much as\rit did in the earlier, unregularized case**This and the next\r  two graphs were produced with the programoverfitting.py.:But this time the accuracy on thetest_datacontinues to\rincrease for the entire 400 epochs:Clearly, the use of regularization has suppressed overfitting.  What's\rmore, the accuracy is considerably higher, with a peak classification\raccuracy of $87.1$ percent, compared to the peak of $82.27$ percent\robtained in the unregularized case.  Indeed, we could almost certainly\rget considerably better results by continuing to train past 400\repochs. It seems that, empirically, regularization is causing our\rnetwork to generalize better, and considerably reducing the effects of\roverfitting.What happens if we move out of the artificial environment of just\rhaving 1,000 training images, and return to the full 50,000 image\rtraining set?  Of course, we've seen already that overfitting is much\rless of a problem with the full 50,000 images.  Does regularization\rhelp any further?  Let's keep the hyper-parameters the same as before\r- $30$ epochs, learning rate $0.5$, mini-batch size of $10$.\rHowever, we need to modify the regularization parameter.  The reason\ris because the size $n$ of the training set has changed from $n =\r1,000$ to $n = 50,000$, and this changes the weight decay factor $1 -\r\\frac{\\eta \\lambda}{n}$.  If we continued to use $\\lambda = 0.1$ that\rwould mean much less weight decay, and thus much less of a\rregularization effect.  We compensate by changing to $\\lambda = 5.0$.Okay, let's train our network, stopping first to re-initialize the\rweights:>>>net.large_weight_initializer()>>>net.SGD(training_data,30,10,0.5,...evaluation_data=test_data,lmbda=5.0,...monitor_evaluation_accuracy=True,monitor_training_accuracy=True)We obtain the results:There's lots of good news here.  First, our classification accuracy on\rthe test data is up, from $95.49$ percent when running unregularized,\rto $96.49$ percent.  That's a big improvement.  Second, we can see\rthat the gap between results on the training and test data is much\rnarrower than before, running at under a percent.  That's still a\rsignificant gap, but we've obviously made substantial progress\rreducing overfitting.Finally, let's see what test classification accuracy we get when we\ruse 100 hidden neurons and a regularization parameter of $\\lambda =\r5.0$. I won't go through a detailed analysis of overfitting here, this\ris purely for fun, just to see how high an accuracy we can get when we\ruse our new tricks: the cross-entropy cost function and L2\rregularization.>>>net=network2.Network([784,100,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data,30,10,0.5,lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True)The final result is a classification accuracy of $97.92$ percent on\rthe validation data.  That's a big jump from the 30 hidden neuron\rcase.In fact, tuning just\ra little more, to run for 60 epochs at $\\eta = 0.1$ and $\\lambda =\r5.0$ we break the $98$ percent barrier, achieving $98.04$ percent\rclassification accuracy on the validation data.  Not bad for what\rturns out to be 152 lines of code!I've described regularization as a way to reduce overfitting and to\rincrease classification accuracies.  In fact, that's not the only\rbenefit.  Empirically, when doing multiple runs of our MNIST networks,\rbut with different (random) weight initializations, I've found that\rthe unregularized runs will occasionally get \"stuck\", apparently\rcaught in local minima of the cost function.  The result is that\rdifferent runs sometimes provide quite different results.  By\rcontrast, the regularized runs have provided much more easily\rreplicable results.Why is this going on?  Heuristically, if the cost function is\runregularized, then the length of the weight vector is likely to grow,\rall other things being equal.  Over time this can lead to the weight\rvector being very large indeed.  This can cause the weight vector to\rget stuck pointing in more or less the same direction, since changes\rdue to gradient descent only make tiny changes to the direction, when\rthe length is long.  I believe this phenomenon is making it hard for\rour learning algorithm to properly explore the weight space, and\rconsequently harder to find good minima of the cost function.Why does regularization help reduce overfitting?We've seen empirically that regularization helps reduce overfitting.\rThat's encouraging but, unfortunately, it's not obvious why\rregularization helps!  A standard story people tell to explain what's\rgoing on is along the following lines: smaller weights are, in some\rsense, lower complexity, and so provide a simpler and more powerful\rexplanation for the data, and should thus be preferred.  That's a\rpretty terse story, though, and contains several elements that perhaps\rseem dubious or mystifying.  Let's unpack the story and examine it\rcritically.  To do that, let's suppose we have a simple data set for\rwhich we wish to build a model:Implicitly, we're studying some real-world phenomenon here, with $x$\rand $y$ representing real-world data.  Our goal is to build a model\rwhich lets us predict $y$ as a function of $x$.  We could try using\rneural networks to build such a model, but I'm going to do something\reven simpler: I'll try to model $y$ as a polynomial in $x$.  I'm doing\rthis instead of using neural nets because using polynomials will make\rthings particularly transparent.  Once we've understood the polynomial\rcase, we'll translate to neural networks.  Now, there are ten points\rin the graph above, which means we can find a unique $9$th-order\rpolynomial $y = a_0 x^9 + a_1 x^8 + \\ldots + a_9$ which fits the data\rexactly.  Here's the graph of that polynomial**I won't show\r  the coefficients explicitly, although they are easy to find using a\r  routine such as Numpy'spolyfit.  You can view the exact form\r  of the polynomial in thesource code\r    for the graphif you're curious.  It's the functionp(x)defined starting on line 14 of the program which produces the\r  graph.:That provides an exact fit.  But we can also get a good fit using the\rlinear model $y = 2x$:Which of these is the better model?  Which is more likely to be true?\rAnd which model is more likely to generalize well to other examples of\rthe same underlying real-world phenomenon?These are difficult questions.  In fact, we can't determine with\rcertainty the answer to any of the above questions, without much more\rinformation about the underlying real-world phenomenon.  But let's\rconsider two possibilities: (1) the $9$th order polynomial is, in\rfact, the model which truly describes the real-world phenomenon, and\rthe model will therefore generalize perfectly; (2) the correct model\ris $y = 2x$, but there's a little additional noise due to, say,\rmeasurement error, and that's why the model isn't an exact fit.It's nota prioripossible to say which of these two\rpossibilities is correct.  (Or, indeed, if some third possibility\rholds).  Logically, either could be true.  And it's not a trivial\rdifference.  It's true that on the data provided there's only a small\rdifference between the two models.  But suppose we want to predict the\rvalue of $y$ corresponding to some large value of $x$, much larger\rthan any shown on the graph above.  If we try to do that there will be\ra dramatic difference between the predictions of the two models, as\rthe $9$th order polynomial model comes to be dominated by the $x^9$\rterm, while the linear model remains, well, linear.One point of view is to say that in science we should go with the\rsimpler explanation, unless compelled not to.  When we find a simple\rmodel that seems to explain many data points we are tempted to shout\r\"Eureka!\"  After all, it seems unlikely that a simple explanation\rshould occur merely by coincidence.  Rather, we suspect that the model\rmust be expressing some underlying truth about the phenomenon.  In the\rcase at hand, the model $y = 2x+{\\rm noise}$ seems much simpler than\r$y = a_0 x^9 + a_1 x^8 + \\ldots$.  It would be surprising if that\rsimplicity had occurred by chance, and so we suspect that $y = 2x+{\\rm\r  noise}$ expresses some underlying truth.  In this point of view, the\r9th order model is really just learning the effects of local\rnoise. And so while the 9th order model works perfectly for these\rparticular data points, the model will fail to generalize to other\rdata points, and the noisy linear model will have greater predictive\rpower.Let's see what this point of view means for neural networks.  Suppose\rour network mostly has small weights, as will tend to happen in a\rregularized network.  The smallness of the weights means that the\rbehaviour of the network won't change too much if we change a few\rrandom inputs here and there.  That makes it difficult for a\rregularized network to learn the effects of local noise in the data.\rThink of it as a way of making it so single pieces of evidence don't\rmatter too much to the output of the network.  Instead, a regularized\rnetwork learns to respond to types of evidence which are seen often\racross the training set.  By contrast, a network with large weights\rmay change its behaviour quite a bit in response to small changes in\rthe input.  And so an unregularized network can use large weights to\rlearn a complex model that carries a lot of information about the\rnoise in the training data.  In a nutshell, regularized networks are\rconstrained to build relatively simple models based on patterns seen\roften in the training data, and are resistant to learning\rpeculiarities of the noise in the training data.  The hope is that\rthis will force our networks to do real learning about the phenomenon\rat hand, and to generalize better from what they learn.With that said, this idea of preferring simpler explanation should\rmake you nervous.  People sometimes refer to this idea as \"Occam's\rRazor\", and will zealously apply it as though it has the status of\rsome general scientific principle.  But, of course, it's not a general\rscientific principle.  There is noa priorilogical reason to\rprefer simple explanations over more complex explanations.  Indeed,\rsometimes the more complex explanation turns out to be correct.Let me describe two examples where more complex explanations have\rturned out to be correct.  In the 1940s the physicist Marcel Schein\rannounced the discovery of a new particle of nature.  The company he\rworked for, General Electric, was ecstatic, and publicized the\rdiscovery widely.  But the physicist Hans Bethe was skeptical.  Bethe\rvisited Schein, and looked at the plates showing the tracks of\rSchein's new particle.  Schein showed Bethe plate after plate, but on\reach plate Bethe identified some problem that suggested the data\rshould be discarded.  Finally, Schein showed Bethe a plate that looked\rgood.  Bethe said it might just be a statistical fluke.  Schein:\r\"Yes, but the chance that this would be statistics, even according to\ryour own formula, is one in five.\"  Bethe: \"But we have already\rlooked at five plates.\"  Finally, Schein said: \"But on my plates,\reach one of the good plates, each one of the good pictures, you\rexplain by a different theory, whereas I have one hypothesis that\rexplains all the plates, that they are [the new particle].\"  Bethe\rreplied: \"The sole difference between your and my explanations is\rthat yours is wrong and all of mine are right.  Your single\rexplanation is wrong, and all of my multiple explanations are right.\"\rSubsequent work confirmed that Nature agreed with Bethe, and Schein's\rparticle is no more**The story is related by the physicist\r  Richard Feynman in aninterviewwith the historian Charles Weiner..As a second example, in 1859 the astronomer Urbain Le Verrier observed\rthat the orbit of the planet Mercury doesn't have quite the shape that\rNewton's theory of gravitation says it should have.  It was a tiny,\rtiny deviation from Newton's theory, and several of the explanations\rproferred at the time boiled down to saying that Newton's theory was\rmore or less right, but needed a tiny alteration.  In 1916, Einstein\rshowed that the deviation could be explained very well using his\rgeneral theory of relativity, a theory radically different to\rNewtonian gravitation, and based on much more complex mathematics.\rDespite that additional complexity, today it's accepted that\rEinstein's explanation is correct, and Newtonian gravity, even in its\rmodified forms, is wrong.  This is in part because we now know that\rEinstein's theory explains many other phenomena which Newton's theory\rhas difficulty with.  Furthermore, and even more impressively,\rEinstein's theory accurately predicts several phenomena which aren't\rpredicted by Newtonian gravity at all. But these impressive qualities\rweren't entirely obvious in the early days.  If one had judged merely\ron the grounds of simplicity, then some modified form of Newton's\rtheory would arguably have been more attractive.There are three morals to draw from these stories.  First, it can be\rquite a subtle business deciding which of two explanations is truly\r\"simpler\".  Second, even if we can make such a judgment, simplicity\ris a guide that must be used with great caution!  Third, the true test\rof a model is not simplicity, but rather how well it does in\rpredicting new phenomena, in new regimes of behaviour.With that said, and keeping the need for caution in mind, it's an\rempirical fact that regularized neural networks usually generalize\rbetter than unregularized networks.  And so through the remainder of\rthe book we will make frequent use of regularization.  I've included\rthe stories above merely to help convey why no-one has yet developed\ran entirely convincing theoretical explanation for why regularization\rhelps networks generalize.  Indeed, researchers continue to write\rpapers where they try different approaches to regularization, compare\rthem to see which works better, and attempt to understand why different\rapproaches work better or worse.  And so you can view regularization\ras something of a kludge.  While it often helps, we don't have an\rentirely satisfactory systematic understanding of what's going on,\rmerely incomplete heuristics and rules of thumb.There's a deeper set of issues here, issues which go to the heart of\rscience.  It's the question of how we generalize.  Regularization may\rgive us a computational magic wand that helps our networks generalize\rbetter, but it doesn't give us a principled understanding of how\rgeneralization works, nor of what the best approach is**These\r  issues go back to theproblem\r    of induction, famously discussed by the Scottish philosopher\r  David Hume in\"An\r    Enquiry Concerning Human Understanding\"(1748).  The problem of\r  induction has been given a modern machine learning form in the\r  no-free lunch theorem\r  (link)\r  of David Wolpert and William Macready (1997)..This is particularly galling because in everyday life, we humans\rgeneralize phenomenally well.  Shown just a few images of an elephant\ra child will quickly learn to recognize other elephants.  Of course,\rthey may occasionally make mistakes, perhaps confusing a rhinoceros\rfor an elephant, but in general this process works remarkably\raccurately.  So we have a system - the human brain - with a huge\rnumber of free parameters.  And after being shown just one or a few\rtraining images that system learns to generalize to other images.  Our\rbrains are, in some sense, regularizing amazingly well!  How do we do\rit?  At this point we don't know.  I expect that in years to come we\rwill develop more powerful techniques for regularization in artificial\rneural networks, techniques that will ultimately enable neural nets to\rgeneralize well even from small data sets.In fact, our networks already generalize better than one mighta\r  prioriexpect.  A network with 100 hidden neurons has nearly 80,000\rparameters.  We have only 50,000 images in our training data.  It's\rlike trying to fit an 80,000th degree polynomial to 50,000 data\rpoints.  By all rights, our network should overfit terribly.  And yet,\ras we saw earlier, such a network actually does a pretty good job\rgeneralizing.  Why is that the case?  It's not well understood.  It\rhas been conjectured**InGradient-Based\r    Learning Applied to Document Recognition, by Yann LeCun,\r  Léon Bottou, Yoshua Bengio, and Patrick Haffner\r  (1998).that \"the dynamics of gradient descent learning in\rmultilayer nets has a `self-regularization' effect\".  This is\rexceptionally fortunate, but it's also somewhat disquieting that we\rdon't understand why it's the case.  In the meantime, we will adopt\rthe pragmatic approach and use regularization whenever we can.  Our\rneural networks will be the better for it.Let me conclude this section by returning to a detail which I left\runexplained earlier: the fact that L2 regularizationdoesn'tconstrain the biases.  Of course, it would be easy to modify the\rregularization procedure to regularize the biases.  Empirically, doing\rthis often doesn't change the results very much, so to some extent\rit's merely a convention whether to regularize the biases or not.\rHowever, it's worth noting that having a large bias doesn't make a\rneuron sensitive to its inputs in the same way as having large\rweights.  And so we don't need to worry about large biases enabling\rour network to learn the noise in our training data.  At the same\rtime, allowing large biases gives our networks more flexibility in\rbehaviour - in particular, large biases make it easier for neurons\rto saturate, which is sometimes desirable.  For these reasons we don't\rusually include bias terms when regularizing.Other techniques for regularizationThere are many regularization techniques other than L2 regularization.\rIn fact, so many techniques have been developed that I can't possibly\rsummarize them all.  In this section I briefly describe three other\rapproaches to reducing overfitting: L1 regularization, dropout, and\rartificially increasing the training set size.  We won't go into\rnearly as much depth studying these techniques as we did earlier.\rInstead, the purpose is to get familiar with the main ideas, and to\rappreciate something of the diversity of regularization techniques\ravailable.L1 regularization:In this approach we modify the\runregularized cost function by adding the sum of the absolute values\rof the weights:\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{n} \\sum_w |w|.\r\\tag{95}\\end{eqnarray}Intuitively, this is similar to L2 regularization, penalizing large\rweights, and tending to make the network prefer small weights.  Of\rcourse, the L1 regularization term isn't the same as the L2\rregularization term, and so we shouldn't expect to get exactly the\rsame behaviour.  Let's try to understand how the behaviour of a\rnetwork trained using L1 regularization differs from a network trained\rusing L2 regularization.To do that, we'll look at the partial derivatives of the cost\rfunction.  Differentiating(95)\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{n} \\sum_w |w| \\nonumber\\end{eqnarray}we obtain:\\begin{eqnarray}  \\frac{\\partial C}{\\partial\r    w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\, {\\rm\r    sgn}(w),\r\\tag{96}\\end{eqnarray}where ${\\rm sgn}(w)$ is the sign of $w$, that is, $+1$ if $w$ is\rpositive, and $-1$ if $w$ is negative.  Using this expression, we can\reasily modify backpropagation to do stochastic gradient descent using\rL1 regularization.  The resulting update rule for an L1 regularized\rnetwork is\\begin{eqnarray}  w \\rightarrow w' =\r  w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w) - \\eta \\frac{\\partial\r    C_0}{\\partial w},\r\\tag{97}\\end{eqnarray}where, as per usual, we can estimate $\\partial C_0 / \\partial w$ using\ra mini-batch average, if we wish.  Compare that to the update rule for\rL2 regularization (c.f. Equation(93)\\begin{eqnarray} \r  w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\r  \\sum_x \\frac{\\partial C_x}{\\partial w},  \\nonumber\\end{eqnarray}),\\begin{eqnarray}\r  w \\rightarrow w' = w\\left(1 - \\frac{\\eta \\lambda}{n} \\right)\r  - \\eta \\frac{\\partial C_0}{\\partial w}.\r\\tag{98}\\end{eqnarray}\rIn both expressions the effect of regularization is to shrink the\rweights.  This accords with our intuition that both kinds of\rregularization penalize large weights.  But the way the weights shrink\ris different.  In L1 regularization, the weights shrink by a constant\ramount toward $0$.  In L2 regularization, the weights shrink by an\ramount which is proportional to $w$.  And so when a particular weight\rhas a large magnitude, $|w|$, L1 regularization shrinks the weight\rmuch less than L2 regularization does.  By contrast, when $|w|$ is\rsmall, L1 regularization shrinks the weight much more than L2\rregularization.  The net result is that L1 regularization tends to\rconcentrate the weight of the network in a relatively small number of\rhigh-importance connections, while the other weights are driven toward\rzero.I've glossed over an issue in the above discussion, which is that the\rpartial derivative $\\partial C / \\partial w$ isn't defined when $w =\r0$.  The reason is that the function $|w|$ has a sharp \"corner\" at\r$w = 0$, and so isn't differentiable at that point.  That's okay,\rthough.  What we'll do is just apply the usual (unregularized) rule\rfor stochastic gradient descent when $w = 0$.  That should be okay -\rintuitively, the effect of regularization is to shrink weights, and\robviously it can't shrink a weight which is already $0$.  To put it\rmore precisely, we'll use Equations(96)\\begin{eqnarray}  \\frac{\\partial C}{\\partial\r    w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\, {\\rm\r    sgn}(w) \\nonumber\\end{eqnarray}and(97)\\begin{eqnarray}  w \\rightarrow w' =\r  w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w) - \\eta \\frac{\\partial\r    C_0}{\\partial w} \\nonumber\\end{eqnarray}with the convention that $\\mbox{sgn}(0) = 0$.\rThat gives a nice, compact rule for doing stochastic gradient descent\rwith L1 regularization.Dropout:Dropout is a radically different technique for\rregularization.  Unlike L1 and L2 regularization, dropout doesn't rely\ron modifying the cost function.  Instead, in dropout we modify the\rnetwork itself.  Let me describe the basic mechanics of how dropout\rworks, before getting into why it works, and what the results are.Suppose we're trying to train a network:In particular, suppose we have a training input $x$ and corresponding\rdesired output $y$.  Ordinarily, we'd train by forward-propagating $x$\rthrough the network, and then backpropagating to determine the\rcontribution to the gradient.  With dropout, this process is modified.\rWe start by randomly (and temporarily) deleting half the hidden\rneurons in the network, while leaving the input and output neurons\runtouched.  After doing this, we'll end up with a network along the\rfollowing lines.  Note that the dropout neurons, i.e., the neurons\rwhich have been temporarily deleted, are still ghosted in:We forward-propagate the input $x$ through the modified network, and\rthen backpropagate the result, also through the modified network.\rAfter doing this over a mini-batch of examples, we update the\rappropriate weights and biases.  We then repeat the process, first\rrestoring the dropout neurons, then choosing a new random subset of\rhidden neurons to delete, estimating the gradient for a different\rmini-batch, and updating the weights and biases in the network.By repeating this process over and over, our network will learn a set\rof weights and biases.  Of course, those weights and biases will have\rbeen learnt under conditions in which half the hidden neurons were\rdropped out.  When we actually run the full network that means that\rtwice as many hidden neurons will be active.  To compensate for that,\rwe halve the weights outgoing from the hidden neurons.This dropout procedure may seem strange andad hoc.  Why would\rwe expect it to help with regularization?  To explain what's going on,\rI'd like you to briefly stop thinking about dropout, and instead\rimagine training neural networks in the standard way (no dropout).  In\rparticular, imagine we train several different neural networks, all\rusing the same training data.  Of course, the networks may not start\rout identical, and as a result after training they may sometimes give\rdifferent results.  When that happens we could use some kind of\raveraging or voting scheme to decide which output to accept.  For\rinstance, if we have trained five networks, and three of them are\rclassifying a digit as a \"3\", then it probably really is a \"3\".\rThe other two networks are probably just making a mistake.  This kind\rof averaging scheme is often found to be a powerful (though expensive)\rway of reducing overfitting.  The reason is that the different\rnetworks may overfit in different ways, and averaging may help\reliminate that kind of overfitting.What's this got to do with dropout?  Heuristically, when we dropout\rdifferent sets of neurons, it's rather like we're training different\rneural networks.  And so the dropout procedure is like averaging the\reffects of a very large number of different networks.  The different\rnetworks will overfit in different ways, and so, hopefully, the net\reffect of dropout will be to reduce overfitting.A related heuristic explanation for dropout is given in one of the\rearliest papers to use the\rtechnique**ImageNet\r    Classification with Deep Convolutional Neural Networks, by Alex\r  Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).: \"This\rtechnique reduces complex co-adaptations of neurons, since a neuron\rcannot rely on the presence of particular other neurons. It is,\rtherefore, forced to learn more robust features that are useful in\rconjunction with many different random subsets of the other neurons.\"\rIn other words, if we think of our network as a model which is making\rpredictions, then we can think of dropout as a way of making sure that\rthe model is robust to the loss of any individual piece of evidence.\rIn this, it's somewhat similar to L1 and L2 regularization, which tend\rto reduce weights, and thus make the network more robust to losing any\rindividual connection in the network.Of course, the true measure of dropout is that it has been very\rsuccessful in improving the performance of neural networks.  The\roriginal\rpaper**Improving\r    neural networks by preventing co-adaptation of feature detectorsby Geoffrey Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\r  Sutskever, and Ruslan Salakhutdinov (2012).  Note that the paper\r  discusses a number of subtleties that I have glossed over in this\r  brief introduction.introducing the technique applied it to many\rdifferent tasks. For us, it's of particular interest that they applied\rdropout to MNIST digit classification, using a vanilla feedforward\rneural network along lines similar to those we've been considering.\rThe paper noted that the best result anyone had achieved up to that\rpoint using such an architecture was $98.4$ percent classification\raccuracy on the test set.  They improved that to $98.7$ percent\raccuracy using a combination of dropout and a modified form of L2\rregularization.  Similarly impressive results have been obtained for\rmany other tasks, including problems in image and speech recognition,\rand natural language processing.  Dropout has been especially useful\rin training large, deep networks, where the problem of overfitting is\roften acute.Artificially expanding the training data:We saw earlier that\rour MNIST classification accuracy dropped down to percentages in the\rmid-80s when we used only 1,000 training images.  It's not surprising\rthat this is the case, since less training data means our network will\rbe exposed to fewer variations in the way human beings write digits.\rLet's try training our 30 hidden neuron network with a variety of\rdifferent training data set sizes, to see how performance varies.  We\rtrain using a mini-batch size of 10, a learning rate $\\eta = 0.5$, a\rregularization parameter $\\lambda = 5.0$, and the cross-entropy cost\rfunction.  We will train for 30 epochs when the full training data set\ris used, and scale up the number of epochs proportionally when smaller\rtraining sets are used.  To ensure the weight decay factor remains the\rsame across training sets, we will use a regularization parameter of\r$\\lambda = 5.0$ when the full training data set is used, and scale\rdown $\\lambda$ proportionally when smaller training sets are\rused**This and the next two graph are produced with the\r  programmore_data.py..As you can see, the classification accuracies improve considerably as\rwe use more training data.  Presumably this improvement would continue\rstill further if more data was available.  Of course, looking at the\rgraph above it does appear that we're getting near saturation.\rSuppose, however, that we redo the graph with the training set size\rplotted logarithmically:It seems clear that the graph is still going up toward the end.  This\rsuggests that if we used vastly more training data - say, millions\ror even billions of handwriting samples, instead of just 50,000 -\rthen we'd likely get considerably better performance, even from this\rvery small network.Obtaining more training data is a great idea. Unfortunately, it can be\rexpensive, and so is not always possible in practice.  However,\rthere's another idea which can work nearly as well, and that's to\rartificially expand the training data.  Suppose, for example, that we\rtake an MNIST training image of a five,and rotate it by a small amount, let's say 15 degrees:It's still recognizably the same digit.  And yet at the pixel level\rit's quite different to any image currently in the MNIST training\rdata.  It's conceivable that adding this image to the training data\rmight help our network learn more about how to classify digits.\rWhat's more, obviously we're not limited to adding just this one\rimage.  We can expand our training data by makingmanysmall\rrotations ofallthe MNIST training images, and then using the\rexpanded training data to improve our network's performance.This idea is very powerful and has been widely used.  Let's look at\rsome of the results from a\rpaper**Best\r    Practices for Convolutional Neural Networks Applied to Visual\r    Document Analysis, by Patrice Simard, Dave Steinkraus, and John\r  Platt (2003).which applied several variations of the idea to\rMNIST.  One of the neural network architectures they considered was\ralong similar lines to what we've been using, a feedforward network\rwith 800 hidden neurons and using the cross-entropy cost function.\rRunning the network with the standard MNIST training data they\rachieved a classification accuracy of 98.4 percent on their test set.\rBut then they expanded the training data, using not just rotations, as\rI described above, but also translating and skewing the images.  By\rtraining on the expanded data set they increased their network's\raccuracy to 98.9 percent.  They also experimented with what they\rcalled \"elastic distortions\", a special type of image distortion\rintended to emulate the random oscillations found in hand muscles.  By\rusing the elastic distortions to expand the data they achieved an even\rhigher accuracy, 99.3 percent.  Effectively, they were broadening the\rexperience of their network by exposing it to the sort of variations\rthat are found in real handwriting.Variations on this idea can be used to improve performance on many\rlearning tasks, not just handwriting recognition.  The general\rprinciple is to expand the training data by applying operations that\rreflect real-world variation.  It's not difficult to think of ways of\rdoing this.  Suppose, for example, that you're building a neural\rnetwork to do speech recognition.  We humans can recognize speech even\rin the presence of distortions such as background noise.  And so you\rcan expand your data by adding background noise.  We can also\rrecognize speech if it's sped up or slowed down. So that's another way\rwe can expand the training data.  These techniques are not always used\r- for instance, instead of expanding the training data by adding\rnoise, it may well be more efficient to clean up the input to the\rnetwork by first applying a noise reduction filter.  Still, it's worth\rkeeping the idea of expanding the training data in mind, and looking\rfor opportunities to apply it.ExerciseAs discussed above, one way of expanding the MNIST training data\r  is to use small rotations of training images.  What's a problem that\r  might occur if we allow arbitrarily large rotations of training\r  images?An aside on big data and what it means to compare\r  classification accuracies:Let's look again at how our neural\rnetwork's accuracy varies with training set size:Suppose that instead of using a neural network we use some other\rmachine learning technique to classify digits.  For instance, let's\rtry using the support vector machines (SVM) which we met briefly back\rinChapter 1.  As was the case in Chapter 1,\rdon't worry if you're not familiar with SVMs, we don't need to\runderstand their details.  Instead, we'll use the SVM supplied by thescikit-learn library.  Here's\rhow SVM performance varies as a function of training set size.  I've\rplotted the neural net results as well, to make comparison\reasy**This graph was produced with the programmore_data.py(as were the last few graphs).:Probably the first thing that strikes you about this graph is that our\rneural network outperforms the SVM for every training set size.\rThat's nice, although you shouldn't read too much into it, since I\rjust used the out-of-the-box settings from scikit-learn's SVM, while\rwe've done a fair bit of work improving our neural network.  A more\rsubtle but more interesting fact about the graph is that if we train\rour SVM using 50,000 images then it actually has better performance\r(94.48 percent accuracy) than our neural network does when trained\rusing 5,000 images (93.24 percent accuracy).  In other words, more\rtraining data can sometimes compensate for differences in the machine\rlearning algorithm used.Something even more interesting can occur.  Suppose we're trying to\rsolve a problem using two machine learning algorithms, algorithm A and\ralgorithm B.  It sometimes happens that algorithm A will outperform\ralgorithm B with one set of training data, while algorithm B will\routperform algorithm A with a different set of training data.  We\rdon't see that above - it would require the two graphs to cross -\rbut it does happen**Striking examples may be found inScaling to very\r    very large corpora for natural language disambiguation, by\r  Michele Banko and Eric Brill (2001)..  The correct response to the\rquestion \"Is algorithm A better than algorithm B?\" is really: \"What\rtraining data set are you using?\"All this is a caution to keep in mind, both when doing development,\rand when reading research papers.  Many papers focus on finding new\rtricks to wring out improved performance on standard benchmark data\rsets.  \"Our whiz-bang technique gave us an improvement of X percent\ron standard benchmark Y\" is a canonical form of research claim.  Such\rclaims are often genuinely interesting, but they must be understood as\rapplying only in the context of the specific training data set used.\rImagine an alternate history in which the people who originally\rcreated the benchmark data set had a larger research grant.  They\rmight have used the extra money to collect more training data.  It's\rentirely possible that the \"improvement\" due to the whiz-bang\rtechnique would disappear on a larger data set.  In other words, the\rpurported improvement might be just an accident of history.  The\rmessage to take away, especially in practical applications, is that\rwhat we want is both better algorithmsandbetter training\rdata.  It's fine to look for better algorithms, but make sure you're\rnot focusing on better algorithms to the exclusion of easy wins\rgetting more or better training data.Problem(Research problem)How do our machine learning algorithms\r  perform in the limit of very large data sets?  For any given\r  algorithm it's natural to attempt to define a notion of asymptotic\r  performance in the limit of truly big data. A quick-and-dirty\r  approach to this problem is to simply try fitting curves to graphs\r  like those shown above, and then to extrapolate the fitted curves\r  out to infinity.  An objection to this approach is that different\r  approaches to curve fitting will give different notions of asymptotic\r  performance.  Can you find a principled justification for fitting to\r  some particular class of curves?  If so, compare the asymptotic\r  performance of several different machine learning algorithms.Summing up:We've now completed our dive into overfitting and\rregularization.  Of course, we'll return again to the issue.  As I've\rmentioned several times, overfitting is a major problem in neural\rnetworks, especially as computers get more powerful, and we have the\rability to train larger networks.  As a result there's a pressing need\rto develop powerful regularization techniques to reduce overfitting,\rand this is an extremely active area of current work.Weight initializationWhen we create our neural networks, we have to make choices for the\rinitial weights and biases.  Up to now, we've been choosing them\raccording to a prescription which I discussed only brieflyback in Chapter 1.  Just to\rremind you, that prescription was to choose both the weights and\rbiases using independent Gaussian random variables, normalized to have\rmean $0$ and standard deviation $1$.  While this approach has worked\rwell, it was quitead hoc, and it's worth revisiting to see if\rwe can find a better way of setting our initial weights and biases,\rand perhaps help our neural networks learn faster.It turns out that we can do quite a bit better than initializing with\rnormalized Gaussians.  To see why, suppose we're working with a\rnetwork with a large number - say $1,000$ - of input neurons.  And\rlet's suppose we've used normalized Gaussians to initialize the\rweights connecting to the first hidden layer.  For now I'm going to\rconcentrate specifically on the weights connecting the input neurons\rto the first neuron in the hidden layer, and ignore the rest of the\rnetwork:We'll suppose for simplicity that we're trying to train using a\rtraining input $x$ in which half the input neurons are on, i.e., set\rto $1$, and half the input neurons are off, i.e., set to $0$.  The\rargument which follows applies more generally, but you'll get the gist\rfrom this special case.  Let's consider the weighted sum $z = \\sum_j\rw_j x_j+b$ of inputs to our hidden neuron.  $500$ terms in this sum\rvanish, because the corresponding input $x_j$ is zero.  And so $z$ is\ra sum over a total of $501$ normalized Gaussian random variables,\raccounting for the $500$ weight terms and the $1$ extra bias term.\rThus $z$ is itself distributed as a Gaussian with mean zero and\rstandard deviation $\\sqrt{501} \\approx 22.4$.  That is, $z$ has a very\rbroad Gaussian distribution, not sharply peaked at all:In particular, we can see from this graph that it's quite likely that\r$|z|$ will be pretty large, i.e., either $z \\gg 1$ or $z \\ll -1$.  If\rthat's the case then the output $\\sigma(z)$ from the hidden neuron\rwill be very close to either $1$ or $0$.  That means our hidden neuron\rwill have saturated.  And when that happens, as we know, making small\rchanges in the weights will make only absolutely miniscule changes in\rthe activation of our hidden neuron.  That miniscule change in the\ractivation of the hidden neuron will, in turn, barely affect the rest\rof the neurons in the network at all, and we'll see a correspondingly\rminiscule change in the cost function.  As a result, those weights\rwill only learn very slowly when we use the gradient descent\ralgorithm**We discussed this in more detail in Chapter 2,\r  where we used theequations\r    of backpropagationto show that weights input to saturated\r  neurons learned slowly..  It's similar to the problem we discussed\rearlier in this chapter, in which output neurons which saturated on\rthe wrong value caused learning to slow down.  We addressed that\rearlier problem with a clever choice of cost function.  Unfortunately,\rwhile that helped with saturated output neurons, it does nothing at\rall for the problem with saturated hidden neurons.I've been talking about the weights input to the first hidden layer.\rOf course, similar arguments apply also to later hidden layers: if the\rweights in later hidden layers are initialized using normalized\rGaussians, then activations will often be very close to $0$ or $1$,\rand learning will proceed very slowly.Is there some way we can choose better initializations for the weights\rand biases, so that we don't get this kind of saturation, and so avoid\ra learning slowdown?  Suppose we have a neuron with $n_{\\rm in}$ input\rweights.  Then we shall initialize those weights as Gaussian random\rvariables with mean $0$ and standard deviation $1/\\sqrt{n_{\\rm in}}$.\rThat is, we'll squash the Gaussians down, making it less likely that\rour neuron will saturate.  We'll continue to choose the bias as a\rGaussian with mean $0$ and standard deviation $1$, for reasons I'll\rreturn to in a moment.  With these choices, the weighted sum $z =\r\\sum_j w_j x_j + b$ will again be a Gaussian random variable with mean\r$0$, but it'll be much more sharply peaked than it was before.\rSuppose, as we did earlier, that $500$ of the inputs are zero and\r$500$ are $1$.  Then it's easy to show (see the exercise below) that\r$z$ has a Gaussian distribution with mean $0$ and standard deviation\r$\\sqrt{3/2} = 1.22\\ldots$.  This is much more sharply peaked than\rbefore, so much so that even the graph below understates the\rsituation, since I've had to rescale the vertical axis, when compared\rto the earlier graph:Such a neuron is much less likely to saturate, and correspondingly\rmuch less likely to have problems with a learning slowdown.ExerciseVerify that the standard deviation of $z = \\sum_j w_j x_j + b$\r  in the paragraph above is $\\sqrt{3/2}$.  It may help to know that:\r  (a) the variance of a sum of independent random variables is the sum\r  of the variances of the individual random variables; and (b) the\r  variance is the square of the standard deviation.I stated above that we'll continue to initialize the biases as before,\ras Gaussian random variables with a mean of $0$ and a standard\rdeviation of $1$.  This is okay, because it doesn't make it too much\rmore likely that our neurons will saturate.  In fact, it doesn't much\rmatter how we initialize the biases, provided we avoid the problem\rwith saturation.  Some people go so far as to initialize all the\rbiases to $0$, and rely on gradient descent to learn appropriate\rbiases.  But since it's unlikely to make much difference, we'll\rcontinue with the same initialization procedure as before.Let's compare the results for both our old and new approaches to\rweight initialization, using the MNIST digit classification task.  As\rbefore, we'll use $30$ hidden neurons, a mini-batch size of $10$, a\rregularization parameter $\\lambda = 5.0$, and the cross-entropy cost\rfunction.  We will decrease the learning rate slightly from $\\eta =\r0.5$ to $0.1$, since that makes the results a little more easily\rvisible in the graphs.  We can train using the old method of weight\rinitialization:>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True)We can also train using the new approach to weight initialization.\rThis is actually even easier, sincenetwork2's default way of\rinitializing the weights is using this new approach.  That means we\rcan omit thenet.large_weight_initializer()call above:>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True)Plotting the results**The program used to generate this and\r  the next graph isweight_initialization.py.,\rwe obtain:In both cases, we end up with a classification accuracy somewhat over\r96 percent.  The final classification accuracy is almost exactly the\rsame in the two cases.  But the new initialization technique brings us\rthere much, much faster.  At the end of the first epoch of training\rthe old approach to weight initialization has a classification\raccuracy under 87 percent, while the new approach is already almost 93\rpercent.  What appears to be going on is that our new approach to\rweight initialization starts us off in a much better regime, which\rlets us get good results much more quickly.  The same phenomenon is\ralso seen if we plot results with $100$ hidden neurons:In this case, the two curves don't quite meet.  However, my\rexperiments suggest that with just a few more epochs of training (not\rshown) the accuracies become almost exactly the same.  So on the basis\rof these experiments it looks as though the improved weight\rinitialization only speeds up learning, it doesn't change the final\rperformance of our networks.  However, in Chapter 4 we'll see examples\rof neural networks where the long-run behaviour is significantly\rbetter with the $1/\\sqrt{n_{\\rm in}}$ weight initialization.  Thus\rit's not only the speed of learning which is improved, it's sometimes\ralso the final performance.The $1/\\sqrt{n_{\\rm in}}$ approach to weight initialization helps\rimprove the way our neural nets learn.  Other techniques for weight\rinitialization have also been proposed, many building on this basic\ridea.  I won't review the other approaches here, since $1/\\sqrt{n_{\\rm\r    in}}$ works well enough for our purposes.  If you're interested in\rlooking further, I recommend looking at the discussion on pages 14 and\r15 of a 2012 paper by Yoshua\rBengio**Practical\r    Recommendations for Gradient-Based Training of Deep\r    Architectures, by Yoshua Bengio (2012)., as well as the\rreferences therein.ProblemConnecting regularization and the improved method of weight\r  initializationL2 regularization sometimes automatically gives us\r  something similar to the new approach to weight initialization.\r  Suppose we are using the old approach to weight initialization.\r  Sketch a heuristic argument that: (1) supposing $\\lambda$ is not too\r  small, the first epochs of training will be dominated almost\r  entirely by weight decay; (2) provided $\\eta \\lambda \\ll n$ the\r  weights will decay by a factor of $\\exp(-\\eta \\lambda / m)$ per\r  epoch; and (3) supposing $\\lambda$ is not too large, the weight\r  decay will tail off when the weights are down to a size around\r  $1/\\sqrt{n}$, where $n$ is the total number of weights in the\r  network.  Argue that these conditions are all satisfied in the\r  examples graphed in this section.Handwriting recognition revisited: the codeLet's implement the ideas we've discussed in this chapter.  We'll\rdevelop a new program,network2.py,\rwhich is an improved version of the programnetwork.pywe developed inChapter\r  1.  If you haven't looked atnetwork.pyin a while then you\rmay find it helpful to spend a few minutes quickly reading over the\rearlier discussion.  It's only 74 lines of code, and is easily\runderstood.As was the case innetwork.py, the star ofnetwork2.pyis theNetworkclass, which we use to represent our neural\rnetworks.  We initialize an instance ofNetworkwith a list ofsizesfor the respective layers in the network, and a choice\rfor thecostto use, defaulting to the cross-entropy:classNetwork(object):def__init__(self,sizes,cost=CrossEntropyCost):self.num_layers=len(sizes)self.sizes=sizesself.default_weight_initializer()self.cost=costThe first couple of lines of the__init__method are the same\ras innetwork.py, and are pretty self-explanatory.  But the\rnext two lines are new, and we need to understand what they're doing\rin detail.Let's start by examining thedefault_weight_initializermethod. This makes use of ournew and\r  improved approachto weight initialization.  As we've seen, in that\rapproach the weights input to a neuron are initialized as Gaussian\rrandom variables with mean 0 and standard deviation $1$ divided by the\rsquare root of the number of connections input to the neuron.  Also in\rthis method we'll initialize the biases, using Gaussian random\rvariables with mean $0$ and standard deviation $1$.  Here's the code:defdefault_weight_initializer(self):self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)/np.sqrt(x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]To understand the code, it may help to recall thatnpis the\rNumpy library for doing linear algebra.  We'llimportNumpy at\rthe beginning of our program.  Also, notice that we don't initialize\rany biases for the first layer of neurons.  We avoid doing this\rbecause the first layer is an input layer, and so any biases would not\rbe used.  We did exactly the same thing innetwork.py.Complementing thedefault_weight_initializerwe'll also include\ralarge_weight_initializermethod.  This method initializes the\rweights and biases using the old approach from Chapter 1, with both\rweights and biases initialized as Gaussian random variables with mean\r$0$ and standard deviation $1$.  The code is, of course, only a tiny\rbit different from thedefault_weight_initializer:deflarge_weight_initializer(self):self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]I've included thelarge_weight_initializermethod mostly as a\rconvenience to make it easier to compare the results in this chapter\rto those in Chapter 1. I can't think of many practical situations\rwhere I would recommend using it!The second new thing inNetwork's__init__method is\rthat we now initialize acostattribute.  To understand how\rthat works, let's look at the class we use to represent the\rcross-entropy cost**If you're not familiar with Python's\r  static methods you can ignore the@staticmethoddecorators,\r  and just treatfnanddeltaas ordinary methods.  If\r  you're curious about details, all@staticmethoddoes is tell\r  the Python interpreter that the method which follows doesn't depend\r  on the object in any way.  That's whyselfisn't passed as a\r  parameter to thefnanddeltamethods.:classCrossEntropyCost(object):@staticmethoddeffn(a,y):returnnp.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))@staticmethoddefdelta(z,a,y):return(a-y)Let's break this down.  The first thing to observe is that even though\rthe cross-entropy is, mathematically speaking, a function, we've\rimplemented it as a Python class, not a Python function.  Why have I\rmade that choice?  The reason is that the cost plays two different\rroles in our network.  The obvious role is that it's a measure of how\rwell an output activation,a, matches the desired output,y.  This role is captured by theCrossEntropyCost.fnmethod.  (Note, by the way, that thenp.nan_to_numcall insideCrossEntropyCost.fnensures that Numpy deals correctly with the\rlog of numbers very close to zero.)  But there's also a second way the\rcost function enters our network.  Recall fromChapter\r  2that when running the backpropagation algorithm we need to\rcompute the network's output error, $\\delta^L$. The form of the output\rerror depends on the choice of cost function: different cost function,\rdifferent form for the output error.  For the cross-entropy the output\rerror is, as we saw in Equation(66)\\begin{eqnarray} \r    \\delta^L = a^L-y.\r   \\nonumber\\end{eqnarray},\\begin{eqnarray}\r  \\delta^L = a^L-y.\r\\tag{99}\\end{eqnarray}\rFor this reason we define a second method,CrossEntropyCost.delta, whose purpose is to tell our network\rhow to compute the output error.  And then we bundle these two methods\rup into a single class containing everything our networks need to know\rabout the cost function.In a similar way,network2.pyalso contains a class to\rrepresent the quadratic cost function.  This is included for\rcomparison with the results of Chapter 1, since going forward we'll\rmostly use the cross entropy.  The code is just below.  TheQuadraticCost.fnmethod is a straightforward computation of the\rquadratic cost associated to the actual output,a, and the\rdesired output,y.  The value returned byQuadraticCost.deltais based on the\rexpression(30)\\begin{eqnarray} \r  \\delta^L = (a^L-y) \\odot \\sigma'(z^L) \\nonumber\\end{eqnarray}for the output error for the\rquadratic cost, which we derived back in Chapter 2.classQuadraticCost(object):@staticmethoddeffn(a,y):return0.5*np.linalg.norm(a-y)**2@staticmethoddefdelta(z,a,y):return(a-y)*sigmoid_prime(z)We've now understood the main differences betweennetwork2.pyandnetwork.py.  It's all pretty simple stuff.  There are a\rnumber of smaller changes, which I'll discuss below, including the\rimplementation of L2 regularization.  Before getting to that, let's\rlook at the complete code fornetwork2.py.  You don't need to\rread all the code in detail, but it is worth understanding the broad\rstructure, and in particular reading the documentation strings, so you\runderstand what each piece of the program is doing.  Of course, you're\ralso welcome to delve as deeply as you wish!  If you get lost, you may\rwish to continue reading the prose below, and return to the code\rlater.  Anyway, here's the code:\"\"\"network2.py~~~~~~~~~~~~~~An improved version of network.py, implementing the stochasticgradient descent learning algorithm for a feedforward neural network.Improvements include the addition of the cross-entropy cost function,regularization, and better initialization of network weights.  Notethat I have focused on making the code simple, easily readable, andeasily modifiable.  It is not optimized, and omits many desirablefeatures.\"\"\"#### Libraries# Standard libraryimportjsonimportrandomimportsys# Third-party librariesimportnumpyasnp#### Define the quadratic and cross-entropy cost functionsclassQuadraticCost(object):@staticmethoddeffn(a,y):\"\"\"Return the cost associated with an output ``a`` and desired output``y``.\"\"\"return0.5*np.linalg.norm(a-y)**2@staticmethoddefdelta(z,a,y):\"\"\"Return the error delta from the output layer.\"\"\"return(a-y)*sigmoid_prime(z)classCrossEntropyCost(object):@staticmethoddeffn(a,y):\"\"\"Return the cost associated with an output ``a`` and desired output``y``.  Note that np.nan_to_num is used to ensure numericalstability.  In particular, if both ``a`` and ``y`` have a 1.0in the same slot, then the expression (1-y)*np.log(1-a)returns nan.  The np.nan_to_num ensures that that is convertedto the correct value (0.0).\"\"\"returnnp.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))@staticmethoddefdelta(z,a,y):\"\"\"Return the error delta from the output layer.  Note that theparameter ``z`` is not used by the method.  It is included inthe method's parameters in order to make the interfaceconsistent with the delta method for other cost classes.\"\"\"return(a-y)#### Main Network classclassNetwork(object):def__init__(self,sizes,cost=CrossEntropyCost):\"\"\"The list ``sizes`` contains the number of neurons in the respectivelayers of the network.  For example, if the list was [2, 3, 1]then it would be a three-layer network, with the first layercontaining 2 neurons, the second layer 3 neurons, and thethird layer 1 neuron.  The biases and weights for the networkare initialized randomly, using``self.default_weight_initializer`` (see docstring for thatmethod).\"\"\"self.num_layers=len(sizes)self.sizes=sizesself.default_weight_initializer()self.cost=costdefdefault_weight_initializer(self):\"\"\"Initialize each weight using a Gaussian distribution with mean 0and standard deviation 1 over the square root of the number ofweights connecting to the same neuron.  Initialize the biasesusing a Gaussian distribution with mean 0 and standarddeviation 1.Note that the first layer is assumed to be an input layer, andby convention we won't set any biases for those neurons, sincebiases are only ever used in computing the outputs from laterlayers.\"\"\"self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)/np.sqrt(x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]deflarge_weight_initializer(self):\"\"\"Initialize the weights using a Gaussian distribution with mean 0and standard deviation 1.  Initialize the biases using aGaussian distribution with mean 0 and standard deviation 1.Note that the first layer is assumed to be an input layer, andby convention we won't set any biases for those neurons, sincebiases are only ever used in computing the outputs from laterlayers.This weight and bias initializer uses the same approach as inChapter 1, and is included for purposes of comparison.  Itwill usually be better to use the default weight initializerinstead.\"\"\"self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]deffeedforward(self,a):\"\"\"Return the output of the network if ``a`` is input.\"\"\"forb,winzip(self.biases,self.weights):a=sigmoid(np.dot(w,a)+b)returnadefSGD(self,training_data,epochs,mini_batch_size,eta,lmbda=0.0,evaluation_data=None,monitor_evaluation_cost=False,monitor_evaluation_accuracy=False,monitor_training_cost=False,monitor_training_accuracy=False):\"\"\"Train the neural network using mini-batch stochastic gradientdescent.  The ``training_data`` is a list of tuples ``(x, y)``representing the training inputs and the desired outputs.  Theother non-optional parameters are self-explanatory, as is theregularization parameter ``lmbda``.  The method also accepts``evaluation_data``, usually either the validation or testdata.  We can monitor the cost and accuracy on either theevaluation data or the training data, by setting theappropriate flags.  The method returns a tuple containing fourlists: the (per-epoch) costs on the evaluation data, theaccuracies on the evaluation data, the costs on the trainingdata, and the accuracies on the training data.  All values areevaluated at the end of each training epoch.  So, for example,if we train for 30 epochs, then the first element of the tuplewill be a 30-element list containing the cost on theevaluation data at the end of each epoch. Note that the listsare empty if the corresponding flag is not set.\"\"\"ifevaluation_data:n_data=len(evaluation_data)n=len(training_data)evaluation_cost,evaluation_accuracy=[],[]training_cost,training_accuracy=[],[]forjinxrange(epochs):random.shuffle(training_data)mini_batches=[training_data[k:k+mini_batch_size]forkinxrange(0,n,mini_batch_size)]formini_batchinmini_batches:self.update_mini_batch(mini_batch,eta,lmbda,len(training_data))print\"Epoch%straining complete\"%jifmonitor_training_cost:cost=self.total_cost(training_data,lmbda)training_cost.append(cost)print\"Cost on training data: {}\".format(cost)ifmonitor_training_accuracy:accuracy=self.accuracy(training_data,convert=True)training_accuracy.append(accuracy)print\"Accuracy on training data: {} / {}\".format(accuracy,n)ifmonitor_evaluation_cost:cost=self.total_cost(evaluation_data,lmbda,convert=True)evaluation_cost.append(cost)print\"Cost on evaluation data: {}\".format(cost)ifmonitor_evaluation_accuracy:accuracy=self.accuracy(evaluation_data)evaluation_accuracy.append(accuracy)print\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data),n_data)printreturnevaluation_cost,evaluation_accuracy,\\training_cost,training_accuracydefupdate_mini_batch(self,mini_batch,eta,lmbda,n):\"\"\"Update the network's weights and biases by applying gradientdescent using backpropagation to a single mini batch.  The``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is thelearning rate, ``lmbda`` is the regularization parameter, and``n`` is the total size of the training data set.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]forx,yinmini_batch:delta_nabla_b,delta_nabla_w=self.backprop(x,y)nabla_b=[nb+dnbfornb,dnbinzip(nabla_b,delta_nabla_b)]nabla_w=[nw+dnwfornw,dnwinzip(nabla_w,delta_nabla_w)]self.weights=[(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nwforw,nwinzip(self.weights,nabla_w)]self.biases=[b-(eta/len(mini_batch))*nbforb,nbinzip(self.biases,nabla_b)]defbackprop(self,x,y):\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing thegradient for the cost function C_x.  ``nabla_b`` and``nabla_w`` are layer-by-layer lists of numpy arrays, similarto ``self.biases`` and ``self.weights``.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]# feedforwardactivation=xactivations=[x]# list to store all the activations, layer by layerzs=[]# list to store all the z vectors, layer by layerforb,winzip(self.biases,self.weights):z=np.dot(w,activation)+bzs.append(z)activation=sigmoid(z)activations.append(activation)# backward passdelta=(self.cost).delta(zs[-1],activations[-1],y)nabla_b[-1]=deltanabla_w[-1]=np.dot(delta,activations[-2].transpose())# Note that the variable l in the loop below is used a little# differently to the notation in Chapter 2 of the book.  Here,# l = 1 means the last layer of neurons, l = 2 is the# second-last layer, and so on.  It's a renumbering of the# scheme in the book, used here to take advantage of the fact# that Python can use negative indices in lists.forlinxrange(2,self.num_layers):z=zs[-l]sp=sigmoid_prime(z)delta=np.dot(self.weights[-l+1].transpose(),delta)*spnabla_b[-l]=deltanabla_w[-l]=np.dot(delta,activations[-l-1].transpose())return(nabla_b,nabla_w)defaccuracy(self,data,convert=False):\"\"\"Return the number of inputs in ``data`` for which the neuralnetwork outputs the correct result. The neural network'soutput is assumed to be the index of whichever neuron in thefinal layer has the highest activation.The flag ``convert`` should be set to False if the data set isvalidation or test data (the usual case), and to True if thedata set is the training data. The need for this flag arisesdue to differences in the way the results ``y`` arerepresented in the different data sets.  In particular, itflags whether we need to convert between the differentrepresentations.  It may seem strange to use differentrepresentations for the different data sets.  Why not use thesame representation for all three data sets?  It's done forefficiency reasons -- the program usually evaluates the coston the training data and the accuracy on other data sets.These are different types of computations, and using differentrepresentations speeds things up.  More details on therepresentations can be found inmnist_loader.load_data_wrapper.\"\"\"ifconvert:results=[(np.argmax(self.feedforward(x)),np.argmax(y))for(x,y)indata]else:results=[(np.argmax(self.feedforward(x)),y)for(x,y)indata]returnsum(int(x==y)for(x,y)inresults)deftotal_cost(self,data,lmbda,convert=False):\"\"\"Return the total cost for the data set ``data``.  The flag``convert`` should be set to False if the data set is thetraining data (the usual case), and to True if the data set isthe validation or test data.  See comments on the similar (butreversed) convention for the ``accuracy`` method, above.\"\"\"cost=0.0forx,yindata:a=self.feedforward(x)ifconvert:y=vectorized_result(y)cost+=self.cost.fn(a,y)/len(data)cost+=0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2forwinself.weights)returncostdefsave(self,filename):\"\"\"Save the neural network to the file ``filename``.\"\"\"data={\"sizes\":self.sizes,\"weights\":[w.tolist()forwinself.weights],\"biases\":[b.tolist()forbinself.biases],\"cost\":str(self.cost.__name__)}f=open(filename,\"w\")json.dump(data,f)f.close()#### Loading a Networkdefload(filename):\"\"\"Load a neural network from the file ``filename``.  Returns aninstance of Network.\"\"\"f=open(filename,\"r\")data=json.load(f)f.close()cost=getattr(sys.modules[__name__],data[\"cost\"])net=Network(data[\"sizes\"],cost=cost)net.weights=[np.array(w)forwindata[\"weights\"]]net.biases=[np.array(b)forbindata[\"biases\"]]returnnet#### Miscellaneous functionsdefvectorized_result(j):\"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th positionand zeroes elsewhere.  This is used to convert a digit (0...9)into a corresponding desired output from the neural network.\"\"\"e=np.zeros((10,1))e[j]=1.0returnedefsigmoid(z):\"\"\"The sigmoid function.\"\"\"return1.0/(1.0+np.exp(-z))defsigmoid_prime(z):\"\"\"Derivative of the sigmoid function.\"\"\"returnsigmoid(z)*(1-sigmoid(z))One of the more interesting changes in the code is to include L2\rregularization.  Although this is a major conceptual change, it's so\rtrivial to implement that it's easy to miss in the code.  For the most\rpart it just involves passing the parameterlmbdato various\rmethods, notably theNetwork.SGDmethod.  The real work is done\rin a single line of the program, the fourth-last line of theNetwork.update_mini_batchmethod.  That's where we modify the\rgradient descent update rule to include weight decay.  But although\rthe modification is tiny, it has a big impact on results!This is, by the way, common when implementing new techniques in neural\rnetworks.  We've spent thousands of words discussing regularization.\rIt's conceptually quite subtle and difficult to understand.  And yet\rit was trivial to add to our program!  It occurs surprisingly often\rthat sophisticated techniques can be implemented with small changes to\rcode.Another small but important change to our code is the addition of\rseveral optional flags to the stochastic gradient descent method,Network.SGD.  These flags make it possible to monitor the cost\rand accuracy either on thetraining_dataor on a set ofevaluation_datawhich can be passed toNetwork.SGD.\rWe've used these flags often earlier in the chapter, but let me give\ran example of how it works, just to remind you:>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.SGD(training_data,30,10,0.5,...lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True,...monitor_evaluation_cost=True,...monitor_training_accuracy=True,...monitor_training_cost=True)Here, we're setting theevaluation_datato be thevalidation_data.  But we could also have monitored performance\ron thetest_dataor any other data set.  We also have four\rflags telling us to monitor the cost and accuracy on both theevaluation_dataand thetraining_data.  Those flags areFalseby default, but they've been turned on here in order to\rmonitor ourNetwork's performance.  Furthermore,network2.py'sNetwork.SGDmethod returns a four-element\rtuple representing the results of the monitoring.  We can use this as\rfollows:>>>evaluation_cost,evaluation_accuracy,...training_cost,training_accuracy=net.SGD(training_data,30,10,0.5,...lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True,...monitor_evaluation_cost=True,...monitor_training_accuracy=True,...monitor_training_cost=True)So, for example,evaluation_costwill be a 30-element list\rcontaining the cost on the evaluation data at the end of each epoch.\rThis sort of information is extremely useful in understanding a\rnetwork's behaviour.  It can, for example, be used to draw graphs\rshowing how the network learns over time.  Indeed, that's exactly how\rI constructed all the graphs earlier in the chapter.  Note, however,\rthat if any of the monitoring flags are not set, then the\rcorresponding element in the tuple will be the empty list.Other additions to the code include aNetwork.savemethod, to\rsaveNetworkobjects to disk, and a function toloadthem back in again later.  Note that the saving and loading is done\rusing JSON, not Python'spickleorcPicklemodules,\rwhich are the usual way we save and load objects to and from disk in\rPython.  Using JSON requires more code thanpickleorcPicklewould.  To understand why I've used JSON, imagine that\rat some time in the future we decided to change ourNetworkclass to allow neurons other than sigmoid neurons.  To implement that\rchange we'd most likely change the attributes defined in theNetwork.__init__method.  If we've simply pickled the objects\rthat would cause ourloadfunction to fail.  Using JSON to do\rthe serialization explicitly makes it easy to ensure that oldNetworks will stillload.There are many other minor changes in the code fornetwork2.py,\rbut they're all simple variations onnetwork.py.  The net\rresult is to expand our 74-line program to a far more capable 152\rlines.ProblemsModify the code above to implement L1 regularization, and use L1\r  regularization to classify MNIST digits using a $30$ hidden neuron\r  network.  Can you find a regularization parameter that enables you\r  to do better than running unregularized?Take a look at theNetwork.cost_derivativemethod innetwork.py.\r  That method was written for the quadratic cost.  How would you\r  rewrite the method for the cross-entropy cost?  Can you think of a\r  problem that might arise in the cross-entropy version?  Innetwork2.pywe've eliminated theNetwork.cost_derivativemethod entirely, instead\r  incorporating its functionality into theCrossEntropyCost.deltamethod.  How does this solve the\r  problem you've just identified?How to choose a neural network's hyper-parameters?Up until now I haven't explained how I've been choosing values for\rhyper-parameters such as the learning rate, $\\eta$, the regularization\rparameter, $\\lambda$, and so on.  I've just been supplying values\rwhich work pretty well.  In practice, when you're using neural nets to\rattack a problem, it can be difficult to find good hyper-parameters.\rImagine, for example, that we've just been introduced to the MNIST\rproblem, and have begun working on it, knowing nothing at all about\rwhat hyper-parameters to use.  Let's suppose that by good fortune in\rour first experiments we choose many of the hyper-parameters in the\rsame way as was done earlier this chapter: 30 hidden neurons, a\rmini-batch size of 10, training for 30 epochs using the cross-entropy.\rBut we choose a learning rate $\\eta = 10.0$ and regularization\rparameter $\\lambda = 1000.0$.  Here's what I saw on one such run:>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10])>>>net.SGD(training_data,30,10,10.0,lmbda=1000.0,...evaluation_data=validation_data,monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:1030/10000Epoch1trainingcompleteAccuracyonevaluationdata:990/10000Epoch2trainingcompleteAccuracyonevaluationdata:1009/10000...Epoch27trainingcompleteAccuracyonevaluationdata:1009/10000Epoch28trainingcompleteAccuracyonevaluationdata:983/10000Epoch29trainingcompleteAccuracyonevaluationdata:967/10000Our classification accuracies are no better than chance!  Our network\ris acting as a random noise generator!\"Well, that's easy to fix,\" you might say, \"just decrease the\rlearning rate and regularization hyper-parameters\".  Unfortunately,\ryou don'ta prioriknow those are the hyper-parameters you need\rto adjust.  Maybe the real problem is that our 30 hidden neuron\rnetwork will never work well, no matter how the other hyper-parameters\rare chosen?  Maybe we really need at least 100 hidden neurons?  Or 300\rhidden neurons?  Or multiple hidden layers?  Or a different approach\rto encoding the output?  Maybe our network is learning, but we need to\rtrain for more epochs?  Maybe the mini-batches are too small?  Maybe\rwe'd do better switching back to the quadratic cost function?  Maybe\rwe need to try a different approach to weight initialization?  And so\ron, on and on and on.  It's easy to feel lost in hyper-parameter\rspace.  This can be particularly frustrating if your network is very\rlarge, or uses a lot of training data, since you may train for hours\ror days or weeks, only to get no result.  If the situation persists,\rit damages your confidence.  Maybe neural networks are the wrong\rapproach to your problem?  Maybe you should quit your job and take up\rbeekeeping?In this section I explain some heuristics which can be used to set the\rhyper-parameters in a neural network.  The goal is to help you develop\ra workflow that enables you to do a pretty good job setting\rhyper-parameters.  Of course, I won't cover everything about\rhyper-parameter optimization.  That's a huge subject, and it's not, in\rany case, a problem that is ever completely solved, nor is there\runiversal agreement amongst practitioners on the right strategies to\ruse.  There's always one more trick you can try to eke out a bit more\rperformance from your network.  But the heuristics in this section\rshould get you started.Broad strategy:When using neural networks to attack a new\rproblem the first challenge is to getanynon-trivial learning,\ri.e., for the network to achieve results better than chance.  This can\rbe surprisingly difficult, especially when confronting a new class of\rproblem.  Let's look at some strategies you can use if you're having\rthis kind of trouble.Suppose, for example, that you're attacking MNIST for the first time.\rYou start out enthusiastic, but are a little discouraged when your\rfirst network fails completely, as in the example above.  The way to\rgo is to strip the problem down.  Get rid of all the training and\rvalidation images except images which are 0s or 1s.  Then try to train\ra network to distinguish 0s from 1s.  Not only is that an inherently\reasier problem than distinguishing all ten digits, it also reduces the\ramount of training data by 80 percent, speeding up training by a\rfactor of 5.  That enables much more rapid experimentation, and so\rgives you more rapid insight into how to build a good network.You can further speed up experimentation by stripping your network\rdown to the simplest network likely to do meaningful learning.  If you\rbelieve a[784, 10]network can likely do better-than-chance\rclassification of MNIST digits, then begin your experimentation with\rsuch a network.  It'll be much faster than training a[784, 30, 10]network, and you can build back up to the latter.You can get another speed up in experimentation by increasing the\rfrequency of monitoring.  Innetwork2.pywe monitor performance\rat the end of each training epoch.  With 50,000 images per epoch, that\rmeans waiting a little while - about ten seconds per epoch, on my\rlaptop, when training a[784, 30, 10]network - before\rgetting feedback on how well the network is learning.  Of course, ten\rseconds isn't very long, but if you want to trial dozens of\rhyper-parameter choices it's annoying, and if you want to trial\rhundreds or thousands of choices it starts to get debilitating.  We\rcan get feedback more quickly by monitoring the validation accuracy\rmore often, say, after every 1,000 training images.  Furthermore,\rinstead of using the full 10,000 image validation set to monitor\rperformance, we can get a much faster estimate using just 100\rvalidation images.  All that matters is that the network sees enough\rimages to do real learning, and to get a pretty good rough estimate of\rperformance. Of course, our programnetwork2.pydoesn't\rcurrently do this kind of monitoring.  But as a kludge to achieve a\rsimilar effect for the purposes of illustration, we'll strip down our\rtraining data to just the first 1,000 MNIST training images.  Let's\rtry it and see what happens.  (To keep the code below simple I haven't\rimplemented the idea of using only 0 and 1 images.  Of course, that\rcan be done with just a little more work.)>>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,10.0,lmbda=1000.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:10/100Epoch1trainingcompleteAccuracyonevaluationdata:10/100Epoch2trainingcompleteAccuracyonevaluationdata:10/100...We're still getting pure noise!  But there's a big win: we're now\rgetting feedback in a fraction of a second, rather than once every ten\rseconds or so.  That means you can more quickly experiment with other\rchoices of hyper-parameter, or even conduct experiments trialling many\rdifferent choices of hyper-parameter nearly simultaneously.In the above example I left $\\lambda$ as $\\lambda = 1000.0$, as we\rused earlier.  But since we changed the number of training examples we\rshould really change $\\lambda$ to keep the weight decay the same.\rThat means changing $\\lambda$ to $20.0$.  If we do that then this is\rwhat happens:>>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,10.0,lmbda=20.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:12/100Epoch1trainingcompleteAccuracyonevaluationdata:14/100Epoch2trainingcompleteAccuracyonevaluationdata:25/100Epoch3trainingcompleteAccuracyonevaluationdata:18/100...Ahah!  We have a signal.  Not a terribly good signal, but a signal\rnonetheless.  That's something we can build on, modifying the\rhyper-parameters to try to get further improvement.  Maybe we guess\rthat our learning rate needs to be higher.  (As you perhaps realize,\rthat's a silly guess, for reasons we'll discuss shortly, but please\rbear with me.)  So to test our guess we try dialing $\\eta$ up to\r$100.0$:>>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,100.0,lmbda=20.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:10/100Epoch1trainingcompleteAccuracyonevaluationdata:10/100Epoch2trainingcompleteAccuracyonevaluationdata:10/100Epoch3trainingcompleteAccuracyonevaluationdata:10/100...That's no good!  It suggests that our guess was wrong, and the problem\rwasn't that the learning rate was too low.  So instead we try dialing\r$\\eta$ down to $\\eta = 1.0$:>>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,1.0,lmbda=20.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:62/100Epoch1trainingcompleteAccuracyonevaluationdata:42/100Epoch2trainingcompleteAccuracyonevaluationdata:43/100Epoch3trainingcompleteAccuracyonevaluationdata:61/100...That's better!  And so we can continue, individually adjusting each\rhyper-parameter, gradually improving performance.  Once we've explored\rto find an improved value for $\\eta$, then we move on to find a good\rvalue for $\\lambda$.  Then experiment with a more complex\rarchitecture, say a network with 10 hidden neurons.  Then adjust the\rvalues for $\\eta$ and $\\lambda$ again.  Then increase to 20 hidden\rneurons.  And then adjust other hyper-parameters some more.  And so\ron, at each stage evaluating performance using our held-out validation\rdata, and using those evaluations to find better and better\rhyper-parameters.  As we do so, it typically takes longer to witness\rthe impact due to modifications of the hyper-parameters, and so we can\rgradually decrease the frequency of monitoring.This all looks very promising as a broad strategy.  However, I want to\rreturn to that initial stage of finding hyper-parameters that enable a\rnetwork to learn anything at all.  In fact, even the above discussion\rconveys too positive an outlook.  It can be immensely frustrating to\rwork with a network that's learning nothing.  You can tweak\rhyper-parameters for days, and still get no meaningful response.  And\rso I'd like to re-emphasize that during the early stages you should\rmake sure you can get quick feedback from experiments.  Intuitively,\rit may seem as though simplifying the problem and the architecture\rwill merely slow you down.  In fact, it speeds things up, since you\rmuch more quickly find a network with a meaningful signal.  Once\ryou've got such a signal, you can often get rapid improvements by\rtweaking the hyper-parameters.  As with many things in life, getting\rstarted can be the hardest thing to do.Okay, that's the broad strategy.  Let's now look at some specific\rrecommendations for setting hyper-parameters.  I will focus on the\rlearning rate, $\\eta$, the L2 regularization parameter, $\\lambda$, and\rthe mini-batch size.  However, many of the remarks apply also to other\rhyper-parameters, including those associated to network architecture,\rother forms of regularization, and some hyper-parameters we'll meet\rlater in the book, such as the momentum co-efficient.Learning rate:Suppose we run three MNIST networks with three\rdifferent learning rates, $\\eta = 0.025$, $\\eta = 0.25$ and $\\eta =\r2.5$, respectively.  We'll set the other hyper-parameters as for the\rexperiments in earlier sections, running over 30 epochs, with a\rmini-batch size of 10, and with $\\lambda = 5.0$.  We'll also return to\rusing the full $50,000$ training images.  Here's a graph showing the\rbehaviour of the training cost as we train**The graph was\r  generated bymultiple_eta.py.:With $\\eta = 0.025$ the cost decreases smoothly until the final epoch.\rWith $\\eta = 0.25$ the cost initially decreases, but after about $20$\repochs it is near saturation, and thereafter most of the changes are\rmerely small and apparently random oscillations.  Finally, with $\\eta\r= 2.5$ the cost makes large oscillations right from the start.  To\runderstand the reason for the oscillations, recall that stochastic\rgradient descent is supposed to step us gradually down into a valley\rof the cost function,However, if $\\eta$ is too large then the steps will be so large that\rthey may actually overshoot the minimum, causing the algorithm to\rclimb up out of the valley instead.  That's likely**This\r  picture is helpful, but it's intended as an intuition-building\r  illustration of what may go on, not as a complete, exhaustive\r  explanation.  Briefly, a more complete explanation is as follows:\r  gradient descent uses a first-order approximation to the cost\r  function as a guide to how to decrease the cost.  For large $\\eta$,\r  higher-order terms in the cost function become more important, and\r  may dominate the behaviour, causing gradient descent to break down.\r  This is especially likely as we approach minima and quasi-minima of\r  the cost function, since near such points the gradient becomes\r  small, making it easier for higher-order terms to dominate\r  behaviour.what's causing the cost to oscillate when $\\eta = 2.5$.\rWhen we choose $\\eta = 0.25$ the initial steps do take us toward a\rminimum of the cost function, and it's only once we get near that\rminimum that we start to suffer from the overshooting problem.  And\rwhen we choose $\\eta = 0.025$ we don't suffer from this problem at all\rduring the first $30$ epochs.  Of course, choosing $\\eta$ so small\rcreates another problem, namely, that it slows down stochastic\rgradient descent.  An even better approach would be to start with\r$\\eta = 0.25$, train for $20$ epochs, and then switch to $\\eta =\r0.025$.  We'll discuss such variable learning rate schedules later.\rFor now, though, let's stick to figuring out how to find a single good\rvalue for the learning rate, $\\eta$.With this picture in mind, we can set $\\eta$ as follows.  First, we\restimate the threshold value for $\\eta$ at which the cost on the\rtraining data immediately begins decreasing, instead of oscillating or\rincreasing.  This estimate doesn't need to be too accurate.  You can\restimate the order of magnitude by starting with $\\eta = 0.01$.  If\rthe cost decreases during the first few epochs, then you should\rsuccessively try $\\eta = 0.1, 1.0, \\ldots$ until you find a value for\r$\\eta$ where the cost oscillates or increases during the first few\repochs.  Alternately, if the cost oscillates or increases during the\rfirst few epochs when $\\eta = 0.01$, then try $\\eta = 0.001, 0.0001,\r\\ldots$ until you find a value for $\\eta$ where the cost decreases\rduring the first few epochs.  Following this procedure will give us an\rorder of magnitude estimate for the threshold value of $\\eta$.  You\rmay optionally refine your estimate, to pick out the largest value of\r$\\eta$ at which the cost decreases during the first few epochs, say\r$\\eta = 0.5$ or $\\eta = 0.2$ (there's no need for this to be\rsuper-accurate).  This gives us an estimate for the threshold value of\r$\\eta$.Obviously, the actual value of $\\eta$ that you use should be no larger\rthan the threshold value.  In fact, if the value of $\\eta$ is to\rremain usable over many epochs then you likely want to use a value\rfor $\\eta$ that is smaller, say, a factor of two below the threshold.\rSuch a choice will typically allow you to train for many epochs,\rwithout causing too much of a slowdown in learning.In the case of the MNIST data, following this strategy leads to an\restimate of $0.1$ for the order of magnitude of the threshold value of\r$\\eta$.  After some more refinement, we obtain a threshold value $\\eta\r= 0.5$.  Following the prescription above, this suggests using $\\eta =\r0.25$ as our value for the learning rate.  In fact, I found that using\r$\\eta = 0.5$ worked well enough over $30$ epochs that for the most\rpart I didn't worry about using a lower value of $\\eta$.This all seems quite straightforward.  However, using the training\rcost to pick $\\eta$ appears to contradict what I said earlier in this\rsection, namely, that we'd pick hyper-parameters by evaluating\rperformance using our held-out validation data.  In fact, we'll use\rvalidation accuracy to pick the regularization hyper-parameter, the\rmini-batch size, and network parameters such as the number of layers\rand hidden neurons, and so on.  Why do things differently for the\rlearning rate?  Frankly, this choice is my personal aesthetic\rpreference, and is perhaps somewhat idiosyncratic.  The reasoning is\rthat the other hyper-parameters are intended to improve the final\rclassification accuracy on the test set, and so it makes sense to\rselect them on the basis of validation accuracy.  However, the\rlearning rate is only incidentally meant to impact the final\rclassification accuracy.  Its primary purpose is really to control\rthe step size in gradient descent, and monitoring the training cost is\rthe best way to detect if the step size is too big.  With that said,\rthis is a personal aesthetic preference.  Early on during learning the\rtraining cost usually only decreases if the validation accuracy\rimproves, and so in practice it's unlikely to make much difference\rwhich criterion you use.Use early stopping to determine the number of training\r  epochs:As we discussed earlier in the chapter, early stopping means\rthat at the end of each epoch we should compute the classification\raccuracy on the validation data.  When that stops improving,\rterminate.  This makes setting the number of epochs very simple.  In\rparticular, it means that we don't need to worry about explicitly\rfiguring out how the number of epochs depends on the other\rhyper-parameters.  Instead, that's taken care of automatically.\rFurthermore, early stopping also automatically prevents us from\roverfitting.  This is, of course, a good thing, although in the early\rstages of experimentation it can be helpful to turn off early\rstopping, so you can see any signs of overfitting, and use it to\rinform your approach to regularization.To implement early stopping we need to say more precisely what it\rmeans that the classification accuracy has stopped improving.  As\rwe've seen, the accuracy can jump around quite a bit, even when the\roverall trend is to improve.  If we stop the first time the accuracy\rdecreases then we'll almost certainly stop when there are more\rimprovements to be had.  A better rule is to terminate if the best\rclassification accuracy doesn't improve for quite some time.  Suppose,\rfor example, that we're doing MNIST.  Then we might elect to terminate\rif the classification accuracy hasn't improved during the last ten\repochs.  This ensures that we don't stop too soon, in response to bad\rluck in training, but also that we're not waiting around forever for\ran improvement that never comes.This no-improvement-in-ten rule is good for initial exploration of\rMNIST.  However, networks can sometimes plateau near a particular\rclassification accuracy for quite some time, only to then begin\rimproving again.  If you're trying to get really good performance, the\rno-improvement-in-ten rule may be too aggressive about stopping.  In\rthat case, I suggest using the no-improvement-in-ten rule for initial\rexperimentation, and gradually adopting more lenient rules, as you\rbetter understand the way your network trains:\rno-improvement-in-twenty, no-improvement-in-fifty, and so on.  Of\rcourse, this introduces a new hyper-parameter to optimize!  In\rpractice, however, it's usually easy to set this hyper-parameter to\rget pretty good results.  Similarly, for problems other than MNIST,\rthe no-improvement-in-ten rule may be much too aggressive or not\rnearly aggressive enough, depending on the details of the problem.\rHowever, with a little experimentation it's usually easy to find a\rpretty good strategy for early stopping.We haven't used early stopping in our MNIST experiments to date.  The\rreason is that we've been doing a lot of comparisons between different\rapproaches to learning.  For such comparisons it's helpful to use the\rsame number of epochs in each case.  However, it's well worth\rmodifyingnetwork2.pyto implement early stopping:ProblemModifynetwork2.pyso that it implements early stopping\r  using a no-improvement-in-$n$ epochs strategy, where $n$ is a\r  parameter that can be set.Can you think of a rule for early stoppingotherthan\r  no-improvement-in-$n$?  Ideally, the rule should compromise between\r  getting high validation accuracies and not training too long.  Add\r  your rule tonetwork2.py, and run three experiments comparing\r  the validation accuracies and number of epochs of training to\r  no-improvement-in-$10$.Learning rate schedule:We've been holding the learning rate\r$\\eta$ constant.  However, it's often advantageous to vary the\rlearning rate.  Early on during the learning process it's likely that\rthe weights are badly wrong.  And so it's best to use a large learning\rrate that causes the weights to change quickly.  Later, we can reduce\rthe learning rate as we make more fine-tuned adjustments to our\rweights.How should we set our learning rate schedule?  Many approaches are\rpossible.  One natural approach is to use the same basic idea as early\rstopping.  The idea is to hold the learning rate constant until the\rvalidation accuracy starts to get worse.  Then decrease the learning\rrate by some amount, say a factor of two or ten.  We repeat this many\rtimes, until, say, the learning rate is a factor of 1,024 (or 1,000)\rtimes lower than the initial value.  Then we terminate.A variable learning schedule can improve performance, but it also\ropens up a world of possible choices for the learning schedule.  Those\rchoices can be a headache - you can spend forever trying to optimize\ryour learning schedule.  For first experiments my suggestion is to use\ra single, constant value for the learning rate.  That'll get you a\rgood first approximation.  Later, if you want to obtain the best\rperformance from your network, it's worth experimenting with a\rlearning schedule, along the lines I've described**A readable\r  recent paper which demonstrates the benefits of variable learning\r  rates in attacking MNIST isDeep, Big, Simple Neural Nets\r    Excel on Handwritten Digit Recognition, by Dan Claudiu\r  Cireșan, Ueli Meier, Luca Maria Gambardella, and\r  Jürgen Schmidhuber (2010)..ExerciseModifynetwork2.pyso that it implements a learning\r  schedule that: halves the learning rate each time the validation\r  accuracy satisfies the no-improvement-in-$10$ rule; and terminates\r  when the learning rate has dropped to $1/128$ of its original value.The regularization parameter, $\\lambda$:I suggest starting\rinitially with no regularization ($\\lambda = 0.0$), and determining a\rvalue for $\\eta$, as above.  Using that choice of $\\eta$, we can then\ruse the validation data to select a good value for $\\lambda$.  Start\rby trialling $\\lambda = 1.0$**I don't have a good principled\r  justification for using this as a starting value.  If anyone knows\r  of a good principled discussion of where to start with $\\lambda$,\r  I'd appreciate hearing it (mn@michaelnielsen.org)., and then\rincrease or decrease by factors of $10$, as needed to improve\rperformance on the validation data.  Once you've found a good order of\rmagnitude, you can fine tune your value of $\\lambda$.  That done, you\rshould return and re-optimize $\\eta$ again.ExerciseIt's tempting to use gradient descent to try to learn good\r  values for hyper-parameters such as $\\lambda$ and $\\eta$.  Can you\r  think of an obstacle to using gradient descent to determine\r  $\\lambda$?  Can you think of an obstacle to using gradient descent\r  to determine $\\eta$?How I selected hyper-parameters earlier in this book:If you\ruse the recommendations in this section you'll find that you get\rvalues for $\\eta$ and $\\lambda$ which don't always exactly match the\rvalues I've used earlier in the book.  The reason is that the book has\rnarrative constraints that have sometimes made it impractical to\roptimize the hyper-parameters.  Think of all the comparisons we've\rmade of different approaches to learning, e.g., comparing the\rquadratic and cross-entropy cost functions, comparing the old and new\rmethods of weight initialization, running with and without\rregularization, and so on.  To make such comparisons meaningful, I've\rusually tried to keep hyper-parameters constant across the approaches\rbeing compared (or to scale them in an appropriate way).  Of course,\rthere's no reason for the same hyper-parameters to be optimal for all\rthe different approaches to learning, so the hyper-parameters I've\rused are something of a compromise.As an alternative to this compromise, I could have tried to optimize\rthe heck out of the hyper-parameters for every single approach to\rlearning.  In principle that'd be a better, fairer approach, since\rthen we'd see the best from every approach to learning.  However,\rwe've made dozens of comparisons along these lines, and in practice I\rfound it too computationally expensive.  That's why I've adopted the\rcompromise of using pretty good (but not necessarily optimal) choices\rfor the hyper-parameters.Mini-batch size:How should we set the mini-batch size?  To\ranswer this question, let's first suppose that we're doing online\rlearning, i.e., that we're using a mini-batch size of $1$.The obvious worry about online learning is that using mini-batches\rwhich contain just a single training example will cause significant\rerrors in our estimate of the gradient.  In fact, though, the errors\rturn out to not be such a problem.  The reason is that the individual\rgradient estimates don't need to be super-accurate.  All we need is an\restimate accurate enough that our cost function tends to keep\rdecreasing.  It's as though you are trying to get to the North\rMagnetic Pole, but have a wonky compass that's 10-20 degrees off each\rtime you look at it.  Provided you stop to check the compass\rfrequently, and the compass gets the direction right on average,\ryou'll end up at the North Magnetic Pole just fine.Based on this argument, it sounds as though we should use online\rlearning.  In fact, the situation turns out to be more complicated\rthan that.  In aproblem\r  in the last chapterI pointed out that it's possible to use matrix\rtechniques to compute the gradient update forallexamples in a\rmini-batch simultaneously, rather than looping over them.  Depending\ron the details of your hardware and linear algebra library this can\rmake it quite a bit faster to compute the gradient estimate for a\rmini-batch of (for example) size $100$, rather than computing the\rmini-batch gradient estimate by looping over the $100$ training\rexamples separately.  It might take (say) only $50$ times as long,\rrather than $100$ times as long.Now, at first it seems as though this doesn't help us that much.  With\rour mini-batch of size $100$ the learning rule for the weights looks\rlike:\\begin{eqnarray}\r  w \\rightarrow w' = w-\\eta \\frac{1}{100} \\sum_x \\nabla C_x,\r\\tag{100}\\end{eqnarray}\rwhere the sum is over training examples in the mini-batch.  This is\rversus\\begin{eqnarray}\r  w \\rightarrow w' = w-\\eta \\nabla C_x\r\\tag{101}\\end{eqnarray}\rfor online learning.  Even if it only takes $50$ times as long to do\rthe mini-batch update, it still seems likely to be better to do online\rlearning, because we'd be updating so much more frequently.  Suppose,\rhowever, that in the mini-batch case we increase the learning rate by\ra factor $100$, so the update rule becomes\\begin{eqnarray}\r  w \\rightarrow w' = w-\\eta \\sum_x \\nabla C_x.\r\\tag{102}\\end{eqnarray}\rThat's a lot like doing $100$ separate instances of online learning\rwith a learning rate of $\\eta$.  But it only takes $50$ times as long\ras doing a single instance of online learning.  Of course, it's not\rtruly the same as $100$ instances of online learning, since in the\rmini-batch the $\\nabla C_x$'s are all evaluated for the same set of\rweights, as opposed to the cumulative learning that occurs in the\ronline case.  Still, it seems distinctly possible that using the\rlarger mini-batch would speed things up.With these factors in mind, choosing the best mini-batch size is a\rcompromise.  Too small, and you don't get to take full advantage of\rthe benefits of good matrix libraries optimized for fast hardware.\rToo large and you're simply not updating your weights often enough.\rWhat you need is to choose a compromise value which maximizes the\rspeed of learning.  Fortunately, the choice of mini-batch size at\rwhich the speed is maximized is relatively independent of the other\rhyper-parameters (apart from the overall architecture), so you don't\rneed to have optimized those hyper-parameters in order to find a good\rmini-batch size.  The way to go is therefore to use some acceptable\r(but not necessarily optimal) values for the other hyper-parameters,\rand then trial a number of different mini-batch sizes, scaling $\\eta$\ras above.  Plot the validation accuracy versustime(as in,\rreal elapsed time, not epoch!), and choose whichever mini-batch size\rgives you the most rapid improvement in performance.  With the\rmini-batch size chosen you can then proceed to optimize the other\rhyper-parameters.Of course, as you've no doubt realized, I haven't done this\roptimization in our work.  Indeed, our implementation doesn't use the\rfaster approach to mini-batch updates at all.  I've simply used a\rmini-batch size of $10$ without comment or explanation in nearly all\rexamples.  Because of this, we could have sped up learning by reducing\rthe mini-batch size.  I haven't done this, in part because I wanted to\rillustrate the use of mini-batches beyond size $1$, and in part\rbecause my preliminary experiments suggested the speedup would be\rrather modest.  In practical implementations, however, we would most\rcertainly implement the faster approach to mini-batch updates, and\rthen make an effort to optimize the mini-batch size, in order to\rmaximize our overall speed.Automated techniques:I've been describing these heuristics\ras though you're optimizing your hyper-parameters by hand.\rHand-optimization is a good way to build up a feel for how neural\rnetworks behave.  However, and unsurprisingly, a great deal of work\rhas been done on automating the process. A common technique isgrid search, which systematically searches through a grid in\rhyper-parameter space.  A review of both the achievements and the\rlimitations of grid search (with suggestions for easily-implemented\ralternatives) may be found in a 2012\rpaper**Random\r    search for hyper-parameter optimization, by James Bergstra and\r  Yoshua Bengio (2012).by James Bergstra and Yoshua Bengio.  Many\rmore sophisticated approaches have also been proposed.  I won't review\rall that work here, but do want to mention a particularly promising\r2012 paper which used a Bayesian approach to automatically optimize\rhyper-parameters**Practical\r    Bayesian optimization of machine learning algorithms, by Jasper\r  Snoek, Hugo Larochelle, and Ryan Adams..  The code from the paper\rispublicly available, and\rhas been used with some success by other researchers.Summing up:Following the rules-of-thumb I've described won't\rgive you the absolute best possible results from your neural network.\rBut it will likely give you a good start and a basis for further\rimprovements.  In particular, I've discussed the hyper-parameters\rlargely independently.  In practice, there are relationships between\rthe hyper-parameters.  You may experiment with $\\eta$, feel that\ryou've got it just right, then start to optimize for $\\lambda$, only\rto find that it's messing up your optimization for $\\eta$.  In\rpractice, it helps to bounce backward and forward, gradually closing\rin good values.  Above all, keep in mind that the heuristics I've\rdescribed are rules of thumb, not rules cast in stone.  You should be\ron the lookout for signs that things aren't working, and be willing to\rexperiment.  In particular, this means carefully monitoring your\rnetwork's behaviour, especially the validation accuracy.The difficulty of choosing hyper-parameters is exacerbated by the fact\rthat the lore about how to choose hyper-parameters is widely spread,\racross many research papers and software programs, and often is only\ravailable inside the heads of individual practitioners.  There are\rmany, many papers setting out (sometimes contradictory)\rrecommendations for how to proceed.  However, there are a few\rparticularly useful papers that synthesize and distill out much of\rthis lore.  Yoshua Bengio has a 2012\rpaper**Practical\r    recommendations for gradient-based training of deep\r    architectures, by Yoshua Bengio (2012).that gives some\rpractical recommendations for using backpropagation and gradient\rdescent to train neural networks, including deep neural nets.  Bengio\rdiscusses many issues in much more detail than I have, including how\rto do more systematic hyper-parameter searches.  Another good paper is\ra 1998\rpaper**Efficient\r    BackProp, by Yann LeCun, Léon Bottou,\r  Genevieve Orr and Klaus-Robert Müller (1998)by\rYann LeCun, Léon Bottou, Genevieve Orr and\rKlaus-Robert Müller.  Both these papers appear in\ran extremely useful 2012 book that collects many tricks commonly used\rin neural\rnets**Neural\r    Networks: Tricks of the Trade, edited by\r  Grégoire Montavon, Geneviève Orr, and Klaus-Robert\r    Müller..  The book is expensive, but many of the articles have\rbeen placed online by their respective authors with, one presumes, the\rblessing of the publisher, and may be located using a search engine.One thing that becomes clear as you read these articles and,\respecially, as you engage in your own experiments, is that\rhyper-parameter optimization is not a problem that is ever completely\rsolved.  There's always another trick you can try to improve\rperformance.  There is a saying common among writers that books are\rnever finished, only abandoned.  The same is also true of neural\rnetwork optimization: the space of hyper-parameters is so large that\rone never really finishes optimizing, one only abandons the network to\rposterity.  So your goal should be to develop a workflow that enables\ryou to quickly do a pretty good job on the optimization, while leaving\ryou the flexibility to try more detailed optimizations, if that's\rimportant.The challenge of setting hyper-parameters has led some people to\rcomplain that neural networks require a lot of work when compared with\rother machine learning techniques.  I've heard many variations on the\rfollowing complaint: \"Yes, a well-tuned neural network may get the\rbest performance on the problem.  On the other hand, I can try a\rrandom forest [or SVM or$\\ldots$ insert your own favorite technique]\rand it just works.  I don't have time to figure out just the right\rneural network.\"  Of course, from a practical point of view it's good\rto have easy-to-apply techniques.  This is particularly true when\ryou're just getting started on a problem, and it may not be obvious\rwhether machine learning can help solve the problem at all.  On the\rother hand, if getting optimal performance is important, then you may\rneed to try approaches that require more specialist knowledge.  While\rit would be nice if machine learning were always easy, there is noa priorireason it should be trivially simple.Other techniquesEach technique developed in this chapter is valuable to know in its\rown right, but that's not the only reason I've explained them.  The\rlarger point is to familiarize you with some of the problems which can\roccur in neural networks, and with a style of analysis which can help\rovercome those problems.  In a sense, we've been learning how to think\rabout neural nets.  Over the remainder of this chapter I briefly\rsketch a handful of other techniques.  These sketches are less\rin-depth than the earlier discussions, but should convey some feeling\rfor the diversity of techniques available for use in neural networks.Variations on stochastic gradient descentStochastic gradient descent by backpropagation has served us well in\rattacking the MNIST digit classification problem.  However, there are\rmany other approaches to optimizing the cost function, and sometimes\rthose other approaches offer performance superior to mini-batch\rstochastic gradient descent.  In this section I sketch two such\rapproaches, the Hessian and momentum techniques.Hessian technique:To begin our discussion it helps to put\rneural networks aside for a bit.  Instead, we're just going to\rconsider the abstract problem of minimizing a cost function $C$ which\ris a function of many variables, $w = w_1, w_2, \\ldots$, so $C =\rC(w)$.  By Taylor's theorem, the cost function can be approximated\rnear a point $w$ by\\begin{eqnarray}\r  C(w+\\Delta w) & = & C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j} \\Delta w_j\r  \\nonumber \\\\ & & + \\frac{1}{2} \\sum_{jk} \\Delta w_j \\frac{\\partial^2 C}{\\partial w_j\r    \\partial w_k} \\Delta w_k + \\ldots\r\\tag{103}\\end{eqnarray}\rWe can rewrite this more compactly as\\begin{eqnarray}\r  C(w+\\Delta w) = C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w + \\ldots,\r\\tag{104}\\end{eqnarray}\rwhere $\\nabla C$ is the usual gradient vector, and $H$ is a matrix\rknown as theHessian matrix, whose $jk$th entry is $\\partial^2\rC / \\partial w_j \\partial w_k$.  Suppose we approximate $C$ by\rdiscarding the higher-order terms represented by $\\ldots$ above,\\begin{eqnarray} \r  C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w.\r\\tag{105}\\end{eqnarray}\rUsing calculus we can show that the expression on the right-hand side\rcan be minimized**Strictly speaking, for this to be a minimum,\r  and not merely an extremum, we need to assume that the Hessian\r  matrix is positive definite.  Intuitively, this means that the\r  function $C$ looks like a valley locally, not a mountain or a\r  saddle.by choosing\\begin{eqnarray}\r  \\Delta w = -H^{-1} \\nabla C.\r\\tag{106}\\end{eqnarray}\rProvided(105)\\begin{eqnarray} \r  C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w \\nonumber\\end{eqnarray}is a good approximate expression for the\rcost function, then we'd expect that moving from the point $w$ to\r$w+\\Delta w = w-H^{-1} \\nabla C$ should significantly decrease the\rcost function.  That suggests a possible algorithm for minimizing the\rcost:Choose a starting point, $w$.Update $w$ to a new point $w' = w-H^{-1} \\nabla C$, where the\r  Hessian $H$ and $\\nabla C$ are computed at $w$.Update $w'$ to a new point $w{'}{'} = w'-H'^{-1} \\nabla' C$,\r  where the Hessian $H'$ and $\\nabla' C$ are computed at $w'$.$\\ldots$In practice,(105)\\begin{eqnarray} \r  C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w \\nonumber\\end{eqnarray}is only an approximation, and it's\rbetter to take smaller steps.  We do this by repeatedly changing $w$\rby an amount $\\Delta w = -\\eta H^{-1} \\nabla C$, where $\\eta$ is known\ras thelearning rate.This approach to minimizing a cost function is known as theHessian techniqueorHessian optimization.  There are\rtheoretical and empirical results showing that Hessian methods\rconverge on a minimum in fewer steps than standard gradient descent.\rIn particular, by incorporating information about second-order changes\rin the cost function it's possible for the Hessian approach to avoid\rmany pathologies that can occur in gradient descent.  Furthermore,\rthere are versions of the backpropagation algorithm which can be used\rto compute the Hessian.If Hessian optimization is so great, why aren't we using it in our\rneural networks?  Unfortunately, while it has many desirable\rproperties, it has one very undesirable property: it's very difficult\rto apply in practice.  Part of the problem is the sheer size of the\rHessian matrix.  Suppose you have a neural network with $10^7$ weights\rand biases.  Then the corresponding Hessian matrix will contain $10^7\r\\times 10^7 = 10^{14}$ entries.  That's a lot of entries!  And that\rmakes computing $H^{-1} \\nabla C$ extremely difficult in practice.\rHowever, that doesn't mean that it's not useful to understand.  In\rfact, there are many variations on gradient descent which are inspired\rby Hessian optimization, but which avoid the problem with overly-large\rmatrices.  Let's take a look at one such technique, momentum-based\rgradient descent.Momentum-based gradient descent:Intuitively, the advantage\rHessian optimization has is that it incorporates not just information\rabout the gradient, but also information about how the gradient is\rchanging.  Momentum-based gradient descent is based on a similar\rintuition, but avoids large matrices of second derivatives.  To\runderstand the momentum technique, think back to ouroriginal pictureof gradient\rdescent, in which we considered a ball rolling down into a valley.  At\rthe time, we observed that gradient descent is, despite its name, only\rloosely similar to a ball falling to the bottom of a valley.  The\rmomentum technique modifies gradient descent in two ways that make it\rmore similar to the physical picture.  First, it introduces a notion\rof \"velocity\" for the parameters we're trying to optimize.  The\rgradient acts to change the velocity, not (directly) the \"position\",\rin much the same way as physical forces change the velocity, and only\rindirectly affect position.  Second, the momentum method introduces a\rkind of friction term, which tends to gradually reduce the velocity.Let's give a more precise mathematical description.  We introduce\rvelocity variables $v = v_1, v_2, \\ldots$, one for each corresponding\r$w_j$ variable**In a neural net the $w_j$ variables would, of\r  course, include all weights and biases.. Then we replace the\rgradient descent update rule $w \\rightarrow w'= w-\\eta \\nabla C$ by\\begin{eqnarray} \r  v & \\rightarrow  & v' = \\mu v - \\eta \\nabla C \\tag{107}\\\\\r  \r  w & \\rightarrow & w' = w+v'.\r\\tag{108}\\end{eqnarray}\rIn these equations, $\\mu$ is a hyper-parameter which controls the\ramount of damping or friction in the system.  To understand the\rmeaning of the equations it's helpful to first consider the case where\r$\\mu = 1$, which corresponds to no friction.  When that's the case,\rinspection of the equations shows that the \"force\" $\\nabla C$ is now\rmodifying the velocity, $v$, and the velocity is controlling the rate\rof change of $w$.  Intuitively, we build up the velocity by repeatedly\radding gradient terms to it.  That means that if the gradient is in\r(roughly) the same direction through several rounds of learning, we\rcan build up quite a bit of steam moving in that direction.  Think,\rfor example, of what happens if we're moving straight down a slope:With each step the velocity gets larger down the slope, so we move\rmore and more quickly to the bottom of the valley.  This can enable\rthe momentum technique to work much faster than standard gradient\rdescent.  Of course, a problem is that once we reach the bottom of the\rvalley we will overshoot.  Or, if the gradient should change rapidly,\rthen we could find ourselves moving in the wrong direction.  That's\rthe reason for the $\\mu$ hyper-parameter in(107)\\begin{eqnarray} \r  v & \\rightarrow  & v' = \\mu v - \\eta \\nabla C  \\nonumber\\end{eqnarray}.  I\rsaid earlier that $\\mu$ controls the amount of friction in the system;\rto be a little more precise, you should think of $1-\\mu$ as the amount\rof friction in the system.  When $\\mu = 1$, as we've seen, there is no\rfriction, and the velocity is completely driven by the gradient\r$\\nabla C$.  By contrast, when $\\mu = 0$ there's a lot of friction,\rthe velocity can't build up, and Equations(107)\\begin{eqnarray} \r  v & \\rightarrow  & v' = \\mu v - \\eta \\nabla C  \\nonumber\\end{eqnarray}and(108)\\begin{eqnarray}  \r  w & \\rightarrow & w' = w+v' \\nonumber\\end{eqnarray}reduce to the usual equation for gradient\rdescent, $w \\rightarrow w'=w-\\eta \\nabla C$.  In practice, using a\rvalue of $\\mu$ intermediate between $0$ and $1$ can give us much of the\rbenefit of being able to build up speed, but without causing\rovershooting.  We can choose such a value for $\\mu$ using the held-out\rvalidation data, in much the same way as we select $\\eta$ and\r$\\lambda$.I've avoided naming the hyper-parameter $\\mu$ up to now.  The reason\ris that the standard name for $\\mu$ is badly chosen: it's called themomentum co-efficient.  This is potentially confusing, since\r$\\mu$ is not at all the same as the notion of momentum from\rphysics. Rather, it is much more closely related to friction.\rHowever, the term momentum co-efficient is widely used, so we will\rcontinue to use it.A nice thing about the momentum technique is that it takes almost no\rwork to modify an implementation of gradient descent to incorporate\rmomentum.  We can still use backpropagation to compute the gradients,\rjust as before, and use ideas such as sampling stochastically chosen\rmini-batches.  In this way, we can get some of the advantages of the\rHessian technique, using information about how the gradient is\rchanging.  But it's done without the disadvantages, and with only\rminor modifications to our code.  In practice, the momentum technique\ris commonly used, and often speeds up learning.ExerciseWhat would go wrong if we used $\\mu > 1$ in the momentum\r  technique?What would go wrong if we used $\\mu < 0$ in the momentum\r  technique?ProblemAdd momentum-based stochastic gradient descent tonetwork2.py.Other approaches to minimizing the cost function:Many other\rapproaches to minimizing the cost function have been developed, and\rthere isn't universal agreement on which is the best approach.  As\ryou go deeper into neural networks it's worth digging into the other\rtechniques, understanding how they work, their strengths and\rweaknesses, and how to apply them in practice.  A paper I mentioned\rearlier**Efficient\r    BackProp, by Yann LeCun, Léon Bottou,\r  Genevieve Orr and Klaus-Robert Müller (1998).introduces and compares several of these techniques, including\rconjugate gradient descent and the BFGS method (see also the closely\rrelated limited-memory BFGS method, known asL-BFGS).\rAnother technique which has recently shown promising\rresults**See, for example,On the\r    importance of initialization and momentum in deep learning, by\r  Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton\r  (2012).is Nesterov's accelerated gradient technique, which\rimproves on the momentum technique.  However, for many problems, plain\rstochastic gradient descent works well, especially if momentum is\rused, and so we'll stick to stochastic gradient descent through the\rremainder of this book.Other models of artificial neuronUp to now we've built our neural networks using sigmoid neurons.  In\rprinciple, a network built from sigmoid neurons can compute any\rfunction.  In practice, however, networks built using other model\rneurons sometimes outperform sigmoid networks.  Depending on the\rapplication, networks based on such alternate models may learn faster,\rgeneralize better to test data, or perhaps do both.  Let me mention a\rcouple of alternate model neurons, to give you the flavor of some\rvariations in common use.Perhaps the simplest variation is the tanh (pronounced \"tanch\")\rneuron, which replaces the sigmoid function by the hyperbolic tangent\rfunction.  The output of a tanh neuron with input $x$, weight vector\r$w$, and bias $b$ is given by\\begin{eqnarray}\r \\tanh(w \\cdot x+b), \r\\tag{109}\\end{eqnarray}\rwhere $\\tanh$ is, of course, the hyperbolic tangent function.  It\rturns out that this is very closely related to the sigmoid neuron.  To\rsee this, recall that the $\\tanh$ function is defined by\\begin{eqnarray}\r  \\tanh(z) \\equiv \\frac{e^z-e^{-z}}{e^z+e^{-z}}.\r\\tag{110}\\end{eqnarray}\rWith a little algebra it can easily be verified that\\begin{eqnarray} \r  \\sigma(z) = \\frac{1+\\tanh(z/2)}{2},\r\\tag{111}\\end{eqnarray}\rthat is, $\\tanh$ is just a rescaled version of the sigmoid function.\rWe can also see graphically that the $\\tanh$ function has the same\rshape as the sigmoid function,One difference between tanh neurons and sigmoid neurons is that the\routput from tanh neurons ranges from -1 to 1, not 0 to 1.  This means\rthat if you're going to build a network based on tanh neurons you may\rneed to normalize your outputs (and, depending on the details of the\rapplication, possibly your inputs) a little differently than in\rsigmoid networks.Similar to sigmoid neurons, a network of tanh neurons can, in\rprinciple, compute any function**There are some technical\r  caveats to this statement for both tanh and sigmoid neurons, as well\r  as for the rectified linear neurons discussed below.  However,\r  informally it's usually fine to think of neural networks as being\r  able to approximate any function to arbitrary accuracy.mapping\rinputs to the range -1 to 1.  Furthermore, ideas such as\rbackpropagation and stochastic gradient descent are as easily applied\rto a network of tanh neurons as to a network of sigmoid neurons.ExerciseProve the identity in Equation(111)\\begin{eqnarray} \r  \\sigma(z) = \\frac{1+\\tanh(z/2)}{2} \\nonumber\\end{eqnarray}.Which type of neuron should you use in your networks, the tanh or\rsigmoid?A priorithe answer is not obvious, to put it mildly!\rHowever, there are theoretical arguments and some empirical evidence\rto suggest that the tanh sometimes performs better**See, for\r  example,Efficient\r    BackProp, by Yann LeCun, Léon Bottou,\r  Genevieve Orr and Klaus-Robert Müller (1998), andUnderstanding\r    the difficulty of training deep feedforward networks, by Xavier\r  Glorot and Yoshua Bengio (2010)..  Let me briefly give you the\rflavor of one of the theoretical arguments for tanh neurons.  Suppose\rwe're using sigmoid neurons, so all activations in our network are\rpositive.  Let's consider the weights $w^{l+1}_{jk}$ input to the\r$j$th neuron in the $l+1$th layer.  The rules for backpropagation (seehere) tell us that the associated gradient\rwill be $a^l_k \\delta^{l+1}_j$.  Because the activations are positive\rthe sign of this gradient will be the same as the sign of\r$\\delta^{l+1}_j$.  What this means is that if $\\delta^{l+1}_j$ is\rpositive thenallthe weights $w^{l+1}_{jk}$ will decrease\rduring gradient descent, while if $\\delta^{l+1}_j$ is negative thenallthe weights $w^{l+1}_{jk}$ will increase during gradient\rdescent.  In other words, all weights to the same neuron must either\rincrease together or decrease together.  That's a problem, since some\rof the weights may need to increase while others need to decrease.\rThat can only happen if some of the input activations have different\rsigns.  That suggests replacing the sigmoid by an activation function,\rsuch as $\\tanh$, which allows both positive and negative activations.\rIndeed, because $\\tanh$ is symmetric about zero, $\\tanh(-z) =\r-\\tanh(z)$, we might even expect that, roughly speaking, the\ractivations in hidden layers would be equally balanced between\rpositive and negative.  That would help ensure that there is no\rsystematic bias for the weight updates to be one way or the other.How seriously should we take this argument?  While the argument is\rsuggestive, it's a heuristic, not a rigorous proof that tanh neurons\routperform sigmoid neurons.  Perhaps there are other properties of the\rsigmoid neuron which compensate for this problem?  Indeed, for many\rtasks the tanh is found empirically to provide only a small or no\rimprovement in performance over sigmoid neurons.  Unfortunately, we\rdon't yet have hard-and-fast rules to know which neuron types will\rlearn fastest, or give the best generalization performance, for any\rparticular application.Another variation on the sigmoid neuron is therectified linear\r  neuronorrectified linear unit.  The output of a rectified\rlinear unit with input $x$, weight vector $w$, and bias $b$ is given\rby\\begin{eqnarray}\r  \\max(0, w \\cdot x+b).\r\\tag{112}\\end{eqnarray}\rGraphically, the rectifying function $\\max(0, z)$ looks like this:Obviously such neurons are quite different from both sigmoid and tanh\rneurons.  However, like the sigmoid and tanh neurons, rectified linear\runits can be used to compute any function, and they can be trained\rusing ideas such as backpropagation and stochastic gradient descent.When should you use rectified linear units instead of sigmoid or tanh\rneurons?  Some recent work on image recognition**See, for\r  example,What\r    is the Best Multi-Stage Architecture for Object Recognition?, by\r  Kevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato and Yann\r  LeCun (2009),Deep\r    Sparse Rectiﬁer Neural Networks, by Xavier Glorot, Antoine\r  Bordes, and Yoshua Bengio (2011), andImageNet\r    Classification with Deep Convolutional Neural Networks, by Alex\r  Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).  Note that\r  these papers fill in important details about how to set up the\r  output layer, cost function, and regularization in networks using\r  rectified linear units. I've glossed over all these details in this\r  brief account. The papers also discuss in more detail the benefits\r  and drawbacks of using rectified linear units.  Another informative\r  paper isRectified\r    Linear Units Improve Restricted Boltzmann Machines, by Vinod Nair\r  and Geoffrey Hinton (2010), which demonstrates the benefits of using\r  rectified linear units in a somewhat different approach to neural\r  networks.has found considerable benefit in using rectified linear\runits through much of the network.  However, as with tanh neurons, we\rdo not yet have a really deep understanding of when, exactly,\rrectified linear units are preferable, nor why.  To give you the\rflavor of some of the issues, recall that sigmoid neurons stop\rlearning when they saturate, i.e., when their output is near either\r$0$ or $1$.  As we've seen repeatedly in this chapter, the problem is\rthat $\\sigma'$ terms reduce the gradient, and that slows down\rlearning.  Tanh neurons suffer from a similar problem when they\rsaturate.  By contrast, increasing the weighted input to a rectified\rlinear unit will never cause it to saturate, and so there is no\rcorresponding learning slowdown.  On the other hand, when the weighted\rinput to a rectified linear unit is negative, the gradient vanishes,\rand so the neuron stops learning entirely.  These are just two of the\rmany issues that make it non-trivial to understand when and why\rrectified linear units perform better than sigmoid or tanh neurons.I've painted a picture of uncertainty here, stressing that we do not\ryet have a solid theory of how activation functions should be chosen.\rIndeed, the problem is harder even than I have described, for there\rare infinitely many possible activation functions.  Which is the best\rfor any given problem?  Which will result in a network which learns\rfastest?  Which will give the highest test accuracies? I am surprised\rhow little really deep and systematic investigation has been done of\rthese questions.  Ideally, we'd have a theory which tells us, in\rdetail, how to choose (and perhaps modify-on-the-fly) our activation\rfunctions.  On the other hand, we shouldn't let the lack of a full\rtheory stop us!  We have powerful tools already at hand, and can make\ra lot of progress with those tools.  Through the remainder of this\rbook I'll continue to use sigmoid neurons as our go-to neuron, since\rthey're powerful and provide concrete illustrations of the core ideas\rabout neural nets.  But keep in the back of your mind that these same\rideas can be applied to other types of neuron, and that there are\rsometimes advantages in doing so.On stories in neural networksQuestion:How do you\r  approach utilizing and researching machine learning techniques that\r  are supported almost entirely empirically, as opposed to\r  mathematically? Also in what situations have you noticed some of\r  these techniques fail?Answer:You have to realize that our theoretical\r  tools are very weak. Sometimes, we have good mathematical intuitions\r  for why a particular technique should work. Sometimes our intuition\r  ends up being wrong [...] The questions become: how well does my\r  method work on this particular problem, and how large is the set of\r  problems on which it works well.-Question\r    and answerwith neural networks researcher Yann LeCunOnce, attending a conference on the foundations of quantum mechanics,\rI noticed what seemed to me a most curious verbal habit: when talks\rfinished, questions from the audience often began with \"I'm very\rsympathetic to your point of view, but [...]\".  Quantum foundations\rwas not my usual field, and I noticed this style of questioning\rbecause at other scientific conferences I'd rarely or never heard a\rquestioner express their sympathy for the point of view of the\rspeaker.  At the time, I thought the prevalence of the question\rsuggested that little genuine progress was being made in quantum\rfoundations, and people were merely spinning their wheels.  Later, I\rrealized that assessment was too harsh.  The speakers were wrestling\rwith some of the hardest problems human minds have ever confronted.\rOf course progress was slow!  But there was still value in hearing\rupdates on how people were thinking, even if they didn't always have\runarguable new progress to report.You may have noticed a verbal tic similar to \"I'm very sympathetic\r[...]\" in the current book.  To explain what we're seeing I've often\rfallen back on saying \"Heuristically, [...]\", or \"Roughly speaking,\r[...]\", following up with a story to explain some phenomenon or\rother.  These stories are plausible, but the empirical evidence I've\rpresented has often been pretty thin.  If you look through the\rresearch literature you'll see that stories in a similar style appear\rin many research papers on neural nets, often with thin supporting\revidence.  What should we think about such stories?In many parts of science - especially those parts that deal with\rsimple phenomena - it's possible to obtain very solid, very reliable\revidence for quite general hypotheses.  But in neural networks there\rare large numbers of parameters and hyper-parameters, and extremely\rcomplex interactions between them.  In such extraordinarily complex\rsystems it's exceedingly difficult to establish reliable general\rstatements.  Understanding neural networks in their full generality is\ra problem that, like quantum foundations, tests the limits of the\rhuman mind.  Instead, we often make do with evidence for or against a\rfew specific instances of a general statement.  As a result those\rstatements sometimes later need to be modified or abandoned, when new\revidence comes to light.One way of viewing this situation is that any heuristic story about\rneural networks carries with it an implied challenge.  For example,\rconsider the statement Iquoted\r  earlier, explaining why dropout works**FromImageNet\r    Classification with Deep Convolutional Neural Networksby Alex\r  Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).: \"This\rtechnique reduces complex co-adaptations of neurons, since a neuron\rcannot rely on the presence of particular other neurons. It is,\rtherefore, forced to learn more robust features that are useful in\rconjunction with many different random subsets of the other neurons.\"\rThis is a rich, provocative statement, and one could build a fruitful\rresearch program entirely around unpacking the statement, figuring out\rwhat in it is true, what is false, what needs variation and\rrefinement.  Indeed, there is now a small industry of researchers who\rare investigating dropout (and many variations), trying to understand\rhow it works, and what its limits are.  And so it goes with many of\rthe heuristics we've discussed.  Each heuristic is not just a\r(potential) explanation, it's also a challenge to investigate and\runderstand in more detail.Of course, there is not time for any single person to investigate all\rthese heuristic explanations in depth.  It's going to take decades (or\rlonger) for the community of neural networks researchers to develop a\rreally powerful, evidence-based theory of how neural networks learn.\rDoes this mean you should reject heuristic explanations as unrigorous,\rand not sufficiently evidence-based?  No!  In fact, we need such\rheuristics to inspire and guide our thinking.  It's like the great age\rof exploration: the early explorers sometimes explored (and made new\rdiscoveries) on the basis of beliefs which were wrong in important\rways.  Later, those mistakes were corrected as we filled in our\rknowledge of geography.  When you understand something poorly - as\rthe explorers understood geography, and as we understand neural nets\rtoday - it's more important to explore boldly than it is to be\rrigorously correct in every step of your thinking.  And so you should\rview these stories as a useful guide to how to think about neural\rnets, while retaining a healthy awareness of the limitations of such\rstories, and carefully keeping track of just how strong the evidence\ris for any given line of reasoning.  Put another way, we need good\rstories to help motivate and inspire us, and rigorous in-depth\rinvestigation in order to uncover the real facts of the matter.",
            "$z^L_1 = $$a^L_1 = $$z^L_2$ =$a^L_2 = $$z^L_3$ =$a^L_3 = $$z^L_4$ =$a^L_4 = $",
            "As you increase $z^L_4$, you'll see an increase in the corresponding\routput activation, $a^L_4$, and a decrease in the other output\ractivations.  Similarly, if you decrease $z^L_4$ then $a^L_4$ will\rdecrease, and all the other output activations will increase.  In\rfact, if you look closely, you'll see that in both cases the total\rchange in the other activations exactly compensates for the change in\r$a^L_4$.  The reason is that the output activations are guaranteed to\ralways sum up to $1$, as we can prove using\rEquation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}and a little algebra:\\begin{eqnarray}\r  \\sum_j a^L_j & = & \\frac{\\sum_j e^{z^L_j}}{\\sum_k e^{z^L_k}} = 1.\r\\tag{79}\\end{eqnarray}\rAs a result, if $a^L_4$ increases, then the other output activations\rmust decrease by the same total amount, to ensure the sum over all\ractivations remains $1$.  And, of course, similar statements hold for\rall the other activations.",
            "Equation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}also implies that the output activations\rare all positive, since the exponential function is positive.\rCombining this with the observation in the last paragraph, we see that\rthe output from the softmax layer is a set of positive numbers which\rsum up to $1$.  In other words, the output from the softmax layer can\rbe thought of as a probability distribution.",
            "The fact that a softmax layer outputs a probability distribution is\rrather pleasing.  In many problems it's convenient to be able to\rinterpret the output activation $a^L_j$ as the network's estimate of\rthe probability that the correct output is $j$.  So, for instance, in\rthe MNIST classification problem, we can interpret $a^L_j$ as the\rnetwork's estimated probability that the correct digit classification\ris $j$.",
            "By contrast, if the output layer was a sigmoid layer, then we\rcertainly couldn't assume that the activations formed a probability\rdistribution.  I won't explicitly prove it, but it should be plausible\rthat the activations from a sigmoid layer won't in general form a\rprobability distribution.  And so with a sigmoid output layer we don't\rhave such a simple interpretation of the output activations.",
            "ExerciseConstruct an example showing explicitly that in a network with a\r  sigmoid output layer, the output activations $a^L_j$ won't always\r  sum to $1$.",
            "We're starting to build up some feel for the softmax function and the\rway softmax layers behave.  Just to review where we're at: the\rexponentials in Equation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}ensure that all the output\ractivations are positive.  And the sum in the denominator of\rEquation(78)\\begin{eqnarray} \r  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} \\nonumber\\end{eqnarray}ensures that the softmax outputs sum to\r$1$.  So that particular form no longer appears so mysterious: rather,\rit is a natural way to ensure that the output activations form a\rprobability distribution.  You can think of softmax as a way of\rrescaling the $z^L_j$, and then squishing them together to form a\rprobability distribution.",
            "ExercisesMonotonicity of softmaxShow that $\\partial a^L_j / \\partial\r  z^L_k$ is positive if $j = k$ and negative if $j \\neq k$.  As a\r  consequence, increasing $z^L_j$ is guaranteed to increase the\r  corresponding output activation, $a^L_j$, and will decrease all the\r  other output activations.  We already saw this empirically with the\r  sliders, but this is a rigorous proof.",
            "Non-locality of softmaxA nice thing about sigmoid layers is\r  that the output $a^L_j$ is a function of the corresponding weighted\r  input, $a^L_j = \\sigma(z^L_j)$.  Explain why this is not the case\r  for a softmax layer: any particular output activation $a^L_j$\r  depends onallthe weighted inputs.",
            "ProblemInverting the softmax layerSuppose we have a neural network\r  with a softmax output layer, and the activations $a^L_j$ are known.\r  Show that the corresponding weighted inputs have the form $z^L_j =\r  \\ln a^L_j + C$, for some constant $C$ that is independent of $j$.",
            "The learning slowdown problem:We've now built up\rconsiderable familiarity with softmax layers of neurons.  But we\rhaven't yet seen how a softmax layer lets us address the learning\rslowdown problem.  To understand that, let's define thelog-likelihoodcost function.  We'll use $x$ to denote a\rtraining input to the network, and $y$ to denote the corresponding\rdesired output.  Then the log-likelihood cost associated to this\rtraining input is\\begin{eqnarray}\r  C \\equiv -\\ln a^L_y.\r\\tag{80}\\end{eqnarray}\rSo, for instance, if we're training with MNIST images, and input an\rimage of a $7$, then the log-likelihood cost is $-\\ln a^L_7$.  To see\rthat this makes intuitive sense, consider the case when the network is\rdoing a good job, that is, it is confident the input is a $7$.  In\rthat case it will estimate a value for the corresponding probability\r$a^L_7$ which is close to $1$, and so the cost $-\\ln a^L_7$ will be\rsmall.  By contrast, when the network isn't doing such a good job, the\rprobability $a^L_7$ will be smaller, and the cost $-\\ln a^L_7$ will be\rlarger.  So the log-likelihood cost behaves as we'd expect a cost\rfunction to behave.",
            "What about the learning slowdown problem?  To analyze that, recall\rthat the key to the learning slowdown is the behaviour of the\rquantities $\\partial C / \\partial w^L_{jk}$ and $\\partial C / \\partial\rb^L_j$.  I won't go through the derivation explicitly - I'll ask you\rto do in the problems, below - but with a little algebra you can\rshow that**Note that I'm abusing notation here, using $y$ in a\r  slightly different way to last paragraph.  In the last paragraph we\r  used $y$ to denote the desired output from the network - e.g.,\r  output a \"$7$\" if an image of a $7$ was input.  But in the\r  equations which follow I'm using $y$ to denote the vector of output\r  activations which corresponds to $7$, that is, a vector which is all\r  $0$s, except for a $1$ in the $7$th location.\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial b^L_j} & = & a^L_j-y_j  \\tag{81}\\\\\r  \\frac{\\partial C}{\\partial w^L_{jk}} & = & a^{L-1}_k (a^L_j-y_j) \r\\tag{82}\\end{eqnarray}\rThese equations are the same as the analogous expressions obtained in\rour earlier analysis of the cross-entropy.  Compare, for example,\rEquation(82)\\begin{eqnarray}  \\frac{\\partial C}{\\partial w^L_{jk}} & = & a^{L-1}_k (a^L_j-y_j)  \\nonumber\\end{eqnarray}to Equation(67)\\begin{eqnarray} \r      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n} \\sum_x \r      a^{L-1}_k  (a^L_j-y_j).\r   \\nonumber\\end{eqnarray}.  It's the\rsame equation, albeit in the latter I've averaged over training\rinstances.  And, just as in the earlier analysis, these expressions\rensure that we will not encounter a learning slowdown.  In fact, it's\ruseful to think of a softmax output layer with log-likelihood cost as\rbeing quite similar to a sigmoid output layer with cross-entropy cost.",
            "",
            "",
            "",
            "Given this similarity, should you use a sigmoid output layer and\rcross-entropy, or a softmax output layer and log-likelihood?  In fact,\rin many situations both approaches work well.  Through the remainder\rof this chapter we'll use a sigmoid output layer, with the\rcross-entropy cost.  Later, inChapter 6, we'll\rsometimes use a softmax output layer, with log-likelihood cost.  The\rreason for the switch is to make some of our later networks more\rsimilar to networks found in certain influential academic papers.  As\ra more general point of principle, softmax plus log-likelihood is\rworth using whenever you want to interpret the output activations as\rprobabilities.  That's not always a concern, but can be useful with\rclassification problems (like MNIST) involving disjoint classes.",
            "ProblemsDerive Equations(81)\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^L_j} & = & a^L_j-y_j   \\nonumber\\end{eqnarray}and(82)\\begin{eqnarray}  \\frac{\\partial C}{\\partial w^L_{jk}} & = & a^{L-1}_k (a^L_j-y_j)  \\nonumber\\end{eqnarray}.",
            "Where does the \"softmax\" name come from?Suppose we change\r  the softmax function so the output activations are given by\\begin{eqnarray}\r  a^L_j = \\frac{e^{c z^L_j}}{\\sum_k e^{c z^L_k}},\r\\tag{83}\\end{eqnarray}\rwhere $c$ is a positive constant.  Note that $c = 1$ corresponds to\rthe standard softmax function.  But if we use a different value of $c$\rwe get a different function, which is nonetheless qualitatively rather\rsimilar to the softmax.  In particular, show that the output\ractivations form a probability distribution, just as for the usual\rsoftmax.  Suppose we allow $c$ to become large, i.e., $c \\rightarrow\r\\infty$.  What is the limiting value for the output activations\r$a^L_j$?  After solving this problem it should be clear to you why we\rthink of the $c = 1$ function as a \"softened\" version of the maximum\rfunction.  This is the origin of the term \"softmax\".",
            "Backpropagation with softmax and the log-likelihood costIn the\r  last chapter we derived the backpropagation algorithm for a network\r  containing sigmoid layers.  To apply the algorithm to a network with\r  a softmax layer we need to figure out an expression for the error\r  $\\delta^L_j \\equiv \\partial C / \\partial z^L_j$ in the final layer.\r  Show that a suitable expression is:\\begin{eqnarray}\r    \\delta^L_j = a^L_j -y_j.\r  \\tag{84}\\end{eqnarray}\r  Using this expression we can apply the backpropagation algorithm to\r  a network using a softmax output layer and the log-likelihood cost.",
            "",
            "Overfitting and regularization"
        ],
        "Overfitting and regularization": [
            "The Nobel prizewinning physicist Enrico Fermi was once asked his\ropinion of a mathematical model some colleagues had proposed as the\rsolution to an important unsolved physics problem.  The model gave\rexcellent agreement with experiment, but Fermi was skeptical.  He\rasked how many free parameters could be set in the model.  \"Four\"\rwas the answer.  Fermi replied**The quote comes from a\r  charming article byFreeman\r    Dyson, who is one of the people who proposed the flawed model. A\r  four-parameter elephant may be foundhere.:\r\"I remember my friend Johnny von Neumann used to say, with four\rparameters I can fit an elephant, and with five I can make him wiggle\rhis trunk.\".",
            "The point, of course, is that models with a large number of free\rparameters can describe an amazingly wide range of phenomena.  Even if\rsuch a model agrees well with the available data, that doesn't make it\ra good model.  It may just mean there's enough freedom in the model\rthat it can describe almost any data set of the given size, without\rcapturing any genuine insights into the underlying phenomenon.  When\rthat happens the model will work well for the existing data, but will\rfail to generalize to new situations.  The true test of a model is its\rability to make predictions in situations it hasn't been exposed to\rbefore.",
            "Fermi and von Neumann were suspicious of models with four parameters.\rOur 30 hidden neuron network for classifying MNIST digits has nearly\r24,000 parameters!  That's a lot of parameters.  Our 100 hidden neuron\rnetwork has nearly 80,000 parameters, and state-of-the-art deep neural\rnets sometimes contain millions or even billions of parameters.\rShould we trust the results?",
            "Let's sharpen this problem up by constructing a situation where our\rnetwork does a bad job generalizing to new situations.  We'll use our\r30 hidden neuron network, with its 23,860 parameters.  But we won't\rtrain the network using all 50,000 MNIST training images.  Instead,\rwe'll use just the first 1,000 training images.  Using that restricted\rset will make the problem with generalization much more evident.\rWe'll train in a similar way to before, using the cross-entropy cost\rfunction, with a learning rate of $\\eta = 0.5$ and a mini-batch size\rof $10$.  However, we'll train for 400 epochs, a somewhat larger\rnumber than before, because we're not using as many training examples.\rLet's usenetwork2to look at the way the cost function\rchanges:",
            ">>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data[:1000],400,10,0.5,evaluation_data=test_data,...monitor_evaluation_accuracy=True,monitor_training_cost=True)",
            "Using the results we can plot the way the cost changes as the network\rlearns**This and the next four graphs were generated by the\r  programoverfitting.py.:",
            "",
            "This looks encouraging, showing a smooth decrease in the cost, just as\rwe expect.  Note that I've only shown training epochs 200 through 399.\rThis gives us a nice up-close view of the later stages of learning,\rwhich, as we'll see, turns out to be where the interesting action is.",
            "Let's now look at how the classification accuracy on the test data\rchanges over time:",
            "",
            "Again, I've zoomed in quite a bit.  In the first 200 epochs (not\rshown) the accuracy rises to just under 82 percent.  The learning then\rgradually slows down.  Finally, at around epoch 280 the classification\raccuracy pretty much stops improving.  Later epochs merely see small\rstochastic fluctuations near the value of the accuracy at epoch 280.\rContrast this with the earlier graph, where the cost associated to the\rtraining data continues to smoothly drop.  If we just look at that\rcost, it appears that our model is still getting \"better\".  But the\rtest accuracy results show the improvement is an illusion.  Just like\rthe model that Fermi disliked, what our network learns after epoch 280\rno longer generalizes to the test data.  And so it's not useful\rlearning.  We say the network isoverfittingorovertrainingbeyond epoch 280.",
            "You might wonder if the problem here is that I'm looking at thecoston the training data, as opposed to theclassification accuracyon the test data.  In other words,\rmaybe the problem is that we're making an apples and oranges\rcomparison.  What would happen if we compared the cost on the training\rdata with the cost on the test data, so we're comparing similar\rmeasures?  Or perhaps we could compare the classification accuracy on\rboth the training data and the test data?  In fact, essentially the\rsame phenomenon shows up no matter how we do the comparison.  The\rdetails do change, however. For instance, let's look at the cost on\rthe test data:",
            "",
            "We can see that the cost on the test data improves until around epoch\r15, but after that it actually starts to get worse, even though the\rcost on the training data is continuing to get better.  This is\ranother sign that our model is overfitting.  It poses a puzzle,\rthough, which is whether we should regard epoch 15 or epoch 280 as the\rpoint at which overfitting is coming to dominate learning?  From a\rpractical point of view, what we really care about is improving\rclassification accuracy on the test data, while the cost on the test\rdata is no more than a proxy for classification accuracy.  And so it\rmakes most sense to regard epoch 280 as the point beyond which\roverfitting is dominating learning in our neural network.",
            "Another sign of overfitting may be seen in the classification accuracy\ron the training data:",
            "",
            "The accuracy rises all the way up to $100$ percent.  That is, our\rnetwork correctly classifies all $1,000$ training images!  Meanwhile,\rour test accuracy tops out at just $82.27$ percent.  So our network\rreally is learning about peculiarities of the training set, not just\rrecognizing digits in general.  It's almost as though our network is\rmerely memorizing the training set, without understanding digits well\renough to generalize to the test set.",
            "Overfitting is a major problem in neural networks.  This is especially\rtrue in modern networks, which often have very large numbers of\rweights and biases.  To train effectively, we need a way of detecting\rwhen overfitting is going on, so we don't overtrain.  And we'd like to\rhave techniques for reducing the effects of overfitting.",
            "The obvious way to detect overfitting is to use the approach above,\rkeeping track of accuracy on the test data as our network trains.  If\rwe see that the accuracy on the test data is no longer improving, then\rwe should stop training.  Of course, strictly speaking, this is not\rnecessarily a sign of overfitting.  It might be that accuracy on the\rtest data and the training data both stop improving at the same time.\rStill, adopting this strategy will prevent overfitting.",
            "In fact, we'll use a variation on this strategy.  Recall that when we\rload in the MNIST data we load in three data sets:>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()Up to now we've been using thetraining_dataandtest_data, and ignoring thevalidation_data.  Thevalidation_datacontains $10,000$ images of digits, images\rwhich are different from the $50,000$ images in the MNIST training\rset, and the $10,000$ images in the MNIST test set.  Instead of using\rthetest_datato prevent overfitting, we will use thevalidation_data.  To do this, we'll use much the same strategy\ras was described above for thetest_data.  That is, we'll\rcompute the classification accuracy on thevalidation_dataat\rthe end of each epoch.  Once the classification accuracy on thevalidation_datahas saturated, we stop training.  This strategy\ris calledearly stopping.  Of course, in practice we won't\rimmediately know when the accuracy has saturated.  Instead, we\rcontinue training until we're confident that the accuracy has\rsaturated**It requires some judgment to determine when to\r  stop.  In my earlier graphs I identified epoch 280 as the place at\r  which accuracy saturated.  It's possible that was too pessimistic.\r  Neural networks sometimes plateau for a while in training, before\r  continuing to improve.  I wouldn't be surprised if more learning\r  could have occurred even after epoch 400, although the magnitude of\r  any further improvement would likely be small.  So it's possible to\r  adopt more or less aggressive strategies for early stopping..",
            "",
            "Why use thevalidation_datato prevent overfitting, rather than\rthetest_data?  In fact, this is part of a more general\rstrategy, which is to use thevalidation_datato evaluate\rdifferent trial choices of hyper-parameters such as the number of\repochs to train for, the learning rate, the best network architecture,\rand so on.  We use such evaluations to find and set good values for\rthe hyper-parameters.  Indeed, although I haven't mentioned it until\rnow, that is, in part, how I arrived at the hyper-parameter choices\rmade earlier in this book. (More on thislater.)",
            "Of course, that doesn't in any way answer the question of why we're\rusing thevalidation_datato prevent overfitting, rather than\rthetest_data.  Instead, it replaces it with a more general\rquestion, which is why we're using thevalidation_datarather\rthan thetest_datato set good hyper-parameters?  To understand\rwhy, consider that when setting hyper-parameters we're likely to try\rmany different choices for the hyper-parameters.  If we set the\rhyper-parameters based on evaluations of thetest_datait's\rpossible we'll end up overfitting our hyper-parameters to thetest_data.  That is, we may end up finding hyper-parameters\rwhich fit particular peculiarities of thetest_data, but where\rthe performance of the network won't generalize to other data sets.\rWe guard against that by figuring out the hyper-parameters using thevalidation_data.  Then, once we've got the hyper-parameters we\rwant, we do a final evaluation of accuracy using thetest_data.\rThat gives us confidence that our results on thetest_dataare\ra true measure of how well our neural network generalizes.  To put it\ranother way, you can think of the validation data as a type of\rtraining data that helps us learn good hyper-parameters.  This\rapproach to finding good hyper-parameters is sometimes known as thehold outmethod, since thevalidation_datais kept apart\ror \"held out\" from thetraining_data.",
            "Now, in practice, even after evaluating performance on thetest_datawe may change our minds and want to try another\rapproach - perhaps a different network architecture - which will\rinvolve finding a new set of hyper-parameters.  If we do this, isn't\rthere a danger we'll end up overfitting to thetest_dataas\rwell?  Do we need a potentially infinite regress of data sets, so we\rcan be confident our results will generalize?  Addressing this concern\rfully is a deep and difficult problem.  But for our practical\rpurposes, we're not going to worry too much about this question.\rInstead, we'll plunge ahead, using the basic hold out method, based on\rthetraining_data,validation_data, andtest_data, as described above.",
            "We've been looking so far at overfitting when we're just using 1,000\rtraining images.  What happens when we use the full training set of\r50,000 images?  We'll keep all the other parameters the same (30\rhidden neurons, learning rate 0.5, mini-batch size of 10), but train\rusing all 50,000 images for 30 epochs.  Here's a graph showing the\rresults for the classification accuracy on both the training data and\rthe test data.  Note that I've used the test data here, rather than\rthe validation data, in order to make the results more directly\rcomparable with the earlier graphs.",
            "",
            "As you can see, the accuracy on the test and training data remain much\rcloser together than when we were using 1,000 training examples.  In\rparticular, the best classification accuracy of $97.86$ percent on the\rtraining data is only $2.53$ percent higher than the $95.33$ percent\ron the test data.  That's compared to the $17.73$ percent gap we had\rearlier!  Overfitting is still going on, but it's been greatly\rreduced.  Our network is generalizing much better from the training\rdata to the test data.  In general, one of the best ways of reducing\roverfitting is to increase the size of the training data.  With enough\rtraining data it is difficult for even a very large network to\roverfit.  Unfortunately, training data can be expensive or difficult\rto acquire, so this is not always a practical option.",
            "Regularization",
            "Increasing the amount of training data is one way of reducing\roverfitting.  Are there other ways we can reduce the extent to which\roverfitting occurs?  One possible approach is to reduce the size of\rour network. However, large networks have the potential to be more\rpowerful than small networks, and so this is an option we'd only adopt\rreluctantly.",
            "Fortunately, there are other techniques which can reduce overfitting,\reven when we have a fixed network and fixed training data.  These are\rknown asregularizationtechniques.  In this section I describe\rone of the most commonly used regularization techniques, a technique\rsometimes known asweight decayorL2 regularization.\rThe idea of L2 regularization is to add an extra term to the cost\rfunction, a term called theregularization term.  Here's the\rregularized cross-entropy:",
            "\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\r(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\r\\tag{85}\\end{eqnarray}",
            "The first term is just the usual expression for the cross-entropy.\rBut we've added a second term, namely the sum of the squares of all\rthe weights in the network.  This is scaled by a factor $\\lambda /\r2n$, where $\\lambda > 0$ is known as theregularization\r  parameter, and $n$ is, as usual, the size of our training set.\rI'll discuss later how $\\lambda$ is chosen.  It's also worth noting\rthat the regularization termdoesn'tinclude the biases.  I'll\ralso come back to that below.",
            "Of course, it's possible to regularize other cost functions, such as\rthe quadratic cost.  This can be done in a similar way:",
            "\\begin{eqnarray} C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 +\r  \\frac{\\lambda}{2n} \\sum_w w^2.\r\\tag{86}\\end{eqnarray}",
            "In both cases we can write the regularized cost function as\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\r\\sum_w w^2,\r\\tag{87}\\end{eqnarray} where $C_0$ is the original, unregularized cost\rfunction.",
            "Intuitively, the effect of regularization is to make it so the network\rprefers to learn small weights, all other things being equal.  Large\rweights will only be allowed if they considerably improve the first\rpart of the cost function.  Put another way, regularization can be\rviewed as a way of compromising between finding small weights and\rminimizing the original cost function.  The relative importance of the\rtwo elements of the compromise depends on the value of $\\lambda$: when\r$\\lambda$ is small we prefer to minimize the original cost function,\rbut when $\\lambda$ is large we prefer small weights.",
            "Now, it's really not at all obvious why making this kind of compromise\rshould help reduce overfitting!  But it turns out that it does. We'll\raddress the question of why it helps in the next section.  But first,\rlet's work through an example showing that regularization really does\rreduce overfitting.",
            "To construct such an example, we first need to figure out how to apply\rour stochastic gradient descent learning algorithm in a regularized\rneural network.  In particular, we need to know how to compute the\rpartial derivatives $\\partial C / \\partial w$ and $\\partial C\r/ \\partial b$ for all the weights and biases in the network.  Taking\rthe partial derivatives of Equation(87)\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\r\\sum_w w^2 \\nonumber\\end{eqnarray}gives",
            "\\begin{eqnarray} \r  \\frac{\\partial C}{\\partial w} & = & \\frac{\\partial C_0}{\\partial w} + \r  \\frac{\\lambda}{n} w \\tag{88}\\\\ \r  \\frac{\\partial C}{\\partial b} & = & \\frac{\\partial C_0}{\\partial b}.\r\\tag{89}\\end{eqnarray}",
            "The $\\partial C_0 / \\partial w$ and $\\partial C_0 / \\partial b$ terms\rcan be computed using backpropagation, as described inthe last chapter.  And so we see that it's easy to\rcompute the gradient of the regularized cost function: just use\rbackpropagation, as usual, and then add $\\frac{\\lambda}{n} w$ to the\rpartial derivative of all the weight terms.  The partial derivatives\rwith respect to the biases are unchanged, and so the gradient descent\rlearning rule for the biases doesn't change from the usual rule:",
            "\\begin{eqnarray}\rb & \\rightarrow & b -\\eta \\frac{\\partial C_0}{\\partial b}.\r\\tag{90}\\end{eqnarray}",
            "The learning rule for the weights becomes:",
            "\\begin{eqnarray} \r  w & \\rightarrow & w-\\eta \\frac{\\partial C_0}{\\partial\r    w}-\\frac{\\eta \\lambda}{n} w \\tag{91}\\\\ \r  & = & \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial\r    C_0}{\\partial w}. \r\\tag{92}\\end{eqnarray}",
            "This is exactly the same as the usual gradient descent learning rule,\rexcept we first rescale the weight $w$ by a factor $1-\\frac{\\eta\r  \\lambda}{n}$.  This rescaling is sometimes referred to asweight decay, since it makes the weights smaller.  At first\rglance it looks as though this means the weights are being driven\runstoppably toward zero.  But that's not right, since the other term\rmay lead the weights to increase, if so doing causes a decrease in the\runregularized cost function.",
            "Okay, that's how gradient descent works.  What about stochastic\rgradient descent?  Well, just as in unregularized stochastic gradient\rdescent, we can estimate $\\partial C_0 / \\partial w$ by averaging over\ra mini-batch of $m$ training examples.  Thus the regularized learning\rrule for stochastic gradient descent becomes\r(c.f. Equation(20)\\begin{eqnarray} \r  w_k & \\rightarrow & w_k' = w_k-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k}  \\nonumber\\end{eqnarray})",
            "\\begin{eqnarray} \r  w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\r  \\sum_x \\frac{\\partial C_x}{\\partial w}, \r\\tag{93}\\end{eqnarray}",
            "where the sum is over training examples $x$ in the mini-batch, and\r$C_x$ is the (unregularized) cost for each training example.  This is\rexactly the same as the usual rule for stochastic gradient descent,\rexcept for the $1-\\frac{\\eta \\lambda}{n}$ weight decay factor.\rFinally, and for completeness, let me state the regularized learning\rrule for the biases.  This is, of course, exactly the same as in the\runregularized case (c.f. Equation(21)\\begin{eqnarray}  \r  b_l & \\rightarrow & b_l' = b_l-\\frac{\\eta}{m}\r  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l} \\nonumber\\end{eqnarray}),",
            "\\begin{eqnarray}\r  b \\rightarrow b - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\r\\tag{94}\\end{eqnarray}\rwhere the sum is over training examples $x$ in the mini-batch.",
            "Let's see how regularization changes the performance of our neural\rnetwork. We'll use a network with $30$ hidden neurons, a mini-batch\rsize of $10$, a learning rate of $0.5$, and the cross-entropy cost\rfunction.  However, this time we'll use a regularization parameter of\r$\\lambda = 0.1$.  Note that in the code, we use the variable namelmbda, becauselambdais a reserved word in Python, with\ran unrelated meaning.  I've also used thetest_dataagain, not\rthevalidation_data.  Strictly speaking, we should use thevalidation_data, for all the reasons we discussed earlier.  But\rI decided to use thetest_databecause it makes the results\rmore directly comparable with our earlier, unregularized results.  You\rcan easily change the code to use thevalidation_datainstead,\rand you'll find that it gives similar results.>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data[:1000],400,10,0.5,...evaluation_data=test_data,lmbda=0.1,...monitor_evaluation_cost=True,monitor_evaluation_accuracy=True,...monitor_training_cost=True,monitor_training_accuracy=True)The cost on the training data decreases over the whole time, much as\rit did in the earlier, unregularized case**This and the next\r  two graphs were produced with the programoverfitting.py.:",
            "",
            "But this time the accuracy on thetest_datacontinues to\rincrease for the entire 400 epochs:",
            "",
            "Clearly, the use of regularization has suppressed overfitting.  What's\rmore, the accuracy is considerably higher, with a peak classification\raccuracy of $87.1$ percent, compared to the peak of $82.27$ percent\robtained in the unregularized case.  Indeed, we could almost certainly\rget considerably better results by continuing to train past 400\repochs. It seems that, empirically, regularization is causing our\rnetwork to generalize better, and considerably reducing the effects of\roverfitting.",
            "What happens if we move out of the artificial environment of just\rhaving 1,000 training images, and return to the full 50,000 image\rtraining set?  Of course, we've seen already that overfitting is much\rless of a problem with the full 50,000 images.  Does regularization\rhelp any further?  Let's keep the hyper-parameters the same as before\r- $30$ epochs, learning rate $0.5$, mini-batch size of $10$.\rHowever, we need to modify the regularization parameter.  The reason\ris because the size $n$ of the training set has changed from $n =\r1,000$ to $n = 50,000$, and this changes the weight decay factor $1 -\r\\frac{\\eta \\lambda}{n}$.  If we continued to use $\\lambda = 0.1$ that\rwould mean much less weight decay, and thus much less of a\rregularization effect.  We compensate by changing to $\\lambda = 5.0$.",
            "Okay, let's train our network, stopping first to re-initialize the\rweights:>>>net.large_weight_initializer()>>>net.SGD(training_data,30,10,0.5,...evaluation_data=test_data,lmbda=5.0,...monitor_evaluation_accuracy=True,monitor_training_accuracy=True)We obtain the results:",
            "",
            "There's lots of good news here.  First, our classification accuracy on\rthe test data is up, from $95.49$ percent when running unregularized,\rto $96.49$ percent.  That's a big improvement.  Second, we can see\rthat the gap between results on the training and test data is much\rnarrower than before, running at under a percent.  That's still a\rsignificant gap, but we've obviously made substantial progress\rreducing overfitting.",
            "",
            "Finally, let's see what test classification accuracy we get when we\ruse 100 hidden neurons and a regularization parameter of $\\lambda =\r5.0$. I won't go through a detailed analysis of overfitting here, this\ris purely for fun, just to see how high an accuracy we can get when we\ruse our new tricks: the cross-entropy cost function and L2\rregularization.",
            ">>>net=network2.Network([784,100,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data,30,10,0.5,lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True)",
            "The final result is a classification accuracy of $97.92$ percent on\rthe validation data.  That's a big jump from the 30 hidden neuron\rcase.In fact, tuning just\ra little more, to run for 60 epochs at $\\eta = 0.1$ and $\\lambda =\r5.0$ we break the $98$ percent barrier, achieving $98.04$ percent\rclassification accuracy on the validation data.  Not bad for what\rturns out to be 152 lines of code!",
            "I've described regularization as a way to reduce overfitting and to\rincrease classification accuracies.  In fact, that's not the only\rbenefit.  Empirically, when doing multiple runs of our MNIST networks,\rbut with different (random) weight initializations, I've found that\rthe unregularized runs will occasionally get \"stuck\", apparently\rcaught in local minima of the cost function.  The result is that\rdifferent runs sometimes provide quite different results.  By\rcontrast, the regularized runs have provided much more easily\rreplicable results.",
            "Why is this going on?  Heuristically, if the cost function is\runregularized, then the length of the weight vector is likely to grow,\rall other things being equal.  Over time this can lead to the weight\rvector being very large indeed.  This can cause the weight vector to\rget stuck pointing in more or less the same direction, since changes\rdue to gradient descent only make tiny changes to the direction, when\rthe length is long.  I believe this phenomenon is making it hard for\rour learning algorithm to properly explore the weight space, and\rconsequently harder to find good minima of the cost function.",
            "Why does regularization help reduce overfitting?",
            "We've seen empirically that regularization helps reduce overfitting.\rThat's encouraging but, unfortunately, it's not obvious why\rregularization helps!  A standard story people tell to explain what's\rgoing on is along the following lines: smaller weights are, in some\rsense, lower complexity, and so provide a simpler and more powerful\rexplanation for the data, and should thus be preferred.  That's a\rpretty terse story, though, and contains several elements that perhaps\rseem dubious or mystifying.  Let's unpack the story and examine it\rcritically.  To do that, let's suppose we have a simple data set for\rwhich we wish to build a model:",
            "",
            "Implicitly, we're studying some real-world phenomenon here, with $x$\rand $y$ representing real-world data.  Our goal is to build a model\rwhich lets us predict $y$ as a function of $x$.  We could try using\rneural networks to build such a model, but I'm going to do something\reven simpler: I'll try to model $y$ as a polynomial in $x$.  I'm doing\rthis instead of using neural nets because using polynomials will make\rthings particularly transparent.  Once we've understood the polynomial\rcase, we'll translate to neural networks.  Now, there are ten points\rin the graph above, which means we can find a unique $9$th-order\rpolynomial $y = a_0 x^9 + a_1 x^8 + \\ldots + a_9$ which fits the data\rexactly.  Here's the graph of that polynomial**I won't show\r  the coefficients explicitly, although they are easy to find using a\r  routine such as Numpy'spolyfit.  You can view the exact form\r  of the polynomial in thesource code\r    for the graphif you're curious.  It's the functionp(x)defined starting on line 14 of the program which produces the\r  graph.:",
            "",
            "That provides an exact fit.  But we can also get a good fit using the\rlinear model $y = 2x$:",
            "",
            "Which of these is the better model?  Which is more likely to be true?\rAnd which model is more likely to generalize well to other examples of\rthe same underlying real-world phenomenon?",
            "These are difficult questions.  In fact, we can't determine with\rcertainty the answer to any of the above questions, without much more\rinformation about the underlying real-world phenomenon.  But let's\rconsider two possibilities: (1) the $9$th order polynomial is, in\rfact, the model which truly describes the real-world phenomenon, and\rthe model will therefore generalize perfectly; (2) the correct model\ris $y = 2x$, but there's a little additional noise due to, say,\rmeasurement error, and that's why the model isn't an exact fit.",
            "It's nota prioripossible to say which of these two\rpossibilities is correct.  (Or, indeed, if some third possibility\rholds).  Logically, either could be true.  And it's not a trivial\rdifference.  It's true that on the data provided there's only a small\rdifference between the two models.  But suppose we want to predict the\rvalue of $y$ corresponding to some large value of $x$, much larger\rthan any shown on the graph above.  If we try to do that there will be\ra dramatic difference between the predictions of the two models, as\rthe $9$th order polynomial model comes to be dominated by the $x^9$\rterm, while the linear model remains, well, linear.",
            "One point of view is to say that in science we should go with the\rsimpler explanation, unless compelled not to.  When we find a simple\rmodel that seems to explain many data points we are tempted to shout\r\"Eureka!\"  After all, it seems unlikely that a simple explanation\rshould occur merely by coincidence.  Rather, we suspect that the model\rmust be expressing some underlying truth about the phenomenon.  In the\rcase at hand, the model $y = 2x+{\\rm noise}$ seems much simpler than\r$y = a_0 x^9 + a_1 x^8 + \\ldots$.  It would be surprising if that\rsimplicity had occurred by chance, and so we suspect that $y = 2x+{\\rm\r  noise}$ expresses some underlying truth.  In this point of view, the\r9th order model is really just learning the effects of local\rnoise. And so while the 9th order model works perfectly for these\rparticular data points, the model will fail to generalize to other\rdata points, and the noisy linear model will have greater predictive\rpower.",
            "Let's see what this point of view means for neural networks.  Suppose\rour network mostly has small weights, as will tend to happen in a\rregularized network.  The smallness of the weights means that the\rbehaviour of the network won't change too much if we change a few\rrandom inputs here and there.  That makes it difficult for a\rregularized network to learn the effects of local noise in the data.\rThink of it as a way of making it so single pieces of evidence don't\rmatter too much to the output of the network.  Instead, a regularized\rnetwork learns to respond to types of evidence which are seen often\racross the training set.  By contrast, a network with large weights\rmay change its behaviour quite a bit in response to small changes in\rthe input.  And so an unregularized network can use large weights to\rlearn a complex model that carries a lot of information about the\rnoise in the training data.  In a nutshell, regularized networks are\rconstrained to build relatively simple models based on patterns seen\roften in the training data, and are resistant to learning\rpeculiarities of the noise in the training data.  The hope is that\rthis will force our networks to do real learning about the phenomenon\rat hand, and to generalize better from what they learn.",
            "With that said, this idea of preferring simpler explanation should\rmake you nervous.  People sometimes refer to this idea as \"Occam's\rRazor\", and will zealously apply it as though it has the status of\rsome general scientific principle.  But, of course, it's not a general\rscientific principle.  There is noa priorilogical reason to\rprefer simple explanations over more complex explanations.  Indeed,\rsometimes the more complex explanation turns out to be correct.",
            "Let me describe two examples where more complex explanations have\rturned out to be correct.  In the 1940s the physicist Marcel Schein\rannounced the discovery of a new particle of nature.  The company he\rworked for, General Electric, was ecstatic, and publicized the\rdiscovery widely.  But the physicist Hans Bethe was skeptical.  Bethe\rvisited Schein, and looked at the plates showing the tracks of\rSchein's new particle.  Schein showed Bethe plate after plate, but on\reach plate Bethe identified some problem that suggested the data\rshould be discarded.  Finally, Schein showed Bethe a plate that looked\rgood.  Bethe said it might just be a statistical fluke.  Schein:\r\"Yes, but the chance that this would be statistics, even according to\ryour own formula, is one in five.\"  Bethe: \"But we have already\rlooked at five plates.\"  Finally, Schein said: \"But on my plates,\reach one of the good plates, each one of the good pictures, you\rexplain by a different theory, whereas I have one hypothesis that\rexplains all the plates, that they are [the new particle].\"  Bethe\rreplied: \"The sole difference between your and my explanations is\rthat yours is wrong and all of mine are right.  Your single\rexplanation is wrong, and all of my multiple explanations are right.\"\rSubsequent work confirmed that Nature agreed with Bethe, and Schein's\rparticle is no more**The story is related by the physicist\r  Richard Feynman in aninterviewwith the historian Charles Weiner..",
            "As a second example, in 1859 the astronomer Urbain Le Verrier observed\rthat the orbit of the planet Mercury doesn't have quite the shape that\rNewton's theory of gravitation says it should have.  It was a tiny,\rtiny deviation from Newton's theory, and several of the explanations\rproferred at the time boiled down to saying that Newton's theory was\rmore or less right, but needed a tiny alteration.  In 1916, Einstein\rshowed that the deviation could be explained very well using his\rgeneral theory of relativity, a theory radically different to\rNewtonian gravitation, and based on much more complex mathematics.\rDespite that additional complexity, today it's accepted that\rEinstein's explanation is correct, and Newtonian gravity, even in its\rmodified forms, is wrong.  This is in part because we now know that\rEinstein's theory explains many other phenomena which Newton's theory\rhas difficulty with.  Furthermore, and even more impressively,\rEinstein's theory accurately predicts several phenomena which aren't\rpredicted by Newtonian gravity at all. But these impressive qualities\rweren't entirely obvious in the early days.  If one had judged merely\ron the grounds of simplicity, then some modified form of Newton's\rtheory would arguably have been more attractive.",
            "There are three morals to draw from these stories.  First, it can be\rquite a subtle business deciding which of two explanations is truly\r\"simpler\".  Second, even if we can make such a judgment, simplicity\ris a guide that must be used with great caution!  Third, the true test\rof a model is not simplicity, but rather how well it does in\rpredicting new phenomena, in new regimes of behaviour.",
            "With that said, and keeping the need for caution in mind, it's an\rempirical fact that regularized neural networks usually generalize\rbetter than unregularized networks.  And so through the remainder of\rthe book we will make frequent use of regularization.  I've included\rthe stories above merely to help convey why no-one has yet developed\ran entirely convincing theoretical explanation for why regularization\rhelps networks generalize.  Indeed, researchers continue to write\rpapers where they try different approaches to regularization, compare\rthem to see which works better, and attempt to understand why different\rapproaches work better or worse.  And so you can view regularization\ras something of a kludge.  While it often helps, we don't have an\rentirely satisfactory systematic understanding of what's going on,\rmerely incomplete heuristics and rules of thumb.",
            "There's a deeper set of issues here, issues which go to the heart of\rscience.  It's the question of how we generalize.  Regularization may\rgive us a computational magic wand that helps our networks generalize\rbetter, but it doesn't give us a principled understanding of how\rgeneralization works, nor of what the best approach is**These\r  issues go back to theproblem\r    of induction, famously discussed by the Scottish philosopher\r  David Hume in\"An\r    Enquiry Concerning Human Understanding\"(1748).  The problem of\r  induction has been given a modern machine learning form in the\r  no-free lunch theorem\r  (link)\r  of David Wolpert and William Macready (1997)..",
            "This is particularly galling because in everyday life, we humans\rgeneralize phenomenally well.  Shown just a few images of an elephant\ra child will quickly learn to recognize other elephants.  Of course,\rthey may occasionally make mistakes, perhaps confusing a rhinoceros\rfor an elephant, but in general this process works remarkably\raccurately.  So we have a system - the human brain - with a huge\rnumber of free parameters.  And after being shown just one or a few\rtraining images that system learns to generalize to other images.  Our\rbrains are, in some sense, regularizing amazingly well!  How do we do\rit?  At this point we don't know.  I expect that in years to come we\rwill develop more powerful techniques for regularization in artificial\rneural networks, techniques that will ultimately enable neural nets to\rgeneralize well even from small data sets.",
            "In fact, our networks already generalize better than one mighta\r  prioriexpect.  A network with 100 hidden neurons has nearly 80,000\rparameters.  We have only 50,000 images in our training data.  It's\rlike trying to fit an 80,000th degree polynomial to 50,000 data\rpoints.  By all rights, our network should overfit terribly.  And yet,\ras we saw earlier, such a network actually does a pretty good job\rgeneralizing.  Why is that the case?  It's not well understood.  It\rhas been conjectured**InGradient-Based\r    Learning Applied to Document Recognition, by Yann LeCun,\r  Léon Bottou, Yoshua Bengio, and Patrick Haffner\r  (1998).that \"the dynamics of gradient descent learning in\rmultilayer nets has a `self-regularization' effect\".  This is\rexceptionally fortunate, but it's also somewhat disquieting that we\rdon't understand why it's the case.  In the meantime, we will adopt\rthe pragmatic approach and use regularization whenever we can.  Our\rneural networks will be the better for it.",
            "Let me conclude this section by returning to a detail which I left\runexplained earlier: the fact that L2 regularizationdoesn'tconstrain the biases.  Of course, it would be easy to modify the\rregularization procedure to regularize the biases.  Empirically, doing\rthis often doesn't change the results very much, so to some extent\rit's merely a convention whether to regularize the biases or not.\rHowever, it's worth noting that having a large bias doesn't make a\rneuron sensitive to its inputs in the same way as having large\rweights.  And so we don't need to worry about large biases enabling\rour network to learn the noise in our training data.  At the same\rtime, allowing large biases gives our networks more flexibility in\rbehaviour - in particular, large biases make it easier for neurons\rto saturate, which is sometimes desirable.  For these reasons we don't\rusually include bias terms when regularizing.",
            "Other techniques for regularization",
            "There are many regularization techniques other than L2 regularization.\rIn fact, so many techniques have been developed that I can't possibly\rsummarize them all.  In this section I briefly describe three other\rapproaches to reducing overfitting: L1 regularization, dropout, and\rartificially increasing the training set size.  We won't go into\rnearly as much depth studying these techniques as we did earlier.\rInstead, the purpose is to get familiar with the main ideas, and to\rappreciate something of the diversity of regularization techniques\ravailable.",
            "L1 regularization:In this approach we modify the\runregularized cost function by adding the sum of the absolute values\rof the weights:",
            "\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{n} \\sum_w |w|.\r\\tag{95}\\end{eqnarray}",
            "Intuitively, this is similar to L2 regularization, penalizing large\rweights, and tending to make the network prefer small weights.  Of\rcourse, the L1 regularization term isn't the same as the L2\rregularization term, and so we shouldn't expect to get exactly the\rsame behaviour.  Let's try to understand how the behaviour of a\rnetwork trained using L1 regularization differs from a network trained\rusing L2 regularization.",
            "To do that, we'll look at the partial derivatives of the cost\rfunction.  Differentiating(95)\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{n} \\sum_w |w| \\nonumber\\end{eqnarray}we obtain:\\begin{eqnarray}  \\frac{\\partial C}{\\partial\r    w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\, {\\rm\r    sgn}(w),\r\\tag{96}\\end{eqnarray}",
            "where ${\\rm sgn}(w)$ is the sign of $w$, that is, $+1$ if $w$ is\rpositive, and $-1$ if $w$ is negative.  Using this expression, we can\reasily modify backpropagation to do stochastic gradient descent using\rL1 regularization.  The resulting update rule for an L1 regularized\rnetwork is\\begin{eqnarray}  w \\rightarrow w' =\r  w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w) - \\eta \\frac{\\partial\r    C_0}{\\partial w},\r\\tag{97}\\end{eqnarray}",
            "where, as per usual, we can estimate $\\partial C_0 / \\partial w$ using\ra mini-batch average, if we wish.  Compare that to the update rule for\rL2 regularization (c.f. Equation(93)\\begin{eqnarray} \r  w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\r  \\sum_x \\frac{\\partial C_x}{\\partial w},  \\nonumber\\end{eqnarray}),\\begin{eqnarray}\r  w \\rightarrow w' = w\\left(1 - \\frac{\\eta \\lambda}{n} \\right)\r  - \\eta \\frac{\\partial C_0}{\\partial w}.\r\\tag{98}\\end{eqnarray}\rIn both expressions the effect of regularization is to shrink the\rweights.  This accords with our intuition that both kinds of\rregularization penalize large weights.  But the way the weights shrink\ris different.  In L1 regularization, the weights shrink by a constant\ramount toward $0$.  In L2 regularization, the weights shrink by an\ramount which is proportional to $w$.  And so when a particular weight\rhas a large magnitude, $|w|$, L1 regularization shrinks the weight\rmuch less than L2 regularization does.  By contrast, when $|w|$ is\rsmall, L1 regularization shrinks the weight much more than L2\rregularization.  The net result is that L1 regularization tends to\rconcentrate the weight of the network in a relatively small number of\rhigh-importance connections, while the other weights are driven toward\rzero.",
            "I've glossed over an issue in the above discussion, which is that the\rpartial derivative $\\partial C / \\partial w$ isn't defined when $w =\r0$.  The reason is that the function $|w|$ has a sharp \"corner\" at\r$w = 0$, and so isn't differentiable at that point.  That's okay,\rthough.  What we'll do is just apply the usual (unregularized) rule\rfor stochastic gradient descent when $w = 0$.  That should be okay -\rintuitively, the effect of regularization is to shrink weights, and\robviously it can't shrink a weight which is already $0$.  To put it\rmore precisely, we'll use Equations(96)\\begin{eqnarray}  \\frac{\\partial C}{\\partial\r    w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\, {\\rm\r    sgn}(w) \\nonumber\\end{eqnarray}and(97)\\begin{eqnarray}  w \\rightarrow w' =\r  w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w) - \\eta \\frac{\\partial\r    C_0}{\\partial w} \\nonumber\\end{eqnarray}with the convention that $\\mbox{sgn}(0) = 0$.\rThat gives a nice, compact rule for doing stochastic gradient descent\rwith L1 regularization.",
            "Dropout:Dropout is a radically different technique for\rregularization.  Unlike L1 and L2 regularization, dropout doesn't rely\ron modifying the cost function.  Instead, in dropout we modify the\rnetwork itself.  Let me describe the basic mechanics of how dropout\rworks, before getting into why it works, and what the results are.",
            "Suppose we're trying to train a network:",
            "",
            "In particular, suppose we have a training input $x$ and corresponding\rdesired output $y$.  Ordinarily, we'd train by forward-propagating $x$\rthrough the network, and then backpropagating to determine the\rcontribution to the gradient.  With dropout, this process is modified.\rWe start by randomly (and temporarily) deleting half the hidden\rneurons in the network, while leaving the input and output neurons\runtouched.  After doing this, we'll end up with a network along the\rfollowing lines.  Note that the dropout neurons, i.e., the neurons\rwhich have been temporarily deleted, are still ghosted in:",
            "",
            "We forward-propagate the input $x$ through the modified network, and\rthen backpropagate the result, also through the modified network.\rAfter doing this over a mini-batch of examples, we update the\rappropriate weights and biases.  We then repeat the process, first\rrestoring the dropout neurons, then choosing a new random subset of\rhidden neurons to delete, estimating the gradient for a different\rmini-batch, and updating the weights and biases in the network.",
            "",
            "",
            "",
            "",
            "By repeating this process over and over, our network will learn a set\rof weights and biases.  Of course, those weights and biases will have\rbeen learnt under conditions in which half the hidden neurons were\rdropped out.  When we actually run the full network that means that\rtwice as many hidden neurons will be active.  To compensate for that,\rwe halve the weights outgoing from the hidden neurons.",
            "This dropout procedure may seem strange andad hoc.  Why would\rwe expect it to help with regularization?  To explain what's going on,\rI'd like you to briefly stop thinking about dropout, and instead\rimagine training neural networks in the standard way (no dropout).  In\rparticular, imagine we train several different neural networks, all\rusing the same training data.  Of course, the networks may not start\rout identical, and as a result after training they may sometimes give\rdifferent results.  When that happens we could use some kind of\raveraging or voting scheme to decide which output to accept.  For\rinstance, if we have trained five networks, and three of them are\rclassifying a digit as a \"3\", then it probably really is a \"3\".\rThe other two networks are probably just making a mistake.  This kind\rof averaging scheme is often found to be a powerful (though expensive)\rway of reducing overfitting.  The reason is that the different\rnetworks may overfit in different ways, and averaging may help\reliminate that kind of overfitting.",
            "What's this got to do with dropout?  Heuristically, when we dropout\rdifferent sets of neurons, it's rather like we're training different\rneural networks.  And so the dropout procedure is like averaging the\reffects of a very large number of different networks.  The different\rnetworks will overfit in different ways, and so, hopefully, the net\reffect of dropout will be to reduce overfitting.",
            "",
            "A related heuristic explanation for dropout is given in one of the\rearliest papers to use the\rtechnique**ImageNet\r    Classification with Deep Convolutional Neural Networks, by Alex\r  Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).: \"This\rtechnique reduces complex co-adaptations of neurons, since a neuron\rcannot rely on the presence of particular other neurons. It is,\rtherefore, forced to learn more robust features that are useful in\rconjunction with many different random subsets of the other neurons.\"\rIn other words, if we think of our network as a model which is making\rpredictions, then we can think of dropout as a way of making sure that\rthe model is robust to the loss of any individual piece of evidence.\rIn this, it's somewhat similar to L1 and L2 regularization, which tend\rto reduce weights, and thus make the network more robust to losing any\rindividual connection in the network.",
            "Of course, the true measure of dropout is that it has been very\rsuccessful in improving the performance of neural networks.  The\roriginal\rpaper**Improving\r    neural networks by preventing co-adaptation of feature detectorsby Geoffrey Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\r  Sutskever, and Ruslan Salakhutdinov (2012).  Note that the paper\r  discusses a number of subtleties that I have glossed over in this\r  brief introduction.introducing the technique applied it to many\rdifferent tasks. For us, it's of particular interest that they applied\rdropout to MNIST digit classification, using a vanilla feedforward\rneural network along lines similar to those we've been considering.\rThe paper noted that the best result anyone had achieved up to that\rpoint using such an architecture was $98.4$ percent classification\raccuracy on the test set.  They improved that to $98.7$ percent\raccuracy using a combination of dropout and a modified form of L2\rregularization.  Similarly impressive results have been obtained for\rmany other tasks, including problems in image and speech recognition,\rand natural language processing.  Dropout has been especially useful\rin training large, deep networks, where the problem of overfitting is\roften acute.",
            "Artificially expanding the training data:We saw earlier that\rour MNIST classification accuracy dropped down to percentages in the\rmid-80s when we used only 1,000 training images.  It's not surprising\rthat this is the case, since less training data means our network will\rbe exposed to fewer variations in the way human beings write digits.\rLet's try training our 30 hidden neuron network with a variety of\rdifferent training data set sizes, to see how performance varies.  We\rtrain using a mini-batch size of 10, a learning rate $\\eta = 0.5$, a\rregularization parameter $\\lambda = 5.0$, and the cross-entropy cost\rfunction.  We will train for 30 epochs when the full training data set\ris used, and scale up the number of epochs proportionally when smaller\rtraining sets are used.  To ensure the weight decay factor remains the\rsame across training sets, we will use a regularization parameter of\r$\\lambda = 5.0$ when the full training data set is used, and scale\rdown $\\lambda$ proportionally when smaller training sets are\rused**This and the next two graph are produced with the\r  programmore_data.py..",
            "",
            "As you can see, the classification accuracies improve considerably as\rwe use more training data.  Presumably this improvement would continue\rstill further if more data was available.  Of course, looking at the\rgraph above it does appear that we're getting near saturation.\rSuppose, however, that we redo the graph with the training set size\rplotted logarithmically:",
            "",
            "It seems clear that the graph is still going up toward the end.  This\rsuggests that if we used vastly more training data - say, millions\ror even billions of handwriting samples, instead of just 50,000 -\rthen we'd likely get considerably better performance, even from this\rvery small network.",
            "Obtaining more training data is a great idea. Unfortunately, it can be\rexpensive, and so is not always possible in practice.  However,\rthere's another idea which can work nearly as well, and that's to\rartificially expand the training data.  Suppose, for example, that we\rtake an MNIST training image of a five,",
            "",
            "",
            "and rotate it by a small amount, let's say 15 degrees:",
            "",
            "It's still recognizably the same digit.  And yet at the pixel level\rit's quite different to any image currently in the MNIST training\rdata.  It's conceivable that adding this image to the training data\rmight help our network learn more about how to classify digits.\rWhat's more, obviously we're not limited to adding just this one\rimage.  We can expand our training data by makingmanysmall\rrotations ofallthe MNIST training images, and then using the\rexpanded training data to improve our network's performance.",
            "This idea is very powerful and has been widely used.  Let's look at\rsome of the results from a\rpaper**Best\r    Practices for Convolutional Neural Networks Applied to Visual\r    Document Analysis, by Patrice Simard, Dave Steinkraus, and John\r  Platt (2003).which applied several variations of the idea to\rMNIST.  One of the neural network architectures they considered was\ralong similar lines to what we've been using, a feedforward network\rwith 800 hidden neurons and using the cross-entropy cost function.\rRunning the network with the standard MNIST training data they\rachieved a classification accuracy of 98.4 percent on their test set.\rBut then they expanded the training data, using not just rotations, as\rI described above, but also translating and skewing the images.  By\rtraining on the expanded data set they increased their network's\raccuracy to 98.9 percent.  They also experimented with what they\rcalled \"elastic distortions\", a special type of image distortion\rintended to emulate the random oscillations found in hand muscles.  By\rusing the elastic distortions to expand the data they achieved an even\rhigher accuracy, 99.3 percent.  Effectively, they were broadening the\rexperience of their network by exposing it to the sort of variations\rthat are found in real handwriting.",
            "Variations on this idea can be used to improve performance on many\rlearning tasks, not just handwriting recognition.  The general\rprinciple is to expand the training data by applying operations that\rreflect real-world variation.  It's not difficult to think of ways of\rdoing this.  Suppose, for example, that you're building a neural\rnetwork to do speech recognition.  We humans can recognize speech even\rin the presence of distortions such as background noise.  And so you\rcan expand your data by adding background noise.  We can also\rrecognize speech if it's sped up or slowed down. So that's another way\rwe can expand the training data.  These techniques are not always used\r- for instance, instead of expanding the training data by adding\rnoise, it may well be more efficient to clean up the input to the\rnetwork by first applying a noise reduction filter.  Still, it's worth\rkeeping the idea of expanding the training data in mind, and looking\rfor opportunities to apply it.",
            "ExerciseAs discussed above, one way of expanding the MNIST training data\r  is to use small rotations of training images.  What's a problem that\r  might occur if we allow arbitrarily large rotations of training\r  images?",
            "An aside on big data and what it means to compare\r  classification accuracies:Let's look again at how our neural\rnetwork's accuracy varies with training set size:",
            "",
            "Suppose that instead of using a neural network we use some other\rmachine learning technique to classify digits.  For instance, let's\rtry using the support vector machines (SVM) which we met briefly back\rinChapter 1.  As was the case in Chapter 1,\rdon't worry if you're not familiar with SVMs, we don't need to\runderstand their details.  Instead, we'll use the SVM supplied by thescikit-learn library.  Here's\rhow SVM performance varies as a function of training set size.  I've\rplotted the neural net results as well, to make comparison\reasy**This graph was produced with the programmore_data.py(as were the last few graphs).:",
            "",
            "Probably the first thing that strikes you about this graph is that our\rneural network outperforms the SVM for every training set size.\rThat's nice, although you shouldn't read too much into it, since I\rjust used the out-of-the-box settings from scikit-learn's SVM, while\rwe've done a fair bit of work improving our neural network.  A more\rsubtle but more interesting fact about the graph is that if we train\rour SVM using 50,000 images then it actually has better performance\r(94.48 percent accuracy) than our neural network does when trained\rusing 5,000 images (93.24 percent accuracy).  In other words, more\rtraining data can sometimes compensate for differences in the machine\rlearning algorithm used.",
            "Something even more interesting can occur.  Suppose we're trying to\rsolve a problem using two machine learning algorithms, algorithm A and\ralgorithm B.  It sometimes happens that algorithm A will outperform\ralgorithm B with one set of training data, while algorithm B will\routperform algorithm A with a different set of training data.  We\rdon't see that above - it would require the two graphs to cross -\rbut it does happen**Striking examples may be found inScaling to very\r    very large corpora for natural language disambiguation, by\r  Michele Banko and Eric Brill (2001)..  The correct response to the\rquestion \"Is algorithm A better than algorithm B?\" is really: \"What\rtraining data set are you using?\"",
            "All this is a caution to keep in mind, both when doing development,\rand when reading research papers.  Many papers focus on finding new\rtricks to wring out improved performance on standard benchmark data\rsets.  \"Our whiz-bang technique gave us an improvement of X percent\ron standard benchmark Y\" is a canonical form of research claim.  Such\rclaims are often genuinely interesting, but they must be understood as\rapplying only in the context of the specific training data set used.\rImagine an alternate history in which the people who originally\rcreated the benchmark data set had a larger research grant.  They\rmight have used the extra money to collect more training data.  It's\rentirely possible that the \"improvement\" due to the whiz-bang\rtechnique would disappear on a larger data set.  In other words, the\rpurported improvement might be just an accident of history.  The\rmessage to take away, especially in practical applications, is that\rwhat we want is both better algorithmsandbetter training\rdata.  It's fine to look for better algorithms, but make sure you're\rnot focusing on better algorithms to the exclusion of easy wins\rgetting more or better training data.",
            "Problem(Research problem)How do our machine learning algorithms\r  perform in the limit of very large data sets?  For any given\r  algorithm it's natural to attempt to define a notion of asymptotic\r  performance in the limit of truly big data. A quick-and-dirty\r  approach to this problem is to simply try fitting curves to graphs\r  like those shown above, and then to extrapolate the fitted curves\r  out to infinity.  An objection to this approach is that different\r  approaches to curve fitting will give different notions of asymptotic\r  performance.  Can you find a principled justification for fitting to\r  some particular class of curves?  If so, compare the asymptotic\r  performance of several different machine learning algorithms.",
            "Summing up:We've now completed our dive into overfitting and\rregularization.  Of course, we'll return again to the issue.  As I've\rmentioned several times, overfitting is a major problem in neural\rnetworks, especially as computers get more powerful, and we have the\rability to train larger networks.  As a result there's a pressing need\rto develop powerful regularization techniques to reduce overfitting,\rand this is an extremely active area of current work.",
            "Weight initialization"
        ],
        "Weight initialization": [
            "When we create our neural networks, we have to make choices for the\rinitial weights and biases.  Up to now, we've been choosing them\raccording to a prescription which I discussed only brieflyback in Chapter 1.  Just to\rremind you, that prescription was to choose both the weights and\rbiases using independent Gaussian random variables, normalized to have\rmean $0$ and standard deviation $1$.  While this approach has worked\rwell, it was quitead hoc, and it's worth revisiting to see if\rwe can find a better way of setting our initial weights and biases,\rand perhaps help our neural networks learn faster.",
            "It turns out that we can do quite a bit better than initializing with\rnormalized Gaussians.  To see why, suppose we're working with a\rnetwork with a large number - say $1,000$ - of input neurons.  And\rlet's suppose we've used normalized Gaussians to initialize the\rweights connecting to the first hidden layer.  For now I'm going to\rconcentrate specifically on the weights connecting the input neurons\rto the first neuron in the hidden layer, and ignore the rest of the\rnetwork:",
            "",
            "We'll suppose for simplicity that we're trying to train using a\rtraining input $x$ in which half the input neurons are on, i.e., set\rto $1$, and half the input neurons are off, i.e., set to $0$.  The\rargument which follows applies more generally, but you'll get the gist\rfrom this special case.  Let's consider the weighted sum $z = \\sum_j\rw_j x_j+b$ of inputs to our hidden neuron.  $500$ terms in this sum\rvanish, because the corresponding input $x_j$ is zero.  And so $z$ is\ra sum over a total of $501$ normalized Gaussian random variables,\raccounting for the $500$ weight terms and the $1$ extra bias term.\rThus $z$ is itself distributed as a Gaussian with mean zero and\rstandard deviation $\\sqrt{501} \\approx 22.4$.  That is, $z$ has a very\rbroad Gaussian distribution, not sharply peaked at all:",
            "",
            "In particular, we can see from this graph that it's quite likely that\r$|z|$ will be pretty large, i.e., either $z \\gg 1$ or $z \\ll -1$.  If\rthat's the case then the output $\\sigma(z)$ from the hidden neuron\rwill be very close to either $1$ or $0$.  That means our hidden neuron\rwill have saturated.  And when that happens, as we know, making small\rchanges in the weights will make only absolutely miniscule changes in\rthe activation of our hidden neuron.  That miniscule change in the\ractivation of the hidden neuron will, in turn, barely affect the rest\rof the neurons in the network at all, and we'll see a correspondingly\rminiscule change in the cost function.  As a result, those weights\rwill only learn very slowly when we use the gradient descent\ralgorithm**We discussed this in more detail in Chapter 2,\r  where we used theequations\r    of backpropagationto show that weights input to saturated\r  neurons learned slowly..  It's similar to the problem we discussed\rearlier in this chapter, in which output neurons which saturated on\rthe wrong value caused learning to slow down.  We addressed that\rearlier problem with a clever choice of cost function.  Unfortunately,\rwhile that helped with saturated output neurons, it does nothing at\rall for the problem with saturated hidden neurons.",
            "I've been talking about the weights input to the first hidden layer.\rOf course, similar arguments apply also to later hidden layers: if the\rweights in later hidden layers are initialized using normalized\rGaussians, then activations will often be very close to $0$ or $1$,\rand learning will proceed very slowly.",
            "Is there some way we can choose better initializations for the weights\rand biases, so that we don't get this kind of saturation, and so avoid\ra learning slowdown?  Suppose we have a neuron with $n_{\\rm in}$ input\rweights.  Then we shall initialize those weights as Gaussian random\rvariables with mean $0$ and standard deviation $1/\\sqrt{n_{\\rm in}}$.\rThat is, we'll squash the Gaussians down, making it less likely that\rour neuron will saturate.  We'll continue to choose the bias as a\rGaussian with mean $0$ and standard deviation $1$, for reasons I'll\rreturn to in a moment.  With these choices, the weighted sum $z =\r\\sum_j w_j x_j + b$ will again be a Gaussian random variable with mean\r$0$, but it'll be much more sharply peaked than it was before.\rSuppose, as we did earlier, that $500$ of the inputs are zero and\r$500$ are $1$.  Then it's easy to show (see the exercise below) that\r$z$ has a Gaussian distribution with mean $0$ and standard deviation\r$\\sqrt{3/2} = 1.22\\ldots$.  This is much more sharply peaked than\rbefore, so much so that even the graph below understates the\rsituation, since I've had to rescale the vertical axis, when compared\rto the earlier graph:",
            "",
            "Such a neuron is much less likely to saturate, and correspondingly\rmuch less likely to have problems with a learning slowdown.",
            "ExerciseVerify that the standard deviation of $z = \\sum_j w_j x_j + b$\r  in the paragraph above is $\\sqrt{3/2}$.  It may help to know that:\r  (a) the variance of a sum of independent random variables is the sum\r  of the variances of the individual random variables; and (b) the\r  variance is the square of the standard deviation.",
            "I stated above that we'll continue to initialize the biases as before,\ras Gaussian random variables with a mean of $0$ and a standard\rdeviation of $1$.  This is okay, because it doesn't make it too much\rmore likely that our neurons will saturate.  In fact, it doesn't much\rmatter how we initialize the biases, provided we avoid the problem\rwith saturation.  Some people go so far as to initialize all the\rbiases to $0$, and rely on gradient descent to learn appropriate\rbiases.  But since it's unlikely to make much difference, we'll\rcontinue with the same initialization procedure as before.",
            "Let's compare the results for both our old and new approaches to\rweight initialization, using the MNIST digit classification task.  As\rbefore, we'll use $30$ hidden neurons, a mini-batch size of $10$, a\rregularization parameter $\\lambda = 5.0$, and the cross-entropy cost\rfunction.  We will decrease the learning rate slightly from $\\eta =\r0.5$ to $0.1$, since that makes the results a little more easily\rvisible in the graphs.  We can train using the old method of weight\rinitialization:>>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.large_weight_initializer()>>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True)We can also train using the new approach to weight initialization.\rThis is actually even easier, sincenetwork2's default way of\rinitializing the weights is using this new approach.  That means we\rcan omit thenet.large_weight_initializer()call above:>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True)Plotting the results**The program used to generate this and\r  the next graph isweight_initialization.py.,\rwe obtain:",
            "",
            "In both cases, we end up with a classification accuracy somewhat over\r96 percent.  The final classification accuracy is almost exactly the\rsame in the two cases.  But the new initialization technique brings us\rthere much, much faster.  At the end of the first epoch of training\rthe old approach to weight initialization has a classification\raccuracy under 87 percent, while the new approach is already almost 93\rpercent.  What appears to be going on is that our new approach to\rweight initialization starts us off in a much better regime, which\rlets us get good results much more quickly.  The same phenomenon is\ralso seen if we plot results with $100$ hidden neurons:",
            "",
            "In this case, the two curves don't quite meet.  However, my\rexperiments suggest that with just a few more epochs of training (not\rshown) the accuracies become almost exactly the same.  So on the basis\rof these experiments it looks as though the improved weight\rinitialization only speeds up learning, it doesn't change the final\rperformance of our networks.  However, in Chapter 4 we'll see examples\rof neural networks where the long-run behaviour is significantly\rbetter with the $1/\\sqrt{n_{\\rm in}}$ weight initialization.  Thus\rit's not only the speed of learning which is improved, it's sometimes\ralso the final performance.",
            "The $1/\\sqrt{n_{\\rm in}}$ approach to weight initialization helps\rimprove the way our neural nets learn.  Other techniques for weight\rinitialization have also been proposed, many building on this basic\ridea.  I won't review the other approaches here, since $1/\\sqrt{n_{\\rm\r    in}}$ works well enough for our purposes.  If you're interested in\rlooking further, I recommend looking at the discussion on pages 14 and\r15 of a 2012 paper by Yoshua\rBengio**Practical\r    Recommendations for Gradient-Based Training of Deep\r    Architectures, by Yoshua Bengio (2012)., as well as the\rreferences therein.",
            "ProblemConnecting regularization and the improved method of weight\r  initializationL2 regularization sometimes automatically gives us\r  something similar to the new approach to weight initialization.\r  Suppose we are using the old approach to weight initialization.\r  Sketch a heuristic argument that: (1) supposing $\\lambda$ is not too\r  small, the first epochs of training will be dominated almost\r  entirely by weight decay; (2) provided $\\eta \\lambda \\ll n$ the\r  weights will decay by a factor of $\\exp(-\\eta \\lambda / m)$ per\r  epoch; and (3) supposing $\\lambda$ is not too large, the weight\r  decay will tail off when the weights are down to a size around\r  $1/\\sqrt{n}$, where $n$ is the total number of weights in the\r  network.  Argue that these conditions are all satisfied in the\r  examples graphed in this section.",
            "",
            "",
            "",
            "",
            "",
            "Handwriting recognition revisited: the code"
        ],
        "Handwriting recognition revisited: the code": [
            "Let's implement the ideas we've discussed in this chapter.  We'll\rdevelop a new program,network2.py,\rwhich is an improved version of the programnetwork.pywe developed inChapter\r  1.  If you haven't looked atnetwork.pyin a while then you\rmay find it helpful to spend a few minutes quickly reading over the\rearlier discussion.  It's only 74 lines of code, and is easily\runderstood.",
            "As was the case innetwork.py, the star ofnetwork2.pyis theNetworkclass, which we use to represent our neural\rnetworks.  We initialize an instance ofNetworkwith a list ofsizesfor the respective layers in the network, and a choice\rfor thecostto use, defaulting to the cross-entropy:",
            "classNetwork(object):def__init__(self,sizes,cost=CrossEntropyCost):self.num_layers=len(sizes)self.sizes=sizesself.default_weight_initializer()self.cost=cost",
            "The first couple of lines of the__init__method are the same\ras innetwork.py, and are pretty self-explanatory.  But the\rnext two lines are new, and we need to understand what they're doing\rin detail.",
            "Let's start by examining thedefault_weight_initializermethod. This makes use of ournew and\r  improved approachto weight initialization.  As we've seen, in that\rapproach the weights input to a neuron are initialized as Gaussian\rrandom variables with mean 0 and standard deviation $1$ divided by the\rsquare root of the number of connections input to the neuron.  Also in\rthis method we'll initialize the biases, using Gaussian random\rvariables with mean $0$ and standard deviation $1$.  Here's the code:",
            "defdefault_weight_initializer(self):self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)/np.sqrt(x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]",
            "To understand the code, it may help to recall thatnpis the\rNumpy library for doing linear algebra.  We'llimportNumpy at\rthe beginning of our program.  Also, notice that we don't initialize\rany biases for the first layer of neurons.  We avoid doing this\rbecause the first layer is an input layer, and so any biases would not\rbe used.  We did exactly the same thing innetwork.py.",
            "Complementing thedefault_weight_initializerwe'll also include\ralarge_weight_initializermethod.  This method initializes the\rweights and biases using the old approach from Chapter 1, with both\rweights and biases initialized as Gaussian random variables with mean\r$0$ and standard deviation $1$.  The code is, of course, only a tiny\rbit different from thedefault_weight_initializer:",
            "deflarge_weight_initializer(self):self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]",
            "I've included thelarge_weight_initializermethod mostly as a\rconvenience to make it easier to compare the results in this chapter\rto those in Chapter 1. I can't think of many practical situations\rwhere I would recommend using it!",
            "The second new thing inNetwork's__init__method is\rthat we now initialize acostattribute.  To understand how\rthat works, let's look at the class we use to represent the\rcross-entropy cost**If you're not familiar with Python's\r  static methods you can ignore the@staticmethoddecorators,\r  and just treatfnanddeltaas ordinary methods.  If\r  you're curious about details, all@staticmethoddoes is tell\r  the Python interpreter that the method which follows doesn't depend\r  on the object in any way.  That's whyselfisn't passed as a\r  parameter to thefnanddeltamethods.:",
            "classCrossEntropyCost(object):@staticmethoddeffn(a,y):returnnp.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))@staticmethoddefdelta(z,a,y):return(a-y)",
            "Let's break this down.  The first thing to observe is that even though\rthe cross-entropy is, mathematically speaking, a function, we've\rimplemented it as a Python class, not a Python function.  Why have I\rmade that choice?  The reason is that the cost plays two different\rroles in our network.  The obvious role is that it's a measure of how\rwell an output activation,a, matches the desired output,y.  This role is captured by theCrossEntropyCost.fnmethod.  (Note, by the way, that thenp.nan_to_numcall insideCrossEntropyCost.fnensures that Numpy deals correctly with the\rlog of numbers very close to zero.)  But there's also a second way the\rcost function enters our network.  Recall fromChapter\r  2that when running the backpropagation algorithm we need to\rcompute the network's output error, $\\delta^L$. The form of the output\rerror depends on the choice of cost function: different cost function,\rdifferent form for the output error.  For the cross-entropy the output\rerror is, as we saw in Equation(66)\\begin{eqnarray} \r    \\delta^L = a^L-y.\r   \\nonumber\\end{eqnarray},",
            "\\begin{eqnarray}\r  \\delta^L = a^L-y.\r\\tag{99}\\end{eqnarray}\rFor this reason we define a second method,CrossEntropyCost.delta, whose purpose is to tell our network\rhow to compute the output error.  And then we bundle these two methods\rup into a single class containing everything our networks need to know\rabout the cost function.",
            "In a similar way,network2.pyalso contains a class to\rrepresent the quadratic cost function.  This is included for\rcomparison with the results of Chapter 1, since going forward we'll\rmostly use the cross entropy.  The code is just below.  TheQuadraticCost.fnmethod is a straightforward computation of the\rquadratic cost associated to the actual output,a, and the\rdesired output,y.  The value returned byQuadraticCost.deltais based on the\rexpression(30)\\begin{eqnarray} \r  \\delta^L = (a^L-y) \\odot \\sigma'(z^L) \\nonumber\\end{eqnarray}for the output error for the\rquadratic cost, which we derived back in Chapter 2.",
            "classQuadraticCost(object):@staticmethoddeffn(a,y):return0.5*np.linalg.norm(a-y)**2@staticmethoddefdelta(z,a,y):return(a-y)*sigmoid_prime(z)",
            "We've now understood the main differences betweennetwork2.pyandnetwork.py.  It's all pretty simple stuff.  There are a\rnumber of smaller changes, which I'll discuss below, including the\rimplementation of L2 regularization.  Before getting to that, let's\rlook at the complete code fornetwork2.py.  You don't need to\rread all the code in detail, but it is worth understanding the broad\rstructure, and in particular reading the documentation strings, so you\runderstand what each piece of the program is doing.  Of course, you're\ralso welcome to delve as deeply as you wish!  If you get lost, you may\rwish to continue reading the prose below, and return to the code\rlater.  Anyway, here's the code:",
            "\"\"\"network2.py~~~~~~~~~~~~~~An improved version of network.py, implementing the stochasticgradient descent learning algorithm for a feedforward neural network.Improvements include the addition of the cross-entropy cost function,regularization, and better initialization of network weights.  Notethat I have focused on making the code simple, easily readable, andeasily modifiable.  It is not optimized, and omits many desirablefeatures.\"\"\"#### Libraries# Standard libraryimportjsonimportrandomimportsys# Third-party librariesimportnumpyasnp#### Define the quadratic and cross-entropy cost functionsclassQuadraticCost(object):@staticmethoddeffn(a,y):\"\"\"Return the cost associated with an output ``a`` and desired output``y``.\"\"\"return0.5*np.linalg.norm(a-y)**2@staticmethoddefdelta(z,a,y):\"\"\"Return the error delta from the output layer.\"\"\"return(a-y)*sigmoid_prime(z)classCrossEntropyCost(object):@staticmethoddeffn(a,y):\"\"\"Return the cost associated with an output ``a`` and desired output``y``.  Note that np.nan_to_num is used to ensure numericalstability.  In particular, if both ``a`` and ``y`` have a 1.0in the same slot, then the expression (1-y)*np.log(1-a)returns nan.  The np.nan_to_num ensures that that is convertedto the correct value (0.0).\"\"\"returnnp.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))@staticmethoddefdelta(z,a,y):\"\"\"Return the error delta from the output layer.  Note that theparameter ``z`` is not used by the method.  It is included inthe method's parameters in order to make the interfaceconsistent with the delta method for other cost classes.\"\"\"return(a-y)#### Main Network classclassNetwork(object):def__init__(self,sizes,cost=CrossEntropyCost):\"\"\"The list ``sizes`` contains the number of neurons in the respectivelayers of the network.  For example, if the list was [2, 3, 1]then it would be a three-layer network, with the first layercontaining 2 neurons, the second layer 3 neurons, and thethird layer 1 neuron.  The biases and weights for the networkare initialized randomly, using``self.default_weight_initializer`` (see docstring for thatmethod).\"\"\"self.num_layers=len(sizes)self.sizes=sizesself.default_weight_initializer()self.cost=costdefdefault_weight_initializer(self):\"\"\"Initialize each weight using a Gaussian distribution with mean 0and standard deviation 1 over the square root of the number ofweights connecting to the same neuron.  Initialize the biasesusing a Gaussian distribution with mean 0 and standarddeviation 1.Note that the first layer is assumed to be an input layer, andby convention we won't set any biases for those neurons, sincebiases are only ever used in computing the outputs from laterlayers.\"\"\"self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)/np.sqrt(x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]deflarge_weight_initializer(self):\"\"\"Initialize the weights using a Gaussian distribution with mean 0and standard deviation 1.  Initialize the biases using aGaussian distribution with mean 0 and standard deviation 1.Note that the first layer is assumed to be an input layer, andby convention we won't set any biases for those neurons, sincebiases are only ever used in computing the outputs from laterlayers.This weight and bias initializer uses the same approach as inChapter 1, and is included for purposes of comparison.  Itwill usually be better to use the default weight initializerinstead.\"\"\"self.biases=[np.random.randn(y,1)foryinself.sizes[1:]]self.weights=[np.random.randn(y,x)forx,yinzip(self.sizes[:-1],self.sizes[1:])]deffeedforward(self,a):\"\"\"Return the output of the network if ``a`` is input.\"\"\"forb,winzip(self.biases,self.weights):a=sigmoid(np.dot(w,a)+b)returnadefSGD(self,training_data,epochs,mini_batch_size,eta,lmbda=0.0,evaluation_data=None,monitor_evaluation_cost=False,monitor_evaluation_accuracy=False,monitor_training_cost=False,monitor_training_accuracy=False):\"\"\"Train the neural network using mini-batch stochastic gradientdescent.  The ``training_data`` is a list of tuples ``(x, y)``representing the training inputs and the desired outputs.  Theother non-optional parameters are self-explanatory, as is theregularization parameter ``lmbda``.  The method also accepts``evaluation_data``, usually either the validation or testdata.  We can monitor the cost and accuracy on either theevaluation data or the training data, by setting theappropriate flags.  The method returns a tuple containing fourlists: the (per-epoch) costs on the evaluation data, theaccuracies on the evaluation data, the costs on the trainingdata, and the accuracies on the training data.  All values areevaluated at the end of each training epoch.  So, for example,if we train for 30 epochs, then the first element of the tuplewill be a 30-element list containing the cost on theevaluation data at the end of each epoch. Note that the listsare empty if the corresponding flag is not set.\"\"\"ifevaluation_data:n_data=len(evaluation_data)n=len(training_data)evaluation_cost,evaluation_accuracy=[],[]training_cost,training_accuracy=[],[]forjinxrange(epochs):random.shuffle(training_data)mini_batches=[training_data[k:k+mini_batch_size]forkinxrange(0,n,mini_batch_size)]formini_batchinmini_batches:self.update_mini_batch(mini_batch,eta,lmbda,len(training_data))print\"Epoch%straining complete\"%jifmonitor_training_cost:cost=self.total_cost(training_data,lmbda)training_cost.append(cost)print\"Cost on training data: {}\".format(cost)ifmonitor_training_accuracy:accuracy=self.accuracy(training_data,convert=True)training_accuracy.append(accuracy)print\"Accuracy on training data: {} / {}\".format(accuracy,n)ifmonitor_evaluation_cost:cost=self.total_cost(evaluation_data,lmbda,convert=True)evaluation_cost.append(cost)print\"Cost on evaluation data: {}\".format(cost)ifmonitor_evaluation_accuracy:accuracy=self.accuracy(evaluation_data)evaluation_accuracy.append(accuracy)print\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data),n_data)printreturnevaluation_cost,evaluation_accuracy,\\training_cost,training_accuracydefupdate_mini_batch(self,mini_batch,eta,lmbda,n):\"\"\"Update the network's weights and biases by applying gradientdescent using backpropagation to a single mini batch.  The``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is thelearning rate, ``lmbda`` is the regularization parameter, and``n`` is the total size of the training data set.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]forx,yinmini_batch:delta_nabla_b,delta_nabla_w=self.backprop(x,y)nabla_b=[nb+dnbfornb,dnbinzip(nabla_b,delta_nabla_b)]nabla_w=[nw+dnwfornw,dnwinzip(nabla_w,delta_nabla_w)]self.weights=[(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nwforw,nwinzip(self.weights,nabla_w)]self.biases=[b-(eta/len(mini_batch))*nbforb,nbinzip(self.biases,nabla_b)]defbackprop(self,x,y):\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing thegradient for the cost function C_x.  ``nabla_b`` and``nabla_w`` are layer-by-layer lists of numpy arrays, similarto ``self.biases`` and ``self.weights``.\"\"\"nabla_b=[np.zeros(b.shape)forbinself.biases]nabla_w=[np.zeros(w.shape)forwinself.weights]# feedforwardactivation=xactivations=[x]# list to store all the activations, layer by layerzs=[]# list to store all the z vectors, layer by layerforb,winzip(self.biases,self.weights):z=np.dot(w,activation)+bzs.append(z)activation=sigmoid(z)activations.append(activation)# backward passdelta=(self.cost).delta(zs[-1],activations[-1],y)nabla_b[-1]=deltanabla_w[-1]=np.dot(delta,activations[-2].transpose())# Note that the variable l in the loop below is used a little# differently to the notation in Chapter 2 of the book.  Here,# l = 1 means the last layer of neurons, l = 2 is the# second-last layer, and so on.  It's a renumbering of the# scheme in the book, used here to take advantage of the fact# that Python can use negative indices in lists.forlinxrange(2,self.num_layers):z=zs[-l]sp=sigmoid_prime(z)delta=np.dot(self.weights[-l+1].transpose(),delta)*spnabla_b[-l]=deltanabla_w[-l]=np.dot(delta,activations[-l-1].transpose())return(nabla_b,nabla_w)defaccuracy(self,data,convert=False):\"\"\"Return the number of inputs in ``data`` for which the neuralnetwork outputs the correct result. The neural network'soutput is assumed to be the index of whichever neuron in thefinal layer has the highest activation.The flag ``convert`` should be set to False if the data set isvalidation or test data (the usual case), and to True if thedata set is the training data. The need for this flag arisesdue to differences in the way the results ``y`` arerepresented in the different data sets.  In particular, itflags whether we need to convert between the differentrepresentations.  It may seem strange to use differentrepresentations for the different data sets.  Why not use thesame representation for all three data sets?  It's done forefficiency reasons -- the program usually evaluates the coston the training data and the accuracy on other data sets.These are different types of computations, and using differentrepresentations speeds things up.  More details on therepresentations can be found inmnist_loader.load_data_wrapper.\"\"\"ifconvert:results=[(np.argmax(self.feedforward(x)),np.argmax(y))for(x,y)indata]else:results=[(np.argmax(self.feedforward(x)),y)for(x,y)indata]returnsum(int(x==y)for(x,y)inresults)deftotal_cost(self,data,lmbda,convert=False):\"\"\"Return the total cost for the data set ``data``.  The flag``convert`` should be set to False if the data set is thetraining data (the usual case), and to True if the data set isthe validation or test data.  See comments on the similar (butreversed) convention for the ``accuracy`` method, above.\"\"\"cost=0.0forx,yindata:a=self.feedforward(x)ifconvert:y=vectorized_result(y)cost+=self.cost.fn(a,y)/len(data)cost+=0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2forwinself.weights)returncostdefsave(self,filename):\"\"\"Save the neural network to the file ``filename``.\"\"\"data={\"sizes\":self.sizes,\"weights\":[w.tolist()forwinself.weights],\"biases\":[b.tolist()forbinself.biases],\"cost\":str(self.cost.__name__)}f=open(filename,\"w\")json.dump(data,f)f.close()#### Loading a Networkdefload(filename):\"\"\"Load a neural network from the file ``filename``.  Returns aninstance of Network.\"\"\"f=open(filename,\"r\")data=json.load(f)f.close()cost=getattr(sys.modules[__name__],data[\"cost\"])net=Network(data[\"sizes\"],cost=cost)net.weights=[np.array(w)forwindata[\"weights\"]]net.biases=[np.array(b)forbindata[\"biases\"]]returnnet#### Miscellaneous functionsdefvectorized_result(j):\"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th positionand zeroes elsewhere.  This is used to convert a digit (0...9)into a corresponding desired output from the neural network.\"\"\"e=np.zeros((10,1))e[j]=1.0returnedefsigmoid(z):\"\"\"The sigmoid function.\"\"\"return1.0/(1.0+np.exp(-z))defsigmoid_prime(z):\"\"\"Derivative of the sigmoid function.\"\"\"returnsigmoid(z)*(1-sigmoid(z))",
            "One of the more interesting changes in the code is to include L2\rregularization.  Although this is a major conceptual change, it's so\rtrivial to implement that it's easy to miss in the code.  For the most\rpart it just involves passing the parameterlmbdato various\rmethods, notably theNetwork.SGDmethod.  The real work is done\rin a single line of the program, the fourth-last line of theNetwork.update_mini_batchmethod.  That's where we modify the\rgradient descent update rule to include weight decay.  But although\rthe modification is tiny, it has a big impact on results!",
            "This is, by the way, common when implementing new techniques in neural\rnetworks.  We've spent thousands of words discussing regularization.\rIt's conceptually quite subtle and difficult to understand.  And yet\rit was trivial to add to our program!  It occurs surprisingly often\rthat sophisticated techniques can be implemented with small changes to\rcode.",
            "Another small but important change to our code is the addition of\rseveral optional flags to the stochastic gradient descent method,Network.SGD.  These flags make it possible to monitor the cost\rand accuracy either on thetraining_dataor on a set ofevaluation_datawhich can be passed toNetwork.SGD.\rWe've used these flags often earlier in the chapter, but let me give\ran example of how it works, just to remind you:",
            ">>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10],cost=network2.CrossEntropyCost)>>>net.SGD(training_data,30,10,0.5,...lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True,...monitor_evaluation_cost=True,...monitor_training_accuracy=True,...monitor_training_cost=True)",
            "Here, we're setting theevaluation_datato be thevalidation_data.  But we could also have monitored performance\ron thetest_dataor any other data set.  We also have four\rflags telling us to monitor the cost and accuracy on both theevaluation_dataand thetraining_data.  Those flags areFalseby default, but they've been turned on here in order to\rmonitor ourNetwork's performance.  Furthermore,network2.py'sNetwork.SGDmethod returns a four-element\rtuple representing the results of the monitoring.  We can use this as\rfollows:",
            ">>>evaluation_cost,evaluation_accuracy,...training_cost,training_accuracy=net.SGD(training_data,30,10,0.5,...lmbda=5.0,...evaluation_data=validation_data,...monitor_evaluation_accuracy=True,...monitor_evaluation_cost=True,...monitor_training_accuracy=True,...monitor_training_cost=True)",
            "So, for example,evaluation_costwill be a 30-element list\rcontaining the cost on the evaluation data at the end of each epoch.\rThis sort of information is extremely useful in understanding a\rnetwork's behaviour.  It can, for example, be used to draw graphs\rshowing how the network learns over time.  Indeed, that's exactly how\rI constructed all the graphs earlier in the chapter.  Note, however,\rthat if any of the monitoring flags are not set, then the\rcorresponding element in the tuple will be the empty list.",
            "Other additions to the code include aNetwork.savemethod, to\rsaveNetworkobjects to disk, and a function toloadthem back in again later.  Note that the saving and loading is done\rusing JSON, not Python'spickleorcPicklemodules,\rwhich are the usual way we save and load objects to and from disk in\rPython.  Using JSON requires more code thanpickleorcPicklewould.  To understand why I've used JSON, imagine that\rat some time in the future we decided to change ourNetworkclass to allow neurons other than sigmoid neurons.  To implement that\rchange we'd most likely change the attributes defined in theNetwork.__init__method.  If we've simply pickled the objects\rthat would cause ourloadfunction to fail.  Using JSON to do\rthe serialization explicitly makes it easy to ensure that oldNetworks will stillload.",
            "There are many other minor changes in the code fornetwork2.py,\rbut they're all simple variations onnetwork.py.  The net\rresult is to expand our 74-line program to a far more capable 152\rlines.",
            "ProblemsModify the code above to implement L1 regularization, and use L1\r  regularization to classify MNIST digits using a $30$ hidden neuron\r  network.  Can you find a regularization parameter that enables you\r  to do better than running unregularized?",
            "Take a look at theNetwork.cost_derivativemethod innetwork.py.\r  That method was written for the quadratic cost.  How would you\r  rewrite the method for the cross-entropy cost?  Can you think of a\r  problem that might arise in the cross-entropy version?  Innetwork2.pywe've eliminated theNetwork.cost_derivativemethod entirely, instead\r  incorporating its functionality into theCrossEntropyCost.deltamethod.  How does this solve the\r  problem you've just identified?",
            "",
            "How to choose a neural network's hyper-parameters?"
        ],
        "How to choose a neural network's hyper-parameters?": [
            "Up until now I haven't explained how I've been choosing values for\rhyper-parameters such as the learning rate, $\\eta$, the regularization\rparameter, $\\lambda$, and so on.  I've just been supplying values\rwhich work pretty well.  In practice, when you're using neural nets to\rattack a problem, it can be difficult to find good hyper-parameters.\rImagine, for example, that we've just been introduced to the MNIST\rproblem, and have begun working on it, knowing nothing at all about\rwhat hyper-parameters to use.  Let's suppose that by good fortune in\rour first experiments we choose many of the hyper-parameters in the\rsame way as was done earlier this chapter: 30 hidden neurons, a\rmini-batch size of 10, training for 30 epochs using the cross-entropy.\rBut we choose a learning rate $\\eta = 10.0$ and regularization\rparameter $\\lambda = 1000.0$.  Here's what I saw on one such run:",
            ">>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()>>>importnetwork2>>>net=network2.Network([784,30,10])>>>net.SGD(training_data,30,10,10.0,lmbda=1000.0,...evaluation_data=validation_data,monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:1030/10000Epoch1trainingcompleteAccuracyonevaluationdata:990/10000Epoch2trainingcompleteAccuracyonevaluationdata:1009/10000...Epoch27trainingcompleteAccuracyonevaluationdata:1009/10000Epoch28trainingcompleteAccuracyonevaluationdata:983/10000Epoch29trainingcompleteAccuracyonevaluationdata:967/10000",
            "Our classification accuracies are no better than chance!  Our network\ris acting as a random noise generator!",
            "\"Well, that's easy to fix,\" you might say, \"just decrease the\rlearning rate and regularization hyper-parameters\".  Unfortunately,\ryou don'ta prioriknow those are the hyper-parameters you need\rto adjust.  Maybe the real problem is that our 30 hidden neuron\rnetwork will never work well, no matter how the other hyper-parameters\rare chosen?  Maybe we really need at least 100 hidden neurons?  Or 300\rhidden neurons?  Or multiple hidden layers?  Or a different approach\rto encoding the output?  Maybe our network is learning, but we need to\rtrain for more epochs?  Maybe the mini-batches are too small?  Maybe\rwe'd do better switching back to the quadratic cost function?  Maybe\rwe need to try a different approach to weight initialization?  And so\ron, on and on and on.  It's easy to feel lost in hyper-parameter\rspace.  This can be particularly frustrating if your network is very\rlarge, or uses a lot of training data, since you may train for hours\ror days or weeks, only to get no result.  If the situation persists,\rit damages your confidence.  Maybe neural networks are the wrong\rapproach to your problem?  Maybe you should quit your job and take up\rbeekeeping?",
            "In this section I explain some heuristics which can be used to set the\rhyper-parameters in a neural network.  The goal is to help you develop\ra workflow that enables you to do a pretty good job setting\rhyper-parameters.  Of course, I won't cover everything about\rhyper-parameter optimization.  That's a huge subject, and it's not, in\rany case, a problem that is ever completely solved, nor is there\runiversal agreement amongst practitioners on the right strategies to\ruse.  There's always one more trick you can try to eke out a bit more\rperformance from your network.  But the heuristics in this section\rshould get you started.",
            "Broad strategy:When using neural networks to attack a new\rproblem the first challenge is to getanynon-trivial learning,\ri.e., for the network to achieve results better than chance.  This can\rbe surprisingly difficult, especially when confronting a new class of\rproblem.  Let's look at some strategies you can use if you're having\rthis kind of trouble.",
            "Suppose, for example, that you're attacking MNIST for the first time.\rYou start out enthusiastic, but are a little discouraged when your\rfirst network fails completely, as in the example above.  The way to\rgo is to strip the problem down.  Get rid of all the training and\rvalidation images except images which are 0s or 1s.  Then try to train\ra network to distinguish 0s from 1s.  Not only is that an inherently\reasier problem than distinguishing all ten digits, it also reduces the\ramount of training data by 80 percent, speeding up training by a\rfactor of 5.  That enables much more rapid experimentation, and so\rgives you more rapid insight into how to build a good network.",
            "You can further speed up experimentation by stripping your network\rdown to the simplest network likely to do meaningful learning.  If you\rbelieve a[784, 10]network can likely do better-than-chance\rclassification of MNIST digits, then begin your experimentation with\rsuch a network.  It'll be much faster than training a[784, 30, 10]network, and you can build back up to the latter.",
            "You can get another speed up in experimentation by increasing the\rfrequency of monitoring.  Innetwork2.pywe monitor performance\rat the end of each training epoch.  With 50,000 images per epoch, that\rmeans waiting a little while - about ten seconds per epoch, on my\rlaptop, when training a[784, 30, 10]network - before\rgetting feedback on how well the network is learning.  Of course, ten\rseconds isn't very long, but if you want to trial dozens of\rhyper-parameter choices it's annoying, and if you want to trial\rhundreds or thousands of choices it starts to get debilitating.  We\rcan get feedback more quickly by monitoring the validation accuracy\rmore often, say, after every 1,000 training images.  Furthermore,\rinstead of using the full 10,000 image validation set to monitor\rperformance, we can get a much faster estimate using just 100\rvalidation images.  All that matters is that the network sees enough\rimages to do real learning, and to get a pretty good rough estimate of\rperformance. Of course, our programnetwork2.pydoesn't\rcurrently do this kind of monitoring.  But as a kludge to achieve a\rsimilar effect for the purposes of illustration, we'll strip down our\rtraining data to just the first 1,000 MNIST training images.  Let's\rtry it and see what happens.  (To keep the code below simple I haven't\rimplemented the idea of using only 0 and 1 images.  Of course, that\rcan be done with just a little more work.)",
            ">>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,10.0,lmbda=1000.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:10/100Epoch1trainingcompleteAccuracyonevaluationdata:10/100Epoch2trainingcompleteAccuracyonevaluationdata:10/100...",
            "We're still getting pure noise!  But there's a big win: we're now\rgetting feedback in a fraction of a second, rather than once every ten\rseconds or so.  That means you can more quickly experiment with other\rchoices of hyper-parameter, or even conduct experiments trialling many\rdifferent choices of hyper-parameter nearly simultaneously.",
            "In the above example I left $\\lambda$ as $\\lambda = 1000.0$, as we\rused earlier.  But since we changed the number of training examples we\rshould really change $\\lambda$ to keep the weight decay the same.\rThat means changing $\\lambda$ to $20.0$.  If we do that then this is\rwhat happens:",
            ">>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,10.0,lmbda=20.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:12/100Epoch1trainingcompleteAccuracyonevaluationdata:14/100Epoch2trainingcompleteAccuracyonevaluationdata:25/100Epoch3trainingcompleteAccuracyonevaluationdata:18/100...",
            "Ahah!  We have a signal.  Not a terribly good signal, but a signal\rnonetheless.  That's something we can build on, modifying the\rhyper-parameters to try to get further improvement.  Maybe we guess\rthat our learning rate needs to be higher.  (As you perhaps realize,\rthat's a silly guess, for reasons we'll discuss shortly, but please\rbear with me.)  So to test our guess we try dialing $\\eta$ up to\r$100.0$:",
            ">>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,100.0,lmbda=20.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:10/100Epoch1trainingcompleteAccuracyonevaluationdata:10/100Epoch2trainingcompleteAccuracyonevaluationdata:10/100Epoch3trainingcompleteAccuracyonevaluationdata:10/100...",
            "That's no good!  It suggests that our guess was wrong, and the problem\rwasn't that the learning rate was too low.  So instead we try dialing\r$\\eta$ down to $\\eta = 1.0$:",
            ">>>net=network2.Network([784,10])>>>net.SGD(training_data[:1000],30,10,1.0,lmbda=20.0,\\...evaluation_data=validation_data[:100],\\...monitor_evaluation_accuracy=True)Epoch0trainingcompleteAccuracyonevaluationdata:62/100Epoch1trainingcompleteAccuracyonevaluationdata:42/100Epoch2trainingcompleteAccuracyonevaluationdata:43/100Epoch3trainingcompleteAccuracyonevaluationdata:61/100...",
            "That's better!  And so we can continue, individually adjusting each\rhyper-parameter, gradually improving performance.  Once we've explored\rto find an improved value for $\\eta$, then we move on to find a good\rvalue for $\\lambda$.  Then experiment with a more complex\rarchitecture, say a network with 10 hidden neurons.  Then adjust the\rvalues for $\\eta$ and $\\lambda$ again.  Then increase to 20 hidden\rneurons.  And then adjust other hyper-parameters some more.  And so\ron, at each stage evaluating performance using our held-out validation\rdata, and using those evaluations to find better and better\rhyper-parameters.  As we do so, it typically takes longer to witness\rthe impact due to modifications of the hyper-parameters, and so we can\rgradually decrease the frequency of monitoring.",
            "This all looks very promising as a broad strategy.  However, I want to\rreturn to that initial stage of finding hyper-parameters that enable a\rnetwork to learn anything at all.  In fact, even the above discussion\rconveys too positive an outlook.  It can be immensely frustrating to\rwork with a network that's learning nothing.  You can tweak\rhyper-parameters for days, and still get no meaningful response.  And\rso I'd like to re-emphasize that during the early stages you should\rmake sure you can get quick feedback from experiments.  Intuitively,\rit may seem as though simplifying the problem and the architecture\rwill merely slow you down.  In fact, it speeds things up, since you\rmuch more quickly find a network with a meaningful signal.  Once\ryou've got such a signal, you can often get rapid improvements by\rtweaking the hyper-parameters.  As with many things in life, getting\rstarted can be the hardest thing to do.",
            "Okay, that's the broad strategy.  Let's now look at some specific\rrecommendations for setting hyper-parameters.  I will focus on the\rlearning rate, $\\eta$, the L2 regularization parameter, $\\lambda$, and\rthe mini-batch size.  However, many of the remarks apply also to other\rhyper-parameters, including those associated to network architecture,\rother forms of regularization, and some hyper-parameters we'll meet\rlater in the book, such as the momentum co-efficient.",
            "Learning rate:Suppose we run three MNIST networks with three\rdifferent learning rates, $\\eta = 0.025$, $\\eta = 0.25$ and $\\eta =\r2.5$, respectively.  We'll set the other hyper-parameters as for the\rexperiments in earlier sections, running over 30 epochs, with a\rmini-batch size of 10, and with $\\lambda = 5.0$.  We'll also return to\rusing the full $50,000$ training images.  Here's a graph showing the\rbehaviour of the training cost as we train**The graph was\r  generated bymultiple_eta.py.:",
            "",
            "With $\\eta = 0.025$ the cost decreases smoothly until the final epoch.\rWith $\\eta = 0.25$ the cost initially decreases, but after about $20$\repochs it is near saturation, and thereafter most of the changes are\rmerely small and apparently random oscillations.  Finally, with $\\eta\r= 2.5$ the cost makes large oscillations right from the start.  To\runderstand the reason for the oscillations, recall that stochastic\rgradient descent is supposed to step us gradually down into a valley\rof the cost function,",
            "",
            "However, if $\\eta$ is too large then the steps will be so large that\rthey may actually overshoot the minimum, causing the algorithm to\rclimb up out of the valley instead.  That's likely**This\r  picture is helpful, but it's intended as an intuition-building\r  illustration of what may go on, not as a complete, exhaustive\r  explanation.  Briefly, a more complete explanation is as follows:\r  gradient descent uses a first-order approximation to the cost\r  function as a guide to how to decrease the cost.  For large $\\eta$,\r  higher-order terms in the cost function become more important, and\r  may dominate the behaviour, causing gradient descent to break down.\r  This is especially likely as we approach minima and quasi-minima of\r  the cost function, since near such points the gradient becomes\r  small, making it easier for higher-order terms to dominate\r  behaviour.what's causing the cost to oscillate when $\\eta = 2.5$.\rWhen we choose $\\eta = 0.25$ the initial steps do take us toward a\rminimum of the cost function, and it's only once we get near that\rminimum that we start to suffer from the overshooting problem.  And\rwhen we choose $\\eta = 0.025$ we don't suffer from this problem at all\rduring the first $30$ epochs.  Of course, choosing $\\eta$ so small\rcreates another problem, namely, that it slows down stochastic\rgradient descent.  An even better approach would be to start with\r$\\eta = 0.25$, train for $20$ epochs, and then switch to $\\eta =\r0.025$.  We'll discuss such variable learning rate schedules later.\rFor now, though, let's stick to figuring out how to find a single good\rvalue for the learning rate, $\\eta$.",
            "",
            "With this picture in mind, we can set $\\eta$ as follows.  First, we\restimate the threshold value for $\\eta$ at which the cost on the\rtraining data immediately begins decreasing, instead of oscillating or\rincreasing.  This estimate doesn't need to be too accurate.  You can\restimate the order of magnitude by starting with $\\eta = 0.01$.  If\rthe cost decreases during the first few epochs, then you should\rsuccessively try $\\eta = 0.1, 1.0, \\ldots$ until you find a value for\r$\\eta$ where the cost oscillates or increases during the first few\repochs.  Alternately, if the cost oscillates or increases during the\rfirst few epochs when $\\eta = 0.01$, then try $\\eta = 0.001, 0.0001,\r\\ldots$ until you find a value for $\\eta$ where the cost decreases\rduring the first few epochs.  Following this procedure will give us an\rorder of magnitude estimate for the threshold value of $\\eta$.  You\rmay optionally refine your estimate, to pick out the largest value of\r$\\eta$ at which the cost decreases during the first few epochs, say\r$\\eta = 0.5$ or $\\eta = 0.2$ (there's no need for this to be\rsuper-accurate).  This gives us an estimate for the threshold value of\r$\\eta$.",
            "",
            "Obviously, the actual value of $\\eta$ that you use should be no larger\rthan the threshold value.  In fact, if the value of $\\eta$ is to\rremain usable over many epochs then you likely want to use a value\rfor $\\eta$ that is smaller, say, a factor of two below the threshold.\rSuch a choice will typically allow you to train for many epochs,\rwithout causing too much of a slowdown in learning.",
            "",
            "In the case of the MNIST data, following this strategy leads to an\restimate of $0.1$ for the order of magnitude of the threshold value of\r$\\eta$.  After some more refinement, we obtain a threshold value $\\eta\r= 0.5$.  Following the prescription above, this suggests using $\\eta =\r0.25$ as our value for the learning rate.  In fact, I found that using\r$\\eta = 0.5$ worked well enough over $30$ epochs that for the most\rpart I didn't worry about using a lower value of $\\eta$.",
            "This all seems quite straightforward.  However, using the training\rcost to pick $\\eta$ appears to contradict what I said earlier in this\rsection, namely, that we'd pick hyper-parameters by evaluating\rperformance using our held-out validation data.  In fact, we'll use\rvalidation accuracy to pick the regularization hyper-parameter, the\rmini-batch size, and network parameters such as the number of layers\rand hidden neurons, and so on.  Why do things differently for the\rlearning rate?  Frankly, this choice is my personal aesthetic\rpreference, and is perhaps somewhat idiosyncratic.  The reasoning is\rthat the other hyper-parameters are intended to improve the final\rclassification accuracy on the test set, and so it makes sense to\rselect them on the basis of validation accuracy.  However, the\rlearning rate is only incidentally meant to impact the final\rclassification accuracy.  Its primary purpose is really to control\rthe step size in gradient descent, and monitoring the training cost is\rthe best way to detect if the step size is too big.  With that said,\rthis is a personal aesthetic preference.  Early on during learning the\rtraining cost usually only decreases if the validation accuracy\rimproves, and so in practice it's unlikely to make much difference\rwhich criterion you use.",
            "",
            "Use early stopping to determine the number of training\r  epochs:As we discussed earlier in the chapter, early stopping means\rthat at the end of each epoch we should compute the classification\raccuracy on the validation data.  When that stops improving,\rterminate.  This makes setting the number of epochs very simple.  In\rparticular, it means that we don't need to worry about explicitly\rfiguring out how the number of epochs depends on the other\rhyper-parameters.  Instead, that's taken care of automatically.\rFurthermore, early stopping also automatically prevents us from\roverfitting.  This is, of course, a good thing, although in the early\rstages of experimentation it can be helpful to turn off early\rstopping, so you can see any signs of overfitting, and use it to\rinform your approach to regularization.",
            "To implement early stopping we need to say more precisely what it\rmeans that the classification accuracy has stopped improving.  As\rwe've seen, the accuracy can jump around quite a bit, even when the\roverall trend is to improve.  If we stop the first time the accuracy\rdecreases then we'll almost certainly stop when there are more\rimprovements to be had.  A better rule is to terminate if the best\rclassification accuracy doesn't improve for quite some time.  Suppose,\rfor example, that we're doing MNIST.  Then we might elect to terminate\rif the classification accuracy hasn't improved during the last ten\repochs.  This ensures that we don't stop too soon, in response to bad\rluck in training, but also that we're not waiting around forever for\ran improvement that never comes.",
            "This no-improvement-in-ten rule is good for initial exploration of\rMNIST.  However, networks can sometimes plateau near a particular\rclassification accuracy for quite some time, only to then begin\rimproving again.  If you're trying to get really good performance, the\rno-improvement-in-ten rule may be too aggressive about stopping.  In\rthat case, I suggest using the no-improvement-in-ten rule for initial\rexperimentation, and gradually adopting more lenient rules, as you\rbetter understand the way your network trains:\rno-improvement-in-twenty, no-improvement-in-fifty, and so on.  Of\rcourse, this introduces a new hyper-parameter to optimize!  In\rpractice, however, it's usually easy to set this hyper-parameter to\rget pretty good results.  Similarly, for problems other than MNIST,\rthe no-improvement-in-ten rule may be much too aggressive or not\rnearly aggressive enough, depending on the details of the problem.\rHowever, with a little experimentation it's usually easy to find a\rpretty good strategy for early stopping.",
            "We haven't used early stopping in our MNIST experiments to date.  The\rreason is that we've been doing a lot of comparisons between different\rapproaches to learning.  For such comparisons it's helpful to use the\rsame number of epochs in each case.  However, it's well worth\rmodifyingnetwork2.pyto implement early stopping:",
            "ProblemModifynetwork2.pyso that it implements early stopping\r  using a no-improvement-in-$n$ epochs strategy, where $n$ is a\r  parameter that can be set.",
            "Can you think of a rule for early stoppingotherthan\r  no-improvement-in-$n$?  Ideally, the rule should compromise between\r  getting high validation accuracies and not training too long.  Add\r  your rule tonetwork2.py, and run three experiments comparing\r  the validation accuracies and number of epochs of training to\r  no-improvement-in-$10$.",
            "",
            "Learning rate schedule:We've been holding the learning rate\r$\\eta$ constant.  However, it's often advantageous to vary the\rlearning rate.  Early on during the learning process it's likely that\rthe weights are badly wrong.  And so it's best to use a large learning\rrate that causes the weights to change quickly.  Later, we can reduce\rthe learning rate as we make more fine-tuned adjustments to our\rweights.",
            "How should we set our learning rate schedule?  Many approaches are\rpossible.  One natural approach is to use the same basic idea as early\rstopping.  The idea is to hold the learning rate constant until the\rvalidation accuracy starts to get worse.  Then decrease the learning\rrate by some amount, say a factor of two or ten.  We repeat this many\rtimes, until, say, the learning rate is a factor of 1,024 (or 1,000)\rtimes lower than the initial value.  Then we terminate.",
            "",
            "A variable learning schedule can improve performance, but it also\ropens up a world of possible choices for the learning schedule.  Those\rchoices can be a headache - you can spend forever trying to optimize\ryour learning schedule.  For first experiments my suggestion is to use\ra single, constant value for the learning rate.  That'll get you a\rgood first approximation.  Later, if you want to obtain the best\rperformance from your network, it's worth experimenting with a\rlearning schedule, along the lines I've described**A readable\r  recent paper which demonstrates the benefits of variable learning\r  rates in attacking MNIST isDeep, Big, Simple Neural Nets\r    Excel on Handwritten Digit Recognition, by Dan Claudiu\r  Cireșan, Ueli Meier, Luca Maria Gambardella, and\r  Jürgen Schmidhuber (2010)..",
            "ExerciseModifynetwork2.pyso that it implements a learning\r  schedule that: halves the learning rate each time the validation\r  accuracy satisfies the no-improvement-in-$10$ rule; and terminates\r  when the learning rate has dropped to $1/128$ of its original value.",
            "The regularization parameter, $\\lambda$:I suggest starting\rinitially with no regularization ($\\lambda = 0.0$), and determining a\rvalue for $\\eta$, as above.  Using that choice of $\\eta$, we can then\ruse the validation data to select a good value for $\\lambda$.  Start\rby trialling $\\lambda = 1.0$**I don't have a good principled\r  justification for using this as a starting value.  If anyone knows\r  of a good principled discussion of where to start with $\\lambda$,\r  I'd appreciate hearing it (mn@michaelnielsen.org)., and then\rincrease or decrease by factors of $10$, as needed to improve\rperformance on the validation data.  Once you've found a good order of\rmagnitude, you can fine tune your value of $\\lambda$.  That done, you\rshould return and re-optimize $\\eta$ again.",
            "ExerciseIt's tempting to use gradient descent to try to learn good\r  values for hyper-parameters such as $\\lambda$ and $\\eta$.  Can you\r  think of an obstacle to using gradient descent to determine\r  $\\lambda$?  Can you think of an obstacle to using gradient descent\r  to determine $\\eta$?",
            "How I selected hyper-parameters earlier in this book:If you\ruse the recommendations in this section you'll find that you get\rvalues for $\\eta$ and $\\lambda$ which don't always exactly match the\rvalues I've used earlier in the book.  The reason is that the book has\rnarrative constraints that have sometimes made it impractical to\roptimize the hyper-parameters.  Think of all the comparisons we've\rmade of different approaches to learning, e.g., comparing the\rquadratic and cross-entropy cost functions, comparing the old and new\rmethods of weight initialization, running with and without\rregularization, and so on.  To make such comparisons meaningful, I've\rusually tried to keep hyper-parameters constant across the approaches\rbeing compared (or to scale them in an appropriate way).  Of course,\rthere's no reason for the same hyper-parameters to be optimal for all\rthe different approaches to learning, so the hyper-parameters I've\rused are something of a compromise.",
            "As an alternative to this compromise, I could have tried to optimize\rthe heck out of the hyper-parameters for every single approach to\rlearning.  In principle that'd be a better, fairer approach, since\rthen we'd see the best from every approach to learning.  However,\rwe've made dozens of comparisons along these lines, and in practice I\rfound it too computationally expensive.  That's why I've adopted the\rcompromise of using pretty good (but not necessarily optimal) choices\rfor the hyper-parameters.",
            "",
            "Mini-batch size:How should we set the mini-batch size?  To\ranswer this question, let's first suppose that we're doing online\rlearning, i.e., that we're using a mini-batch size of $1$.",
            "The obvious worry about online learning is that using mini-batches\rwhich contain just a single training example will cause significant\rerrors in our estimate of the gradient.  In fact, though, the errors\rturn out to not be such a problem.  The reason is that the individual\rgradient estimates don't need to be super-accurate.  All we need is an\restimate accurate enough that our cost function tends to keep\rdecreasing.  It's as though you are trying to get to the North\rMagnetic Pole, but have a wonky compass that's 10-20 degrees off each\rtime you look at it.  Provided you stop to check the compass\rfrequently, and the compass gets the direction right on average,\ryou'll end up at the North Magnetic Pole just fine.",
            "Based on this argument, it sounds as though we should use online\rlearning.  In fact, the situation turns out to be more complicated\rthan that.  In aproblem\r  in the last chapterI pointed out that it's possible to use matrix\rtechniques to compute the gradient update forallexamples in a\rmini-batch simultaneously, rather than looping over them.  Depending\ron the details of your hardware and linear algebra library this can\rmake it quite a bit faster to compute the gradient estimate for a\rmini-batch of (for example) size $100$, rather than computing the\rmini-batch gradient estimate by looping over the $100$ training\rexamples separately.  It might take (say) only $50$ times as long,\rrather than $100$ times as long.",
            "Now, at first it seems as though this doesn't help us that much.  With\rour mini-batch of size $100$ the learning rule for the weights looks\rlike:\\begin{eqnarray}\r  w \\rightarrow w' = w-\\eta \\frac{1}{100} \\sum_x \\nabla C_x,\r\\tag{100}\\end{eqnarray}\rwhere the sum is over training examples in the mini-batch.  This is\rversus\\begin{eqnarray}\r  w \\rightarrow w' = w-\\eta \\nabla C_x\r\\tag{101}\\end{eqnarray}\rfor online learning.  Even if it only takes $50$ times as long to do\rthe mini-batch update, it still seems likely to be better to do online\rlearning, because we'd be updating so much more frequently.  Suppose,\rhowever, that in the mini-batch case we increase the learning rate by\ra factor $100$, so the update rule becomes\\begin{eqnarray}\r  w \\rightarrow w' = w-\\eta \\sum_x \\nabla C_x.\r\\tag{102}\\end{eqnarray}\rThat's a lot like doing $100$ separate instances of online learning\rwith a learning rate of $\\eta$.  But it only takes $50$ times as long\ras doing a single instance of online learning.  Of course, it's not\rtruly the same as $100$ instances of online learning, since in the\rmini-batch the $\\nabla C_x$'s are all evaluated for the same set of\rweights, as opposed to the cumulative learning that occurs in the\ronline case.  Still, it seems distinctly possible that using the\rlarger mini-batch would speed things up.",
            "With these factors in mind, choosing the best mini-batch size is a\rcompromise.  Too small, and you don't get to take full advantage of\rthe benefits of good matrix libraries optimized for fast hardware.\rToo large and you're simply not updating your weights often enough.\rWhat you need is to choose a compromise value which maximizes the\rspeed of learning.  Fortunately, the choice of mini-batch size at\rwhich the speed is maximized is relatively independent of the other\rhyper-parameters (apart from the overall architecture), so you don't\rneed to have optimized those hyper-parameters in order to find a good\rmini-batch size.  The way to go is therefore to use some acceptable\r(but not necessarily optimal) values for the other hyper-parameters,\rand then trial a number of different mini-batch sizes, scaling $\\eta$\ras above.  Plot the validation accuracy versustime(as in,\rreal elapsed time, not epoch!), and choose whichever mini-batch size\rgives you the most rapid improvement in performance.  With the\rmini-batch size chosen you can then proceed to optimize the other\rhyper-parameters.",
            "Of course, as you've no doubt realized, I haven't done this\roptimization in our work.  Indeed, our implementation doesn't use the\rfaster approach to mini-batch updates at all.  I've simply used a\rmini-batch size of $10$ without comment or explanation in nearly all\rexamples.  Because of this, we could have sped up learning by reducing\rthe mini-batch size.  I haven't done this, in part because I wanted to\rillustrate the use of mini-batches beyond size $1$, and in part\rbecause my preliminary experiments suggested the speedup would be\rrather modest.  In practical implementations, however, we would most\rcertainly implement the faster approach to mini-batch updates, and\rthen make an effort to optimize the mini-batch size, in order to\rmaximize our overall speed.",
            "",
            "Automated techniques:I've been describing these heuristics\ras though you're optimizing your hyper-parameters by hand.\rHand-optimization is a good way to build up a feel for how neural\rnetworks behave.  However, and unsurprisingly, a great deal of work\rhas been done on automating the process. A common technique isgrid search, which systematically searches through a grid in\rhyper-parameter space.  A review of both the achievements and the\rlimitations of grid search (with suggestions for easily-implemented\ralternatives) may be found in a 2012\rpaper**Random\r    search for hyper-parameter optimization, by James Bergstra and\r  Yoshua Bengio (2012).by James Bergstra and Yoshua Bengio.  Many\rmore sophisticated approaches have also been proposed.  I won't review\rall that work here, but do want to mention a particularly promising\r2012 paper which used a Bayesian approach to automatically optimize\rhyper-parameters**Practical\r    Bayesian optimization of machine learning algorithms, by Jasper\r  Snoek, Hugo Larochelle, and Ryan Adams..  The code from the paper\rispublicly available, and\rhas been used with some success by other researchers.",
            "Summing up:Following the rules-of-thumb I've described won't\rgive you the absolute best possible results from your neural network.\rBut it will likely give you a good start and a basis for further\rimprovements.  In particular, I've discussed the hyper-parameters\rlargely independently.  In practice, there are relationships between\rthe hyper-parameters.  You may experiment with $\\eta$, feel that\ryou've got it just right, then start to optimize for $\\lambda$, only\rto find that it's messing up your optimization for $\\eta$.  In\rpractice, it helps to bounce backward and forward, gradually closing\rin good values.  Above all, keep in mind that the heuristics I've\rdescribed are rules of thumb, not rules cast in stone.  You should be\ron the lookout for signs that things aren't working, and be willing to\rexperiment.  In particular, this means carefully monitoring your\rnetwork's behaviour, especially the validation accuracy.",
            "The difficulty of choosing hyper-parameters is exacerbated by the fact\rthat the lore about how to choose hyper-parameters is widely spread,\racross many research papers and software programs, and often is only\ravailable inside the heads of individual practitioners.  There are\rmany, many papers setting out (sometimes contradictory)\rrecommendations for how to proceed.  However, there are a few\rparticularly useful papers that synthesize and distill out much of\rthis lore.  Yoshua Bengio has a 2012\rpaper**Practical\r    recommendations for gradient-based training of deep\r    architectures, by Yoshua Bengio (2012).that gives some\rpractical recommendations for using backpropagation and gradient\rdescent to train neural networks, including deep neural nets.  Bengio\rdiscusses many issues in much more detail than I have, including how\rto do more systematic hyper-parameter searches.  Another good paper is\ra 1998\rpaper**Efficient\r    BackProp, by Yann LeCun, Léon Bottou,\r  Genevieve Orr and Klaus-Robert Müller (1998)by\rYann LeCun, Léon Bottou, Genevieve Orr and\rKlaus-Robert Müller.  Both these papers appear in\ran extremely useful 2012 book that collects many tricks commonly used\rin neural\rnets**Neural\r    Networks: Tricks of the Trade, edited by\r  Grégoire Montavon, Geneviève Orr, and Klaus-Robert\r    Müller..  The book is expensive, but many of the articles have\rbeen placed online by their respective authors with, one presumes, the\rblessing of the publisher, and may be located using a search engine.",
            "One thing that becomes clear as you read these articles and,\respecially, as you engage in your own experiments, is that\rhyper-parameter optimization is not a problem that is ever completely\rsolved.  There's always another trick you can try to improve\rperformance.  There is a saying common among writers that books are\rnever finished, only abandoned.  The same is also true of neural\rnetwork optimization: the space of hyper-parameters is so large that\rone never really finishes optimizing, one only abandons the network to\rposterity.  So your goal should be to develop a workflow that enables\ryou to quickly do a pretty good job on the optimization, while leaving\ryou the flexibility to try more detailed optimizations, if that's\rimportant.",
            "The challenge of setting hyper-parameters has led some people to\rcomplain that neural networks require a lot of work when compared with\rother machine learning techniques.  I've heard many variations on the\rfollowing complaint: \"Yes, a well-tuned neural network may get the\rbest performance on the problem.  On the other hand, I can try a\rrandom forest [or SVM or$\\ldots$ insert your own favorite technique]\rand it just works.  I don't have time to figure out just the right\rneural network.\"  Of course, from a practical point of view it's good\rto have easy-to-apply techniques.  This is particularly true when\ryou're just getting started on a problem, and it may not be obvious\rwhether machine learning can help solve the problem at all.  On the\rother hand, if getting optimal performance is important, then you may\rneed to try approaches that require more specialist knowledge.  While\rit would be nice if machine learning were always easy, there is noa priorireason it should be trivially simple.",
            "Other techniques"
        ],
        "Other techniques": [
            "Each technique developed in this chapter is valuable to know in its\rown right, but that's not the only reason I've explained them.  The\rlarger point is to familiarize you with some of the problems which can\roccur in neural networks, and with a style of analysis which can help\rovercome those problems.  In a sense, we've been learning how to think\rabout neural nets.  Over the remainder of this chapter I briefly\rsketch a handful of other techniques.  These sketches are less\rin-depth than the earlier discussions, but should convey some feeling\rfor the diversity of techniques available for use in neural networks.",
            "Variations on stochastic gradient descent",
            "Stochastic gradient descent by backpropagation has served us well in\rattacking the MNIST digit classification problem.  However, there are\rmany other approaches to optimizing the cost function, and sometimes\rthose other approaches offer performance superior to mini-batch\rstochastic gradient descent.  In this section I sketch two such\rapproaches, the Hessian and momentum techniques.",
            "Hessian technique:To begin our discussion it helps to put\rneural networks aside for a bit.  Instead, we're just going to\rconsider the abstract problem of minimizing a cost function $C$ which\ris a function of many variables, $w = w_1, w_2, \\ldots$, so $C =\rC(w)$.  By Taylor's theorem, the cost function can be approximated\rnear a point $w$ by\\begin{eqnarray}\r  C(w+\\Delta w) & = & C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j} \\Delta w_j\r  \\nonumber \\\\ & & + \\frac{1}{2} \\sum_{jk} \\Delta w_j \\frac{\\partial^2 C}{\\partial w_j\r    \\partial w_k} \\Delta w_k + \\ldots\r\\tag{103}\\end{eqnarray}\rWe can rewrite this more compactly as\\begin{eqnarray}\r  C(w+\\Delta w) = C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w + \\ldots,\r\\tag{104}\\end{eqnarray}\rwhere $\\nabla C$ is the usual gradient vector, and $H$ is a matrix\rknown as theHessian matrix, whose $jk$th entry is $\\partial^2\rC / \\partial w_j \\partial w_k$.  Suppose we approximate $C$ by\rdiscarding the higher-order terms represented by $\\ldots$ above,\\begin{eqnarray} \r  C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w.\r\\tag{105}\\end{eqnarray}\rUsing calculus we can show that the expression on the right-hand side\rcan be minimized**Strictly speaking, for this to be a minimum,\r  and not merely an extremum, we need to assume that the Hessian\r  matrix is positive definite.  Intuitively, this means that the\r  function $C$ looks like a valley locally, not a mountain or a\r  saddle.by choosing\\begin{eqnarray}\r  \\Delta w = -H^{-1} \\nabla C.\r\\tag{106}\\end{eqnarray}\rProvided(105)\\begin{eqnarray} \r  C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w \\nonumber\\end{eqnarray}is a good approximate expression for the\rcost function, then we'd expect that moving from the point $w$ to\r$w+\\Delta w = w-H^{-1} \\nabla C$ should significantly decrease the\rcost function.  That suggests a possible algorithm for minimizing the\rcost:",
            "Choose a starting point, $w$.",
            "Update $w$ to a new point $w' = w-H^{-1} \\nabla C$, where the\r  Hessian $H$ and $\\nabla C$ are computed at $w$.",
            "Update $w'$ to a new point $w{'}{'} = w'-H'^{-1} \\nabla' C$,\r  where the Hessian $H'$ and $\\nabla' C$ are computed at $w'$.",
            "$\\ldots$In practice,(105)\\begin{eqnarray} \r  C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\r  \\frac{1}{2} \\Delta w^T H \\Delta w \\nonumber\\end{eqnarray}is only an approximation, and it's\rbetter to take smaller steps.  We do this by repeatedly changing $w$\rby an amount $\\Delta w = -\\eta H^{-1} \\nabla C$, where $\\eta$ is known\ras thelearning rate.",
            "This approach to minimizing a cost function is known as theHessian techniqueorHessian optimization.  There are\rtheoretical and empirical results showing that Hessian methods\rconverge on a minimum in fewer steps than standard gradient descent.\rIn particular, by incorporating information about second-order changes\rin the cost function it's possible for the Hessian approach to avoid\rmany pathologies that can occur in gradient descent.  Furthermore,\rthere are versions of the backpropagation algorithm which can be used\rto compute the Hessian.",
            "If Hessian optimization is so great, why aren't we using it in our\rneural networks?  Unfortunately, while it has many desirable\rproperties, it has one very undesirable property: it's very difficult\rto apply in practice.  Part of the problem is the sheer size of the\rHessian matrix.  Suppose you have a neural network with $10^7$ weights\rand biases.  Then the corresponding Hessian matrix will contain $10^7\r\\times 10^7 = 10^{14}$ entries.  That's a lot of entries!  And that\rmakes computing $H^{-1} \\nabla C$ extremely difficult in practice.\rHowever, that doesn't mean that it's not useful to understand.  In\rfact, there are many variations on gradient descent which are inspired\rby Hessian optimization, but which avoid the problem with overly-large\rmatrices.  Let's take a look at one such technique, momentum-based\rgradient descent.",
            "",
            "Momentum-based gradient descent:Intuitively, the advantage\rHessian optimization has is that it incorporates not just information\rabout the gradient, but also information about how the gradient is\rchanging.  Momentum-based gradient descent is based on a similar\rintuition, but avoids large matrices of second derivatives.  To\runderstand the momentum technique, think back to ouroriginal pictureof gradient\rdescent, in which we considered a ball rolling down into a valley.  At\rthe time, we observed that gradient descent is, despite its name, only\rloosely similar to a ball falling to the bottom of a valley.  The\rmomentum technique modifies gradient descent in two ways that make it\rmore similar to the physical picture.  First, it introduces a notion\rof \"velocity\" for the parameters we're trying to optimize.  The\rgradient acts to change the velocity, not (directly) the \"position\",\rin much the same way as physical forces change the velocity, and only\rindirectly affect position.  Second, the momentum method introduces a\rkind of friction term, which tends to gradually reduce the velocity.",
            "Let's give a more precise mathematical description.  We introduce\rvelocity variables $v = v_1, v_2, \\ldots$, one for each corresponding\r$w_j$ variable**In a neural net the $w_j$ variables would, of\r  course, include all weights and biases.. Then we replace the\rgradient descent update rule $w \\rightarrow w'= w-\\eta \\nabla C$ by\\begin{eqnarray} \r  v & \\rightarrow  & v' = \\mu v - \\eta \\nabla C \\tag{107}\\\\\r  \r  w & \\rightarrow & w' = w+v'.\r\\tag{108}\\end{eqnarray}\rIn these equations, $\\mu$ is a hyper-parameter which controls the\ramount of damping or friction in the system.  To understand the\rmeaning of the equations it's helpful to first consider the case where\r$\\mu = 1$, which corresponds to no friction.  When that's the case,\rinspection of the equations shows that the \"force\" $\\nabla C$ is now\rmodifying the velocity, $v$, and the velocity is controlling the rate\rof change of $w$.  Intuitively, we build up the velocity by repeatedly\radding gradient terms to it.  That means that if the gradient is in\r(roughly) the same direction through several rounds of learning, we\rcan build up quite a bit of steam moving in that direction.  Think,\rfor example, of what happens if we're moving straight down a slope:",
            "",
            "With each step the velocity gets larger down the slope, so we move\rmore and more quickly to the bottom of the valley.  This can enable\rthe momentum technique to work much faster than standard gradient\rdescent.  Of course, a problem is that once we reach the bottom of the\rvalley we will overshoot.  Or, if the gradient should change rapidly,\rthen we could find ourselves moving in the wrong direction.  That's\rthe reason for the $\\mu$ hyper-parameter in(107)\\begin{eqnarray} \r  v & \\rightarrow  & v' = \\mu v - \\eta \\nabla C  \\nonumber\\end{eqnarray}.  I\rsaid earlier that $\\mu$ controls the amount of friction in the system;\rto be a little more precise, you should think of $1-\\mu$ as the amount\rof friction in the system.  When $\\mu = 1$, as we've seen, there is no\rfriction, and the velocity is completely driven by the gradient\r$\\nabla C$.  By contrast, when $\\mu = 0$ there's a lot of friction,\rthe velocity can't build up, and Equations(107)\\begin{eqnarray} \r  v & \\rightarrow  & v' = \\mu v - \\eta \\nabla C  \\nonumber\\end{eqnarray}and(108)\\begin{eqnarray}  \r  w & \\rightarrow & w' = w+v' \\nonumber\\end{eqnarray}reduce to the usual equation for gradient\rdescent, $w \\rightarrow w'=w-\\eta \\nabla C$.  In practice, using a\rvalue of $\\mu$ intermediate between $0$ and $1$ can give us much of the\rbenefit of being able to build up speed, but without causing\rovershooting.  We can choose such a value for $\\mu$ using the held-out\rvalidation data, in much the same way as we select $\\eta$ and\r$\\lambda$.",
            "",
            "",
            "I've avoided naming the hyper-parameter $\\mu$ up to now.  The reason\ris that the standard name for $\\mu$ is badly chosen: it's called themomentum co-efficient.  This is potentially confusing, since\r$\\mu$ is not at all the same as the notion of momentum from\rphysics. Rather, it is much more closely related to friction.\rHowever, the term momentum co-efficient is widely used, so we will\rcontinue to use it.",
            "A nice thing about the momentum technique is that it takes almost no\rwork to modify an implementation of gradient descent to incorporate\rmomentum.  We can still use backpropagation to compute the gradients,\rjust as before, and use ideas such as sampling stochastically chosen\rmini-batches.  In this way, we can get some of the advantages of the\rHessian technique, using information about how the gradient is\rchanging.  But it's done without the disadvantages, and with only\rminor modifications to our code.  In practice, the momentum technique\ris commonly used, and often speeds up learning.",
            "ExerciseWhat would go wrong if we used $\\mu > 1$ in the momentum\r  technique?",
            "What would go wrong if we used $\\mu < 0$ in the momentum\r  technique?",
            "ProblemAdd momentum-based stochastic gradient descent tonetwork2.py.",
            "Other approaches to minimizing the cost function:Many other\rapproaches to minimizing the cost function have been developed, and\rthere isn't universal agreement on which is the best approach.  As\ryou go deeper into neural networks it's worth digging into the other\rtechniques, understanding how they work, their strengths and\rweaknesses, and how to apply them in practice.  A paper I mentioned\rearlier**Efficient\r    BackProp, by Yann LeCun, Léon Bottou,\r  Genevieve Orr and Klaus-Robert Müller (1998).introduces and compares several of these techniques, including\rconjugate gradient descent and the BFGS method (see also the closely\rrelated limited-memory BFGS method, known asL-BFGS).\rAnother technique which has recently shown promising\rresults**See, for example,On the\r    importance of initialization and momentum in deep learning, by\r  Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton\r  (2012).is Nesterov's accelerated gradient technique, which\rimproves on the momentum technique.  However, for many problems, plain\rstochastic gradient descent works well, especially if momentum is\rused, and so we'll stick to stochastic gradient descent through the\rremainder of this book.",
            "Other models of artificial neuron",
            "Up to now we've built our neural networks using sigmoid neurons.  In\rprinciple, a network built from sigmoid neurons can compute any\rfunction.  In practice, however, networks built using other model\rneurons sometimes outperform sigmoid networks.  Depending on the\rapplication, networks based on such alternate models may learn faster,\rgeneralize better to test data, or perhaps do both.  Let me mention a\rcouple of alternate model neurons, to give you the flavor of some\rvariations in common use.",
            "Perhaps the simplest variation is the tanh (pronounced \"tanch\")\rneuron, which replaces the sigmoid function by the hyperbolic tangent\rfunction.  The output of a tanh neuron with input $x$, weight vector\r$w$, and bias $b$ is given by\\begin{eqnarray}\r \\tanh(w \\cdot x+b), \r\\tag{109}\\end{eqnarray}\rwhere $\\tanh$ is, of course, the hyperbolic tangent function.  It\rturns out that this is very closely related to the sigmoid neuron.  To\rsee this, recall that the $\\tanh$ function is defined by\\begin{eqnarray}\r  \\tanh(z) \\equiv \\frac{e^z-e^{-z}}{e^z+e^{-z}}.\r\\tag{110}\\end{eqnarray}\rWith a little algebra it can easily be verified that\\begin{eqnarray} \r  \\sigma(z) = \\frac{1+\\tanh(z/2)}{2},\r\\tag{111}\\end{eqnarray}\rthat is, $\\tanh$ is just a rescaled version of the sigmoid function.\rWe can also see graphically that the $\\tanh$ function has the same\rshape as the sigmoid function,",
            "",
            "One difference between tanh neurons and sigmoid neurons is that the\routput from tanh neurons ranges from -1 to 1, not 0 to 1.  This means\rthat if you're going to build a network based on tanh neurons you may\rneed to normalize your outputs (and, depending on the details of the\rapplication, possibly your inputs) a little differently than in\rsigmoid networks.",
            "Similar to sigmoid neurons, a network of tanh neurons can, in\rprinciple, compute any function**There are some technical\r  caveats to this statement for both tanh and sigmoid neurons, as well\r  as for the rectified linear neurons discussed below.  However,\r  informally it's usually fine to think of neural networks as being\r  able to approximate any function to arbitrary accuracy.mapping\rinputs to the range -1 to 1.  Furthermore, ideas such as\rbackpropagation and stochastic gradient descent are as easily applied\rto a network of tanh neurons as to a network of sigmoid neurons.",
            "ExerciseProve the identity in Equation(111)\\begin{eqnarray} \r  \\sigma(z) = \\frac{1+\\tanh(z/2)}{2} \\nonumber\\end{eqnarray}.",
            "Which type of neuron should you use in your networks, the tanh or\rsigmoid?A priorithe answer is not obvious, to put it mildly!\rHowever, there are theoretical arguments and some empirical evidence\rto suggest that the tanh sometimes performs better**See, for\r  example,Efficient\r    BackProp, by Yann LeCun, Léon Bottou,\r  Genevieve Orr and Klaus-Robert Müller (1998), andUnderstanding\r    the difficulty of training deep feedforward networks, by Xavier\r  Glorot and Yoshua Bengio (2010)..  Let me briefly give you the\rflavor of one of the theoretical arguments for tanh neurons.  Suppose\rwe're using sigmoid neurons, so all activations in our network are\rpositive.  Let's consider the weights $w^{l+1}_{jk}$ input to the\r$j$th neuron in the $l+1$th layer.  The rules for backpropagation (seehere) tell us that the associated gradient\rwill be $a^l_k \\delta^{l+1}_j$.  Because the activations are positive\rthe sign of this gradient will be the same as the sign of\r$\\delta^{l+1}_j$.  What this means is that if $\\delta^{l+1}_j$ is\rpositive thenallthe weights $w^{l+1}_{jk}$ will decrease\rduring gradient descent, while if $\\delta^{l+1}_j$ is negative thenallthe weights $w^{l+1}_{jk}$ will increase during gradient\rdescent.  In other words, all weights to the same neuron must either\rincrease together or decrease together.  That's a problem, since some\rof the weights may need to increase while others need to decrease.\rThat can only happen if some of the input activations have different\rsigns.  That suggests replacing the sigmoid by an activation function,\rsuch as $\\tanh$, which allows both positive and negative activations.\rIndeed, because $\\tanh$ is symmetric about zero, $\\tanh(-z) =\r-\\tanh(z)$, we might even expect that, roughly speaking, the\ractivations in hidden layers would be equally balanced between\rpositive and negative.  That would help ensure that there is no\rsystematic bias for the weight updates to be one way or the other.",
            "How seriously should we take this argument?  While the argument is\rsuggestive, it's a heuristic, not a rigorous proof that tanh neurons\routperform sigmoid neurons.  Perhaps there are other properties of the\rsigmoid neuron which compensate for this problem?  Indeed, for many\rtasks the tanh is found empirically to provide only a small or no\rimprovement in performance over sigmoid neurons.  Unfortunately, we\rdon't yet have hard-and-fast rules to know which neuron types will\rlearn fastest, or give the best generalization performance, for any\rparticular application.",
            "Another variation on the sigmoid neuron is therectified linear\r  neuronorrectified linear unit.  The output of a rectified\rlinear unit with input $x$, weight vector $w$, and bias $b$ is given\rby\\begin{eqnarray}\r  \\max(0, w \\cdot x+b).\r\\tag{112}\\end{eqnarray}\rGraphically, the rectifying function $\\max(0, z)$ looks like this:",
            "",
            "Obviously such neurons are quite different from both sigmoid and tanh\rneurons.  However, like the sigmoid and tanh neurons, rectified linear\runits can be used to compute any function, and they can be trained\rusing ideas such as backpropagation and stochastic gradient descent.",
            "When should you use rectified linear units instead of sigmoid or tanh\rneurons?  Some recent work on image recognition**See, for\r  example,What\r    is the Best Multi-Stage Architecture for Object Recognition?, by\r  Kevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato and Yann\r  LeCun (2009),Deep\r    Sparse Rectiﬁer Neural Networks, by Xavier Glorot, Antoine\r  Bordes, and Yoshua Bengio (2011), andImageNet\r    Classification with Deep Convolutional Neural Networks, by Alex\r  Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).  Note that\r  these papers fill in important details about how to set up the\r  output layer, cost function, and regularization in networks using\r  rectified linear units. I've glossed over all these details in this\r  brief account. The papers also discuss in more detail the benefits\r  and drawbacks of using rectified linear units.  Another informative\r  paper isRectified\r    Linear Units Improve Restricted Boltzmann Machines, by Vinod Nair\r  and Geoffrey Hinton (2010), which demonstrates the benefits of using\r  rectified linear units in a somewhat different approach to neural\r  networks.has found considerable benefit in using rectified linear\runits through much of the network.  However, as with tanh neurons, we\rdo not yet have a really deep understanding of when, exactly,\rrectified linear units are preferable, nor why.  To give you the\rflavor of some of the issues, recall that sigmoid neurons stop\rlearning when they saturate, i.e., when their output is near either\r$0$ or $1$.  As we've seen repeatedly in this chapter, the problem is\rthat $\\sigma'$ terms reduce the gradient, and that slows down\rlearning.  Tanh neurons suffer from a similar problem when they\rsaturate.  By contrast, increasing the weighted input to a rectified\rlinear unit will never cause it to saturate, and so there is no\rcorresponding learning slowdown.  On the other hand, when the weighted\rinput to a rectified linear unit is negative, the gradient vanishes,\rand so the neuron stops learning entirely.  These are just two of the\rmany issues that make it non-trivial to understand when and why\rrectified linear units perform better than sigmoid or tanh neurons.",
            "I've painted a picture of uncertainty here, stressing that we do not\ryet have a solid theory of how activation functions should be chosen.\rIndeed, the problem is harder even than I have described, for there\rare infinitely many possible activation functions.  Which is the best\rfor any given problem?  Which will result in a network which learns\rfastest?  Which will give the highest test accuracies? I am surprised\rhow little really deep and systematic investigation has been done of\rthese questions.  Ideally, we'd have a theory which tells us, in\rdetail, how to choose (and perhaps modify-on-the-fly) our activation\rfunctions.  On the other hand, we shouldn't let the lack of a full\rtheory stop us!  We have powerful tools already at hand, and can make\ra lot of progress with those tools.  Through the remainder of this\rbook I'll continue to use sigmoid neurons as our go-to neuron, since\rthey're powerful and provide concrete illustrations of the core ideas\rabout neural nets.  But keep in the back of your mind that these same\rideas can be applied to other types of neuron, and that there are\rsometimes advantages in doing so.",
            "On stories in neural networks",
            "Question:How do you\r  approach utilizing and researching machine learning techniques that\r  are supported almost entirely empirically, as opposed to\r  mathematically? Also in what situations have you noticed some of\r  these techniques fail?",
            "Answer:You have to realize that our theoretical\r  tools are very weak. Sometimes, we have good mathematical intuitions\r  for why a particular technique should work. Sometimes our intuition\r  ends up being wrong [...] The questions become: how well does my\r  method work on this particular problem, and how large is the set of\r  problems on which it works well.",
            "-Question\r    and answerwith neural networks researcher Yann LeCun",
            "Once, attending a conference on the foundations of quantum mechanics,\rI noticed what seemed to me a most curious verbal habit: when talks\rfinished, questions from the audience often began with \"I'm very\rsympathetic to your point of view, but [...]\".  Quantum foundations\rwas not my usual field, and I noticed this style of questioning\rbecause at other scientific conferences I'd rarely or never heard a\rquestioner express their sympathy for the point of view of the\rspeaker.  At the time, I thought the prevalence of the question\rsuggested that little genuine progress was being made in quantum\rfoundations, and people were merely spinning their wheels.  Later, I\rrealized that assessment was too harsh.  The speakers were wrestling\rwith some of the hardest problems human minds have ever confronted.\rOf course progress was slow!  But there was still value in hearing\rupdates on how people were thinking, even if they didn't always have\runarguable new progress to report.",
            "You may have noticed a verbal tic similar to \"I'm very sympathetic\r[...]\" in the current book.  To explain what we're seeing I've often\rfallen back on saying \"Heuristically, [...]\", or \"Roughly speaking,\r[...]\", following up with a story to explain some phenomenon or\rother.  These stories are plausible, but the empirical evidence I've\rpresented has often been pretty thin.  If you look through the\rresearch literature you'll see that stories in a similar style appear\rin many research papers on neural nets, often with thin supporting\revidence.  What should we think about such stories?",
            "In many parts of science - especially those parts that deal with\rsimple phenomena - it's possible to obtain very solid, very reliable\revidence for quite general hypotheses.  But in neural networks there\rare large numbers of parameters and hyper-parameters, and extremely\rcomplex interactions between them.  In such extraordinarily complex\rsystems it's exceedingly difficult to establish reliable general\rstatements.  Understanding neural networks in their full generality is\ra problem that, like quantum foundations, tests the limits of the\rhuman mind.  Instead, we often make do with evidence for or against a\rfew specific instances of a general statement.  As a result those\rstatements sometimes later need to be modified or abandoned, when new\revidence comes to light.",
            "One way of viewing this situation is that any heuristic story about\rneural networks carries with it an implied challenge.  For example,\rconsider the statement Iquoted\r  earlier, explaining why dropout works**FromImageNet\r    Classification with Deep Convolutional Neural Networksby Alex\r  Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).: \"This\rtechnique reduces complex co-adaptations of neurons, since a neuron\rcannot rely on the presence of particular other neurons. It is,\rtherefore, forced to learn more robust features that are useful in\rconjunction with many different random subsets of the other neurons.\"\rThis is a rich, provocative statement, and one could build a fruitful\rresearch program entirely around unpacking the statement, figuring out\rwhat in it is true, what is false, what needs variation and\rrefinement.  Indeed, there is now a small industry of researchers who\rare investigating dropout (and many variations), trying to understand\rhow it works, and what its limits are.  And so it goes with many of\rthe heuristics we've discussed.  Each heuristic is not just a\r(potential) explanation, it's also a challenge to investigate and\runderstand in more detail.",
            "Of course, there is not time for any single person to investigate all\rthese heuristic explanations in depth.  It's going to take decades (or\rlonger) for the community of neural networks researchers to develop a\rreally powerful, evidence-based theory of how neural networks learn.\rDoes this mean you should reject heuristic explanations as unrigorous,\rand not sufficiently evidence-based?  No!  In fact, we need such\rheuristics to inspire and guide our thinking.  It's like the great age\rof exploration: the early explorers sometimes explored (and made new\rdiscoveries) on the basis of beliefs which were wrong in important\rways.  Later, those mistakes were corrected as we filled in our\rknowledge of geography.  When you understand something poorly - as\rthe explorers understood geography, and as we understand neural nets\rtoday - it's more important to explore boldly than it is to be\rrigorously correct in every step of your thinking.  And so you should\rview these stories as a useful guide to how to think about neural\rnets, while retaining a healthy awareness of the limitations of such\rstories, and carefully keeping track of just how strong the evidence\ris for any given line of reasoning.  Put another way, we need good\rstories to help motivate and inspire us, and rigorous in-depth\rinvestigation in order to uncover the real facts of the matter.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ]
    },
    "CHAPTER 4": {
        "Two caveats": [
            "Before explaining why the universality theorem is true, I want to\rmention two caveats to the informal statement \"a neural network can\rcompute any function\".",
            "First, this doesn't mean that a network can be used toexactlycompute any function. Rather, we can get anapproximationthat is as good as we want.  By increasing the number of hidden\rneurons we can improve the approximation.  For instance,earlierI illustrated a network\rcomputing some function $f(x)$ using three hidden neurons.  For most\rfunctions only a low-quality approximation will be possible using\rthree hidden neurons.  By increasing the number of hidden neurons\r(say, to five) we can typically get a better approximation:",
            "",
            "And we can do still better by further increasing the number of hidden\rneurons.",
            "To make this statement more precise, suppose we're given a function\r$f(x)$ which we'd like to compute to within some desired accuracy\r$\\epsilon > 0$.  The guarantee is that by using enough hidden neurons\rwe can always find a neural network whose output $g(x)$ satisfies\r$|g(x) - f(x)| < \\epsilon$, for all inputs $x$.  In other words, the\rapproximation will be good to within the desired accuracy for every\rpossible input.",
            "The second caveat is that the class of functions which can be\rapproximated in the way described are thecontinuousfunctions.\rIf a function is discontinuous, i.e., makes sudden, sharp jumps, then\rit won't in general be possible to approximate using a neural net.\rThis is not surprising, since our neural networks compute continuous\rfunctions of their input.  However, even if the function we'd really\rlike to compute is discontinuous, it's often the case that a\rcontinuous approximation is good enough.  If that's so, then we can\ruse a neural network.  In practice, this is not usually an important\rlimitation.",
            "Summing up, a more precise statement of the universality theorem is\rthat neural networks with a single hidden layer can be used to\rapproximate any continuous function to any desired precision.  In this\rchapter we'll actually prove a slightly weaker version of this result,\rusing two hidden layers instead of one.  In the problems I'll briefly\routline how the explanation can, with a few tweaks, be adapted to give\ra proof which uses only a single hidden layer.Universality with one input and one output"
        ],
        "Universality with one input and one output": [
            "To understand why the universality theorem is true, let's start by\runderstanding how to construct a neural network which approximates a\rfunction with just one input and one output:",
            "",
            "It turns out that this is the core of the problem of universality.\rOnce we've understood this special case it's actually pretty easy to\rextend to functions with many inputs and many outputs.",
            "To build insight into how to construct a network to compute $f$, let's\rstart with a network containing just a single hidden layer, with two\rhidden neurons, and an output layer containing a single output neuron:",
            "",
            "To get a feel for how components in the network work, let's focus on\rthe top hidden neuron.  In the diagram below, click on the weight,\r$w$, and drag the mouse a little ways to the right to increase $w$.\rYou can immediately see how the function computed by the top hidden\rneuron changes:",
            "",
            "As we learntearlier in the book,\rwhat's being computed by the hidden neuron is $\\sigma(wx + b)$, where\r$\\sigma(z) \\equiv 1/(1+e^{-z})$ is the sigmoid function.  Up to now,\rwe've made frequent use of this algebraic form.  But for the proof of\runiversality we will obtain more insight by ignoring the algebra\rentirely, and instead manipulating and observing the shape shown in\rthe graph.  This won't just give us a better feel for what's going on,\rit will also give us a proof**Strictly speaking, the visual\r  approach I'm taking isn't what's traditionally thought of as a\r  proof.  But I believe the visual approach gives more insight into\r  why the result is true than a traditional proof.  And, of course,\r  that kind of insight is the real purpose behind a proof.\r  Occasionally, there will be small gaps in the reasoning I present:\r  places where I make a visual argument that is plausible, but not\r  quite rigorous.  If this bothers you, then consider it a challenge\r  to fill in the missing steps.  But don't lose sight of the real\r  purpose: to understand why the universality theorem is true.of\runiversality that applies to activation functions other than the\rsigmoid function.",
            "To get started on this proof, try clicking on the bias, $b$, in the\rdiagram above, and dragging to the right to increase it.  You'll see\rthat as the bias increases the graph moves to the left, but its shape\rdoesn't change.",
            "Next, click and drag to the left in order to decrease the bias.\rYou'll see that as the bias decreases the graph moves to the right,\rbut, again, its shape doesn't change.",
            "Next, decrease the weight to around $2$ or $3$.  You'll see that as\ryou decrease the weight, the curve broadens out.  You might need to\rchange the bias as well, in order to keep the curve in-frame.",
            "Finally, increase the weight up past $w = 100$.  As you do, the curve\rgets steeper, until eventually it begins to look like a step function.\rTry to adjust the bias so the step occurs near $x = 0.3$.  The\rfollowing short clip shows what your result should look like.  Click\ron the play button to play (or replay) the video:",
            "",
            "We can simplify our analysis quite a bit by increasing the weight so\rmuch that the output really is a step function, to a very good\rapproximation.  Below I've plotted the output from the top hidden\rneuron when the weight is $w = 999$.  Note that this plot is static,\rand you can't change parameters such as the weight.",
            "",
            "It's actually quite a bit easier to work with step functions than\rgeneral sigmoid functions.  The reason is that in the output layer we\radd up contributions from all the hidden neurons.  It's easy to\ranalyze the sum of a bunch of step functions, but rather more\rdifficult to reason about what happens when you add up a bunch of\rsigmoid shaped curves.  And so it makes things much easier to assume\rthat our hidden neurons are outputting step functions.  More\rconcretely, we do this by fixing the weight $w$ to be some very large\rvalue, and then setting the position of the step by modifying the\rbias.  Of course, treating the output as a step function is an\rapproximation, but it's a very good approximation, and for now we'll\rtreat it as exact.  I'll come back later to discuss the impact of\rdeviations from this approximation.",
            "At what value of $x$ does the step occur?  Put another way, how does\rthe position of the step depend upon the weight and bias?",
            "To answer this question, try modifying the weight and bias in the\rdiagram above (you may need to scroll back a bit).  Can you figure out\rhow the position of the step depends on $w$ and $b$?  With a little\rwork you should be able to convince yourself that the position of the\rstep isproportionalto $b$, andinversely proportionalto $w$.",
            "In fact, the step is at position $s = -b/w$, as you can see by\rmodifying the weight and bias in the following diagram:",
            "",
            "It will greatly simplify our lives to describe hidden neurons using\rjust a single parameter, $s$, which is the step position, $s = -b/w$.\rTry modifying $s$ in the following diagram, in order to get used to\rthe new parameterization:",
            "",
            "As noted above, we've implicitly set the weight $w$ on the input to be\rsome large value - big enough that the step function is a very good\rapproximation.  We can easily convert a neuron parameterized in this\rway back into the conventional model, by choosing the bias $b = -w s$.",
            "Up to now we've been focusing on the output from just the top hidden\rneuron.  Let's take a look at the behavior of the entire network.  In\rparticular, we'll suppose the hidden neurons are computing step\rfunctions parameterized by step points $s_1$ (top neuron) and $s_2$\r(bottom neuron).  And they'll have respective output weights $w_1$ and\r$w_2$.  Here's the network:",
            "",
            "What's being plotted on the right is theweighted output$w_1\ra_1 + w_2 a_2$ from the hidden layer.  Here, $a_1$ and $a_2$ are the\routputs from the top and bottom hidden neurons,\rrespectively**Note, by the way, that the output from the whole\r  network is $\\sigma(w_1 a_1+w_2 a_2 + b)$, where $b$ is the bias on\r  the output neuron.  Obviously, this isn't the same as the weighted\r  output from the hidden layer, which is what we're plotting here.\r  We're going to focus on the weighted output from the hidden layer\r  right now, and only later will we think about how that relates to\r  the output from the whole network..  These outputs are denoted with\r$a$s because they're often known as the neurons'activations.",
            "Try increasing and decreasing the step point $s_1$ of the top hidden\rneuron.  Get a feel for how this changes the weighted output from the\rhidden layer.\r\t\rIt's particularly worth understanding what happens when $s_1$ goes\rpast $s_2$.  You'll see that the graph changes shape when this\rhappens, since we have moved from a situation where the top hidden\rneuron is the first to be activated to a situation where the bottom\rhidden neuron is the first to be activated.",
            "Similarly, try manipulating the step point $s_2$ of the bottom hidden\rneuron, and get a feel for how this changes the combined output from\rthe hidden neurons.",
            "Try increasing and decreasing each of the output weights.  Notice how\rthis rescales the contribution from the respective hidden neurons.\rWhat happens when one of the weights is zero?",
            "Finally, try setting $w_1$ to be $0.8$ and $w_2$ to be $-0.8$.  You\rget a \"bump\" function, which starts at point $s_1$, ends at point\r$s_2$, and has height $0.8$.  For instance, the weighted output might\rlook like this:",
            "",
            "Of course, we can rescale the bump to have any height at all.  Let's\ruse a single parameter, $h$, to denote the height.  To reduce clutter\rI'll also remove the \"$s_1 = \\ldots$\" and \"$w_1 = \\ldots$\" notations.",
            "",
            "Try changing the value of $h$ up and down, to see how the height of\rthe bump changes.  Try changing the height so it's negative, and\robserve what happens.  And try changing the step points to see how\rthat changes the shape of the bump.",
            "You'll notice, by the way, that we're using our neurons in a way that\rcan be thought of not just in graphical terms, but in more\rconventional programming terms, as a kind ofif-then-elsestatement, e.g.:",
            "ifinput >=step point:\n        add1to the weighted outputelse:\n        add0to the weighted output",
            "For the most part I'm going to stick with the graphical point of view.\rBut in what follows you may sometimes find it helpful to switch points\rof view, and think about things in terms ofif-then-else.",
            "We can use our bump-making trick to get two bumps, by gluing two pairs\rof hidden neurons together into the same network:",
            "",
            "I've suppressed the weights here, simply writing the $h$ values for\reach pair of hidden neurons.  Try increasing and decreasing both $h$\rvalues, and observe how it changes the graph.  Move the bumps around\rby changing the step points.",
            "More generally, we can use this idea to get as many peaks as we want,\rof any height.  In particular, we can divide the interval $[0, 1]$ up\rinto a large number, $N$, of subintervals, and use $N$ pairs of hidden\rneurons to set up peaks of any desired height.  Let's see how this\rworks for $N = 5$.  That's quite a few neurons, so I'm going to pack\rthings in a bit.  Apologies for the complexity of the diagram: I could\rhide the complexity by abstracting away further, but I think it's\rworth putting up with a little complexity, for the sake of getting a\rmore concrete feel for how these networks work.",
            "",
            "You can see that there are five pairs of hidden neurons.  The step\rpoints for the respective pairs of neurons are $0, 1/5$, then $1/5,\r2/5$, and so on, out to $4/5, 5/5$.  These values are fixed - they\rmake it so we get five evenly spaced bumps on the graph.",
            "Each pair of neurons has a value of $h$ associated to it.  Remember,\rthe connections output from the neurons have weights $h$ and $-h$ (not\rmarked).  Click on one of the $h$ values, and drag the mouse to the\rright or left to change the value.  As you do so, watch the function\rchange.  By changing the output weights we're actuallydesigningthe function!",
            "Contrariwise, try clicking on the graph, and dragging up or down to\rchange the height of any of the bump functions.  As you change the\rheights, you can see the corresponding change in $h$ values.  And,\ralthough it's not shown, there is also a change in the corresponding\routput weights, which are $+h$ and $-h$.",
            "In other words, we can directly manipulate the function appearing in\rthe graph on the right, and see that reflected in the $h$ values on\rthe left.  A fun thing to do is to hold the mouse button down and drag\rthe mouse from one side of the graph to the other.  As you do this you\rdraw out a function, and get to watch the parameters in the neural\rnetwork adapt.",
            "Time for a challenge.",
            "Let's think back to the function I plotted at the beginning of the\rchapter:",
            "",
            "I didn't say it at the time, but what I plotted is actually the\rfunction\\begin{eqnarray}\rf(x) = 0.2+0.4 x^2+0.3x \\sin(15 x) + 0.05 \\cos(50 x),\r\\tag{113}\\end{eqnarray}\rplotted over $x$ from $0$ to $1$, and with the $y$ axis taking\rvalues from $0$ to $1$.",
            "That's obviously not a trivial function.",
            "You're going to figure out how to compute it using a neural network.",
            "In our networks above we've been analyzing the weighted combination\r$\\sum_j w_j a_j$ output from the hidden neurons.  We now know how to\rget a lot of control over this quantity.  But, as I noted earlier,\rthis quantity is not what's output from the network.  What's output\rfrom the network is $\\sigma(\\sum_j w_j a_j + b)$ where $b$ is the bias\ron the output neuron.  Is there some way we can achieve control over\rthe actual output from the network?",
            "The solution is to design a neural network whose hidden layer has a\rweighted output given by $\\sigma^{-1} \\circ f(x)$, where $\\sigma^{-1}$\ris just the inverse of the $\\sigma$ function.  That is, we want the\rweighted output from the hidden layer to be:",
            "",
            "If we can do this, then the output from the network as a whole will be\ra good approximation to $f(x)$**Note that I have set the bias\r  on the output neuron to $0$..",
            "Your challenge, then, is to design a neural network to approximate the\rgoal function shown just above.  To learn as much as possible, I want\ryou to solve the problem twice.  The first time, please click on the\rgraph, directly adjusting the heights of the different bump functions.\rYou should find it fairly easy to get a good match to the goal\rfunction.  How well you're doing is measured by theaverage\r  deviationbetween the goal function and the function the network is\ractually computing.  Your challenge is to drive the average deviation\raslowas possible.  You complete the challenge when you drive\rthe average deviation to $0.40$ or below.",
            "Once you've done that, click on \"Reset\" to randomly re-initialize\rthe bumps.  The second time you solve the problem, resist the urge to\rclick on the graph.  Instead, modify the $h$ values on the left-hand\rside, and again attempt to drive the average deviation to $0.40$ or\rbelow.",
            "",
            "You've now figured out all the elements necessary for the network to\rapproximately compute the function $f(x)$!  It's only a coarse\rapproximation, but we could easily do much better, merely by\rincreasing the number of pairs of hidden neurons, allowing more bumps.",
            "In particular, it's easy to convert all the data we have found back\rinto the standard parameterization used for neural networks.  Let me\rjust recap quickly how that works.",
            "The first layer of weights all have some large, constant value, say $w\r= 1000$.",
            "The biases on the hidden neurons are just $b = -w s$.  So, for\rinstance, for the second hidden neuron $s = 0.2$ becomes $b = -1000\r\\times 0.2 = -200$.",
            "The final layer of weights are determined by the $h$ values.  So, for\rinstance, the value you've chosen above for the first $h$, $h = $, means that\rthe output weights from the top two hidden neurons areand, respectively.  And\rso on, for the entire layer of output weights.",
            "Finally, the bias on the output neuron is $0$.",
            "That's everything: we now have a complete description of a neural\rnetwork which does a pretty good job computing our original goal\rfunction.  And we understand how to improve the quality of the\rapproximation by improving the number of hidden neurons.",
            "What's more, there was nothing special about our original goal\rfunction, $f(x) = 0.2+0.4 x^2+0.3 \\sin(15 x) + 0.05 \\cos(50 x)$.  We\rcould have used this procedure for any continuous function from $[0,\r1]$ to $[0, 1]$.  In essence, we're using our single-layer neural\rnetworks to build a lookup table for the function.  And we'll be able\rto build on this idea to provide a general proof of universality.",
            "Many input variables"
        ],
        "Many input variables": [
            "Let's extend our results to the case of many input variables.  This\rsounds complicated, but all the ideas we need can be understood in the\rcase of just two inputs.  So let's address the two-input case.",
            "We'll start by considering what happens when we have two inputs to a\rneuron:",
            "",
            "Here, we have inputs $x$ and $y$, with corresponding weights $w_1$ and\r$w_2$, and a bias $b$ on the neuron.  Let's set the weight $w_2$ to\r$0$, and then play around with the first weight, $w_1$, and the bias,\r$b$, to see how they affect the output from the neuron:",
            "",
            "",
            "As you can see, with $w_2 = 0$ the input $y$ makes no difference to\rthe output from the neuron.  It's as though $x$ is the only input.",
            "Given this, what do you think happens when we increase the weight\r$w_1$ to $w_1 = 100$, with $w_2$ remaining $0$?  If you don't\rimmediately see the answer, ponder the question for a bit, and see if\ryou can figure out what happens.  Then try it out and see if you're\rright.  I've shown what happens in the following movie:",
            "",
            "",
            "Just as in our earlier discussion, as the input weight gets larger the\routput approaches a step function.  The difference is that now the\rstep function is in three dimensions.  Also as before, we can move the\rlocation of the step point around by modifying the bias.  The actual\rlocation of the step point is $s_x \\equiv -b / w_1$.",
            "Let's redo the above using the position of the step as the parameter:",
            "",
            "Here, we assume the weight on the $x$ input has some large value\r- I've used $w_1 = 1000$ - and the weight $w_2 = 0$.  The\rnumber on the neuron is the step point, and the little $x$ above the\rnumber reminds us that the step is in the $x$ direction.\r\t\rOf course, it's also possible to get a step function in the $y$\rdirection, by making the weight on the $y$ input very large (say, $w_2\r= 1000$), and the weight on the $x$ equal to $0$, i.e., $w_1 = 0$:",
            "",
            "The number on the neuron is again the step point, and in this case the\rlittle $y$ above the number reminds us that the step is in the $y$\rdirection.  I could have explicitly marked the weights on the $x$ and\r$y$ inputs, but decided not to, since it would make the diagram rather\rcluttered.  But do keep in mind that the little $y$ marker implicitly\rtells us that the $y$ weight is large, and the $x$ weight is $0$.",
            "We can use the step functions we've just constructed to compute a\rthree-dimensional bump function.  To do this, we use two neurons, each\rcomputing a step function in the $x$ direction.  Then we combine those\rstep functions with weight $h$ and $-h$, respectively, where $h$ is\rthe desired height of the bump.  It's all illustrated in the following\rdiagram:",
            "",
            "Try changing the value of the height, $h$. Observe how it relates to\rthe weights in the network.  And see how it changes the height of the\rbump function on the right.",
            "Also, try changing the step point $0.30$ associated to the top hidden\rneuron.  Witness how it changes the shape of the bump.  What happens\rwhen you move it past the step point $0.70$ associated to the bottom\rhidden neuron?",
            "We've figured out how to make a bump function in the $x$ direction.\rOf course, we can easily make a bump function in the $y$ direction, by\rusing two step functions in the $y$ direction.  Recall that we do this\rby making the weight large on the $y$ input, and the weight $0$ on the\r$x$ input.  Here's the result:",
            "",
            "This looks nearly identical to the earlier network!  The only thing\rexplicitly shown as changing is that there's now little $y$ markers on\rour hidden neurons.  That reminds us that they're producing $y$ step\rfunctions, not $x$ step functions, and so the weight is very large on\rthe $y$ input, and zero on the $x$ input, not vice versa.  As before,\rI decided not to show this explicitly, in order to avoid clutter.",
            "Let's consider what happens when we add up two bump functions, one in\rthe $x$ direction, the other in the $y$ direction, both of height $h$:",
            "",
            "To simplify the diagram I've dropped the connections with zero weight.\rFor now, I've left in the little $x$ and $y$ markers on the hidden\rneurons, to remind you in what directions the bump functions are being\rcomputed.  We'll drop even those markers later, since they're implied\rby the input variable.",
            "Try varying the parameter $h$.  As you can see, this causes the output\rweights to change, and also the heights of both the $x$ and $y$ bump\rfunctions.",
            "What we've built looks a little like atowerfunction:",
            "",
            "If we could build such tower functions, then we could use them to\rapproximate arbitrary functions, just by adding up many towers of\rdifferent heights, and in different locations:",
            "",
            "Of course, we haven't yet figured out how to build a tower function.\rWhat we have constructed looks like a central tower, of height $2h$,\rwith a surrounding plateau, of height $h$.",
            "But we can make a tower function.  Remember that earlier we saw\rneurons can be used to implement a type ofif-then-elsestatement:",
            "ifinput >=threshold: \n        output 1else:\n        output 0",
            "That was for a neuron with just a single input.  What we want is to\rapply a similar idea to the combined output from the hidden neurons:",
            "ifcombined output from hidden neurons >=threshold:\n        output 1else:\n        output 0",
            "If we choose thethresholdappropriately - say, a value of\r$3h/2$, which is sandwiched between the height of the plateau and the\rheight of the central tower - we could squash the plateau down to\rzero, and leave just the tower standing.",
            "Can you see how to do this?  Try experimenting with the following\rnetwork to figure it out.  Note that we're now plotting the output\rfrom the entire network, not just the weighted output from the hidden\rlayer.  This means we add a bias term to the weighted output from the\rhidden layer, and apply the sigma function.  Can you find values for\r$h$ and $b$ which produce a tower?  This is a bit tricky, so if you\rthink about this for a while and remain stuck, here's two hints: (1)\rTo get the output neuron to show the right kind ofif-then-elsebehaviour, we need the input weights (all $h$ or $-h$) to be large;\rand (2) the value of $b$ determines the scale of theif-then-elsethreshold.",
            "",
            "With our initial parameters, the output looks like a flattened version\rof the earlier diagram, with its tower and plateau.  To get the\rdesired behaviour, we increase the parameter $h$ until it becomes\rlarge.  That gives theif-then-elsethresholding\rbehaviour.  Second, to get the threshold right, we'll choose $b\r\\approx -3h/2$.  Try it, and see how it works!",
            "Here's what it looks like, when we use $h = 10$:",
            "",
            "Even for this relatively modest value of $h$, we get a pretty good\rtower function.  And, of course, we can make it as good as we want by\rincreasing $h$ still further, and keeping the bias as $b = -3h/2$.",
            "Let's try gluing two such networks together, in order to compute two\rdifferent tower functions.  To make the respective roles of the two\rsub-networks clear I've put them in separate boxes, below: each box\rcomputes a tower function, using the technique described above.  The\rgraph on the right shows the weighted output from thesecondhidden layer, that is, it's a weighted combination of tower functions.",
            "",
            "In particular, you can see that by modifying the weights in the final\rlayer you can change the height of the output towers.",
            "The same idea can be used to compute as many towers as we like.  We\rcan also make them as thin as we like, and whatever height we like.\rAs a result, we can ensure that the weighted output from the second\rhidden layer approximates any desired function of two variables:",
            "",
            "In particular, by making the weighted output from the second hidden\rlayer a good approximation to $\\sigma^{-1} \\circ f$, we ensure the\routput from our network will be a good approximation to any desired\rfunction, $f$.",
            "What about functions of more than two variables?",
            "Let's try three variables $x_1, x_2, x_3$.  The following network can\rbe used to compute a tower function in four dimensions:",
            "",
            "Here, the $x_1, x_2, x_3$ denote inputs to the network.  The $s_1,\rt_1$ and so on are step points for neurons - that is, all the\rweights in the first layer are large, and the biases are set to give\rthe step points $s_1, t_1, s_2, \\ldots$.  The weights in the second\rlayer alternate $+h, -h$, where $h$ is some very large number.  And\rthe output bias is $-5h/2$.",
            "This network computes a function which is $1$ provided three\rconditions are met: $x_1$ is between $s_1$ and $t_1$; $x_2$ is between\r$s_2$ and $t_2$; and $x_3$ is between $s_3$ and $t_3$.  The network is\r$0$ everywhere else.  That is, it's a kind of tower which is $1$ in a\rlittle region of input space, and $0$ everywhere else.",
            "By gluing together many such networks we can get as many towers as we\rwant, and so approximate an arbitrary function of three variables.\rExactly the same idea works in $m$ dimensions.  The only change needed\ris to make the output bias $(-m+1/2)h$, in order to get the right kind\rof sandwiching behavior to level the plateau.",
            "Okay, so we now know how to use neural networks to approximate a\rreal-valued function of many variables.  What about vector-valued\rfunctions $f(x_1, \\ldots, x_m) \\in R^n$?  Of course, such a function\rcan be regarded as just $n$ separate real-valued functions, $f^1(x_1,\r\\ldots, x_m), f^2(x_1, \\ldots, x_m)$, and so on.  So we create a\rnetwork approximating $f^1$, another network for $f^2$, and so on.\rAnd then we simply glue all the networks together.  So that's also\reasy to cope with.",
            "Problem",
            "We've seen how to use networks with two hidden layers to\r  approximate an arbitrary function.  Can you find a proof showing\r  that it's possible with just a single hidden layer?  As a hint, try\r  working in the case of just two input variables, and showing that:\r  (a) it's possible to get step functions not just in the $x$ or $y$\r  directions, but in an arbitrary direction; (b) by adding up many of\r  the constructions from part (a) it's possible to approximate a tower\r  function which is circular in shape, rather than rectangular; (c)\r  using these circular towers, it's possible to approximate an\r  arbitrary function.  To do part (c) it may help to use ideas from abit later in this\r    chapter.",
            "",
            "Extension beyond sigmoid neurons"
        ],
        "Extension beyond sigmoid neurons": [
            "We've proved that networks made up of sigmoid neurons can compute any\rfunction.  Recall that in a sigmoid neuron the inputs $x_1, x_2,\r\\ldots$ result in the output $\\sigma(\\sum_j w_j x_j + b)$, where $w_j$\rare the weights, $b$ is the bias, and $\\sigma$ is the sigmoid\rfunction:",
            "",
            "What if we consider a different type of neuron, one using some other\ractivation function, $s(z)$:",
            "",
            "That is, we'll assume that if our neurons has inputs $x_1, x_2,\r\\ldots$, weights $w_1, w_2, \\ldots$ and bias $b$, then the output is\r$s(\\sum_j w_j x_j + b)$.",
            "We can use this activation function to get a step function, just as we\rdid with the sigmoid.  Try ramping up the weight in the following, say\rto $w = 100$:",
            "",
            "Just as with the sigmoid, this causes the activation function to\rcontract, and ultimately it becomes a very good approximation to a\rstep function.  Try changing the bias, and you'll see that we can set\rthe position of the step to be wherever we choose.  And so we can use\rall the same tricks as before to compute any desired function.",
            "What properties does $s(z)$ need to satisfy in order for this to work?\rWe do need to assume that $s(z)$ is well-defined as $z \\rightarrow\r-\\infty$ and $z \\rightarrow \\infty$.  These two limits are the two\rvalues taken on by our step function.  We also need to assume that\rthese limits are different from one another.  If they weren't, there'd\rbe no step, simply a flat graph!  But provided the activation function\r$s(z)$ satisfies these properties, neurons based on such an activation\rfunction are universal for computation.",
            "ProblemsEarlier in the book we met another type of neuron known as arectified linear\r  unit.  Explain why such neurons don't satisfy the conditions\r  just given for universality.  Find a proof of universality showing\r  that rectified linear units are universal for computation.",
            "Suppose we consider linear neurons, i.e., neurons with the\r  activation function $s(z) = z$.  Explain why linear neurons don't\r  satisfy the conditions just given for universality.  Show that such\r  neurons can't be used to do universal computation.",
            "Fixing up the step functions"
        ],
        "Fixing up the step functions": [
            "Up to now, we've been assuming that our neurons can produce step\rfunctions exactly.  That's a pretty good approximation, but it is only\ran approximation.  In fact, there will be a narrow window of failure,\rillustrated in the following graph, in which the function behaves very\rdifferently from a step function:",
            "",
            "In these windows of failure the explanation I've given for\runiversality will fail.",
            "Now, it's not a terrible failure.  By making the weights input to the\rneurons big enough we can make these windows of failure as small as we\rlike.  Certainly, we can make the window much narrower than I've shown\rabove - narrower, indeed, than our eye could see.  So perhaps we\rmight not worry too much about this problem.",
            "Nonetheless, it'd be nice to have some way of addressing the problem.",
            "In fact, the problem turns out to be easy to fix.  Let's look at the\rfix for neural networks computing functions with just one input and\rone output.  The same ideas work also to address the problem when\rthere are more inputs and outputs.",
            "In particular, suppose we want our network to compute some function,\r$f$.  As before, we do this by trying to design our network so that\rthe weighted output from our hidden layer of neurons is $\\sigma^{-1}\r\\circ f(x)$:",
            "",
            "If we were to do this using the technique described earlier, we'd use\rthe hidden neurons to produce a sequence of bump functions:",
            "",
            "Again, I've exaggerated the size of the windows of failure, in order\rto make them easier to see.  It should be pretty clear that if we add\rall these bump functions up we'll end up with a reasonable\rapproximation to $\\sigma^{-1} \\circ f(x)$, except within the windows\rof failure.",
            "Suppose that instead of using the approximation just described, we use\ra set of hidden neurons to compute an approximation tohalfour\roriginal goal function, i.e., to $\\sigma^{-1} \\circ f(x) / 2$.  Of\rcourse, this looks just like a scaled down version of the last graph:",
            "",
            "And suppose we use another set of hidden neurons to compute an\rapproximation to $\\sigma^{-1} \\circ f(x)/ 2$, but with the bases of\rthe bumpsshiftedby half the width of a bump:",
            "",
            "Now we have two different approximations to $\\sigma^{-1} \\circ f(x) /\r2$.  If we add up the two approximations we'll get an overall\rapproximation to $\\sigma^{-1} \\circ f(x)$.  That overall approximation\rwill still have failures in small windows.  But the problem will be\rmuch less than before.  The reason is that points in a failure window\rfor one approximation won't be in a failure window for the other.  And\rso the approximation will be a factor roughly $2$ better in those\rwindows.",
            "We could do even better by adding up a large number, $M$, of\roverlapping approximations to the function $\\sigma^{-1} \\circ f(x) /\rM$. Provided the windows of failure are narrow enough, a point will\ronly ever be in one window of failure.  And provided we're using a\rlarge enough number $M$ of overlapping approximations, the result will\rbe an excellent overall approximation.",
            "Conclusion"
        ],
        "Conclusion": [
            "The explanation for universality we've discussed is certainly not a\rpractical prescription for how to compute using neural networks!  In\rthis, it's much like proofs of universality forNANDgates and\rthe like.  For this reason, I've focused mostly on trying to make the\rconstruction clear and easy to follow, and not on optimizing the\rdetails of the construction.  However, you may find it a fun and\rinstructive exercise to see if you can improve the construction.",
            "Although the result isn't directly useful in constructing networks,\rit's important because it takes off the table the question of whether\rany particular function is computable using a neural network.  The\ranswer to that question is always \"yes\".  So the right question to\rask is not whether any particular function is computable, but rather\rwhat's agoodway to compute the function.",
            "The universality construction we've developed uses just two hidden\rlayers to compute an arbitrary function.  Furthermore, as we've\rdiscussed, it's possible to get the same result with just a single\rhidden layer.  Given this, you might wonder why we would ever be\rinterested in deep networks, i.e., networks with many hidden layers.\rCan't we simply replace those networks with shallow, single hidden\rlayer networks?",
            "Chapter\r  acknowledgments:Thanks toJen DoddandChris Olahfor many\r  discussions about universality in neural networks.  My thanks, in\r  particular, to Chris for suggesting the use of a lookup table to\r  prove universality.  The interactive visual form of the chapter is\r  inspired by the work of people such asMike Bostock,Amit\r  Patel,Bret Victor, andSteven Wittens.",
            "While in principle that's possible, there are good practical reasons\rto use deep networks.  As argued inChapter 1, deep networks\rhave a hierarchical structure which makes them particularly well\radapted to learn the hierarchies of knowledge that seem to be useful\rin solving real-world problems.  Put more concretely, when attacking\rproblems such as image recognition, it helps to use a system that\runderstands not just individual pixels, but also increasingly more\rcomplex concepts: from edges to simple geometric shapes, all the way\rup through complex, multi-object scenes. In later chapters, we'll see\revidence suggesting that deep networks do a better job than shallow\rnetworks at learning such hierarchies of knowledge.  To sum up:\runiversality tells us that neural networks can compute any function;\rand empirical evidence suggests that deep networks are the networks\rbest adapted to learn the functions useful in solving many real-world\rproblems.",
            "..",
            "",
            "In academic work,\nplease cite this book as: Michael A. Nielsen, \"Neural Networks and\nDeep Learning\", Determination Press, 2015This work is licensed under aCreative Commons Attribution-NonCommercial 3.0\nUnported License.  This means you're free to copy, share, and\nbuild on this book, but not to sell it.  If you're interested in\ncommercial use, pleasecontact me.Last update: Thu Dec 26 15:26:33 2019"
        ]
    },
    "CHAPTER 5": {
        "The vanishing gradient problem": [
            "So, what goes wrong when we try to train a deep network?",
            "To answer that question, let's first revisit the case of a network\rwith just a single hidden layer.  As per usual, we'll use the MNIST\rdigit classification problem as our playground for learning and\rexperimentation**I introduced the MNIST problem and datahereandhere..",
            "If you wish, you can follow along by training networks on your\rcomputer.  It is also, of course, fine to just read along.  If you do\rwish to follow live, then you'll need Python 2.7, Numpy, and a copy of\rthe code, which you can get by cloning the relevant repository from\rthe command line:git clone https://github.com/mnielsen/neural-networks-and-deep-learning.gitIf you don't usegitthen you can download the data and codehere.  You'll need to change into thesrcsubdirectory.",
            "Then, from a Python shell we load the MNIST data:",
            ">>>importmnist_loader>>>training_data,validation_data,test_data=\\...mnist_loader.load_data_wrapper()",
            "We set up our network:",
            ">>>importnetwork2>>>net=network2.Network([784,30,10])",
            "This network has 784 neurons in the input layer, corresponding to the\r$28 \\times 28 = 784$ pixels in the input image.  We use 30 hidden\rneurons, as well as 10 output neurons, corresponding to the 10\rpossible classifications for the MNIST digits ('0', '1', '2', $\\ldots$,\r'9').",
            "Let's try training our network for 30 complete epochs, using\rmini-batches of 10 training examples at a time, a learning rate $\\eta\r= 0.1$, and regularization parameter $\\lambda = 5.0$.  As we train\rwe'll monitor the classification accuracy on thevalidation_data**Note that the networks is likely to\r  take some minutes to train, depending on the speed of your machine.\r  So if you're running the code you may wish to continue reading and\r  return later, not wait for the code to finish executing.:",
            ">>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,monitor_evaluation_accuracy=True)",
            "We get a classification accuracy of 96.48 percent (or thereabouts -\rit'll vary a bit from run to run), comparable to our earlier results\rwith a similar configuration.",
            "Now, let's add another hidden layer, also with 30 neurons in it, and\rtry training with the same hyper-parameters:",
            ">>>net=network2.Network([784,30,30,10])>>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,monitor_evaluation_accuracy=True)",
            "This gives an improved classification accuracy, 96.90 percent.  That's\rencouraging: a little more depth is helping.  Let's add another\r30-neuron hidden layer:",
            ">>>net=network2.Network([784,30,30,30,10])>>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,monitor_evaluation_accuracy=True)",
            "That doesn't help at all.  In fact, the result drops back down to\r96.57 percent, close to our original shallow network.  And suppose we\rinsert one further hidden layer:",
            ">>>net=network2.Network([784,30,30,30,30,10])>>>net.SGD(training_data,30,10,0.1,lmbda=5.0,...evaluation_data=validation_data,monitor_evaluation_accuracy=True)",
            "The classification accuracy drops again, to 96.53 percent.  That's\rprobably not a statistically significant drop, but it's not\rencouraging, either.",
            "This behaviour seems strange.  Intuitively, extra hidden layers ought\rto make the network able to learn more complex classification\rfunctions, and thus do a better job classifying.  Certainly, things\rshouldn't get worse, since the extra layers can, in the worst case,\rsimply do nothing**Seethis later\r    problemto understand how to build a hidden layer that does\r  nothing..  But that's not what's going on.",
            "So what is going on?  Let's assume that the extra hidden layers really\rcould help in principle, and the problem is that our learning\ralgorithm isn't finding the right weights and biases.  We'd like to\rfigure out what's going wrong in our learning algorithm, and how to do\rbetter.",
            "To get some insight into what's going wrong, let's visualize how the\rnetwork learns.  Below, I've plotted part of a $[784, 30, 30, 10]$\rnetwork, i.e., a network with two hidden layers, each containing $30$\rhidden neurons.  Each neuron in the diagram has a little bar on it,\rrepresenting how quickly that neuron is changing as the network\rlearns.  A big bar means the neuron's weights and bias are changing\rrapidly, while a small bar means the weights and bias are changing\rslowly.  More precisely, the bars denote the gradient $\\partial C\r/ \\partial b$ for each neuron, i.e., the rate of change of the cost\rwith respect to the neuron's bias.  Back inChapter\r  2we saw that this gradient quantity controlled not just how\rrapidly the bias changes during learning, but also how rapidly the\rweights input to the neuron change, too.  Don't worry if you don't\rrecall the details: the thing to keep in mind is simply that these\rbars show how quickly each neuron's weights and bias are changing as\rthe network learns.",
            "To keep the diagram simple, I've shown just the top six neurons in the\rtwo hidden layers.  I've omitted the input neurons, since they've got\rno weights or biases to learn.  I've also omitted the output neurons,\rsince we're doing layer-wise comparisons, and it makes most sense to\rcompare layers with the same number of neurons.  The results are\rplotted at the very beginning of training, i.e., immediately after the\rnetwork is initialized.  Here they are**The data plotted is\r  generated using the programgenerate_gradient.py.\r  The same program is also used to generate the results quoted later\r  in this section.:",
            "",
            "The network was initialized randomly, and so it's not surprising that\rthere's a lot of variation in how rapidly the neurons learn.  Still,\rone thing that jumps out is that the bars in the second hidden layer\rare mostly much larger than the bars in the first hidden layer.  As a\rresult, the neurons in the second hidden layer will learn quite a bit\rfaster than the neurons in the first hidden layer.  Is this merely a\rcoincidence, or are the neurons in the second hidden layer likely to\rlearn faster than neurons in the first hidden layer in general?",
            "To determine whether this is the case, it helps to have a global way\rof comparing the speed of learning in the first and second hidden\rlayers.  To do this, let's denote the gradient as $\\delta^l_j\r= \\partial C /\r\\partial b^l_j$, i.e., the gradient for the $j$th neuron in the $l$th\rlayer**Back inChapter\r    2we referred to this as the error, but here we'll adopt the\r  informal term \"gradient\".  I say \"informal\" because of course\r  this doesn't explicitly include the partial derivatives of the cost\r  with respect to the weights, $\\partial C / \\partial w$..  We can\rthink of the gradient $\\delta^1$ as a vector whose entries determine\rhow quickly the first hidden layer learns, and $\\delta^2$ as a vector\rwhose entries determine how quickly the second hidden layer learns.\rWe'll then use the lengths of these vectors as (rough!)  global\rmeasures of the speed at which the layers are learning.  So, for\rinstance, the length $\\| \\delta^1 \\|$ measures the speed at which the\rfirst hidden layer is learning, while the length $\\| \\delta^2 \\|$\rmeasures the speed at which the second hidden layer is learning.",
            "With these definitions, and in the same configuration as was plotted\rabove, we find $\\| \\delta^1 \\| = 0.07\\ldots$ and $\\| \\delta^2 \\| =\r0.31\\ldots$.  So this confirms our earlier suspicion: the neurons in\rthe second hidden layer really are learning much faster than the\rneurons in the first hidden layer.",
            "What happens if we add more hidden layers?  If we have three hidden\rlayers, in a $[784, 30, 30, 30, 10]$ network, then the respective\rspeeds of learning turn out to be 0.012, 0.060, and 0.283.  Again,\rearlier hidden layers are learning much slower than later hidden\rlayers.  Suppose we add yet another layer with $30$ hidden neurons.\rIn that case, the respective speeds of learning are 0.003, 0.017,\r0.070, and 0.285.  The pattern holds: early layers learn slower than\rlater layers.",
            "We've been looking at the speed of learning at the start of training,\rthat is, just after the networks are initialized.  How does the speed\rof learning change as we train our networks?  Let's return to look at\rthe network with just two hidden layers.  The speed of learning\rchanges as follows:",
            "",
            "To generate these results, I used batch gradient descent with just\r1,000 training images, trained over 500 epochs.  This is a bit\rdifferent than the way we usually train - I've used no mini-batches,\rand just 1,000 training images, rather than the full 50,000 image\rtraining set.  I'm not trying to do anything sneaky, or pull the wool\rover your eyes, but it turns out that using mini-batch stochastic\rgradient descent gives much noisier (albeit very similar, when you\raverage away the noise) results.  Using the parameters I've chosen is\ran easy way of smoothing the results out, so we can see what's going\ron.",
            "In any case, as you can see the two layers start out learning at very\rdifferent speeds (as we already know).  The speed in both layers then\rdrops very quickly, before rebounding.  But through it all, the first\rhidden layer learns much more slowly than the second hidden layer.",
            "What about more complex networks?  Here's the results of a similar\rexperiment, but this time with three hidden layers (a $[784, 30, 30,\r30, 10]$ network):",
            "",
            "Again, early hidden layers learn much more slowly than later hidden\rlayers.  Finally, let's add a fourth hidden layer (a $[784, 30, 30,\r30, 30, 10]$ network), and see what happens when we train:",
            "",
            "Again, early hidden layers learn much more slowly than later hidden\rlayers.  In this case, the first hidden layer is learning roughly 100\rtimes slower than the final hidden layer.  No wonder we were having\rtrouble training these networks earlier!",
            "We have here an important observation: in at least some deep neural\rnetworks, the gradient tends to get smaller as we move backward\rthrough the hidden layers.  This means that neurons in the earlier\rlayers learn much more slowly than neurons in later layers.  And while\rwe've seen this in just a single network, there are fundamental\rreasons why this happens in many neural networks.  The phenomenon is\rknown as thevanishing gradient problem**SeeGradient\r    flow in recurrent nets: the difficulty of learning long-term\r    dependencies, by Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi,\r  and Jürgen Schmidhuber (2001).  This paper studied recurrent\r  neural nets, but the essential phenomenon is the same as in the\r  feedforward networks we are studying.  See also Sepp Hochreiter's\r  earlier Diploma Thesis,Untersuchungen\r    zu dynamischen neuronalen Netzen(1991, in German)..",
            "Why does the vanishing gradient problem occur?  Are there ways we can\ravoid it?  And how should we deal with it in training deep neural\rnetworks?  In fact, we'll learn shortly that it's not inevitable,\ralthough the alternative is not very attractive, either: sometimes the\rgradient gets much larger in earlier layers!  This is theexploding gradient problem, and it's not much better news than\rthe vanishing gradient problem.  More generally, it turns out that the\rgradient in deep neural networks isunstable, tending to either\rexplode or vanish in earlier layers.  This instability is a\rfundamental problem for gradient-based learning in deep neural\rnetworks.  It's something we need to understand, and, if possible,\rtake steps to address.",
            "One response to vanishing (or unstable) gradients is to wonder if\rthey're really such a problem.  Momentarily stepping away from neural\rnets, imagine we were trying to numerically minimize a function $f(x)$\rof a single variable.  Wouldn't it be good news if the derivative\r$f'(x)$ was small?  Wouldn't that mean we were already near an\rextremum?  In a similar way, might the small gradient in early layers\rof a deep network mean that we don't need to do much adjustment of the\rweights and biases?",
            "Of course, this isn't the case.  Recall that we randomly initialized\rthe weight and biases in the network.  It is extremely unlikely our\rinitial weights and biases will do a good job at whatever it is we\rwant our network to do.  To be concrete, consider the first layer of\rweights in a $[784, 30, 30, 30, 10]$ network for the MNIST problem.\rThe random initialization means the first layer throws away most\rinformation about the input image.  Even if later layers have been\rextensively trained, they will still find it extremely difficult to\ridentify the input image, simply because they don't have enough\rinformation.  And so it can't possibly be the case that not much\rlearning needs to be done in the first layer.  If we're going to train\rdeep networks, we need to figure out how to address the vanishing\rgradient problem.",
            "What's causing the vanishing gradient problem?  Unstable gradients in deep neural nets"
        ],
        "What's causing the vanishing gradient problem?  Unstable gradients in deep neural nets": [
            "To get insight into why the vanishing gradient problem occurs, let's\rconsider the simplest deep neural network: one with just a single\rneuron in each layer.  Here's a network with three hidden layers:",
            "Here, $w_1, w_2, \\ldots$ are the weights, $b_1, b_2, \\ldots$ are the\rbiases, and $C$ is some cost function.  Just to remind you how this\rworks, the output $a_j$ from the $j$th neuron is $\\sigma(z_j)$, where\r$\\sigma$ is the usualsigmoid\r  activation function, and $z_j = w_{j} a_{j-1}+b_j$ is the weighted\rinput to the neuron.  I've drawn the cost $C$ at the end to emphasize\rthat the cost is a function of the network's output, $a_4$: if the\ractual output from the network is close to the desired output, then\rthe cost will be low, while if it's far away, the cost will be high.",
            "We're going to study the gradient $\\partial C / \\partial b_1$\rassociated to the first hidden neuron.  We'll figure out an expression\rfor $\\partial C / \\partial b_1$, and by studying that expression we'll\runderstand why the vanishing gradient problem occurs.",
            "I'll start by simply showing you the expression for $\\partial C /\r\\partial b_1$.  It looks forbidding, but it's actually got a simple\rstructure, which I'll describe in a moment.  Here's the expression\r(ignore the network, for now, and note that $\\sigma'$ is just the\rderivative of the $\\sigma$ function):",
            "",
            "The structure in the expression is as follows: there is a\r$\\sigma'(z_j)$ term in the product for each neuron in the network; a\rweight $w_j$ term for each weight in the network; and a final\r$\\partial C / \\partial a_4$ term, corresponding to the cost function\rat the end.  Notice that I've placed each term in the expression above\rthe corresponding part of the network.  So the network itself is a\rmnemonic for the expression.",
            "You're welcome to take this expression for granted, and skip to thediscussion of how it relates to the vanishing\r  gradient problem.  There's no harm in doing this, since the\rexpression is a special case of ourearlier\r  discussion of backpropagation.  But there's also a simple\rexplanation of why the expression is true, and so it's fun (and\rperhaps enlightening) to take a look at that explanation.",
            "Imagine we make a small change $\\Delta b_1$ in the bias $b_1$.  That\rwill set off a cascading series of changes in the rest of the network.\rFirst, it causes a change $\\Delta a_1$ in the output from the first\rhidden neuron.  That, in turn, will cause a change $\\Delta z_2$ in the\rweighted input to the second hidden neuron.  Then a change $\\Delta\ra_2$ in the output from the second hidden neuron.  And so on, all the\rway through to a change $\\Delta C$ in the cost at the output.  We have\\begin{eqnarray}\r  \\frac{\\partial C}{\\partial b_1} \\approx \\frac{\\Delta C}{\\Delta b_1}.\r\\tag{114}\\end{eqnarray}\rThis suggests that we can figure out an expression for the gradient\r$\\partial C / \\partial b_1$ by carefully tracking the effect of each\rstep in this cascade.",
            "To do this, let's think about how $\\Delta b_1$ causes the output $a_1$\rfrom the first hidden neuron to change.  We have $a_1 = \\sigma(z_1) =\r\\sigma(w_1 a_0 + b_1)$, so\\begin{eqnarray}\r  \\Delta a_1 & \\approx & \r  \\frac{\\partial \\sigma(w_1 a_0+b_1)}{\\partial b_1} \\Delta b_1 \\tag{115}\\\\\r  & = & \\sigma'(z_1) \\Delta b_1.\r\\tag{116}\\end{eqnarray}\rThat $\\sigma'(z_1)$ term should look familiar: it's the first term in\rour claimed expression for the gradient $\\partial C / \\partial b_1$.\rIntuitively, this term converts a change $\\Delta b_1$ in the bias into\ra change $\\Delta a_1$ in the output activation.  That change $\\Delta\ra_1$ in turn causes a change in the weighted input $z_2 = w_2 a_1 +\rb_2$ to the second hidden neuron:\\begin{eqnarray}\r  \\Delta z_2 & \\approx &\r  \\frac{\\partial z_2}{\\partial a_1} \\Delta a_1 \\tag{117}\\\\\r  & = & w_2 \\Delta a_1.\r\\tag{118}\\end{eqnarray}\rCombining our expressions for $\\Delta z_2$ and $\\Delta a_1$, we see\rhow the change in the bias $b_1$ propagates along the network to\raffect $z_2$:\\begin{eqnarray}\r\\Delta z_2 & \\approx & \\sigma'(z_1) w_2 \\Delta b_1.\r\\tag{119}\\end{eqnarray}\rAgain, that should look familiar: we've now got the first two terms in\rour claimed expression for the gradient $\\partial C / \\partial b_1$.",
            "We can keep going in this fashion, tracking the way changes propagate\rthrough the rest of the network.  At each neuron we pick up a\r$\\sigma'(z_j)$ term, and through each weight we pick up a $w_j$ term.\rThe end result is an expression relating the final change $\\Delta C$\rin cost to the initial change $\\Delta b_1$ in the bias:\\begin{eqnarray}\r\\Delta C & \\approx & \\sigma'(z_1) w_2 \\sigma'(z_2) \\ldots \\sigma'(z_4) \r\\frac{\\partial C}{\\partial a_4} \\Delta b_1.\r\\tag{120}\\end{eqnarray}\rDividing by $\\Delta b_1$ we do indeed get the desired expression for\rthe gradient:\\begin{eqnarray}\r\\frac{\\partial C}{\\partial b_1} = \\sigma'(z_1) w_2 \\sigma'(z_2) \\ldots\r\\sigma'(z_4) \\frac{\\partial C}{\\partial a_4}.\r\\tag{121}\\end{eqnarray}",
            "",
            "Why the vanishing gradient problem occurs:To understand why\rthe vanishing gradient problem occurs, let's explicitly write out the\rentire expression for the gradient:\\begin{eqnarray}\r\\frac{\\partial C}{\\partial b_1} = \\sigma'(z_1) \\, w_2 \\sigma'(z_2) \\,\r w_3 \\sigma'(z_3) \\, w_4 \\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}.\r\\tag{122}\\end{eqnarray}\rExcepting the very last term, this expression is a product of terms of\rthe form $w_j \\sigma'(z_j)$.  To understand how each of those terms\rbehave, let's look at a plot of the function $\\sigma'$:",
            "The derivative reaches a maximum at $\\sigma'(0) = 1/4$.  Now, if we\ruse ourstandard approachto initializing the weights in the network, then we'll choose the\rweights using a Gaussian with mean $0$ and standard deviation $1$.  So\rthe weights will usually satisfy $|w_j| < 1$.  Putting these\robservations together, we see that the terms $w_j \\sigma'(z_j)$ will\rusually satisfy $|w_j \\sigma'(z_j)| < 1/4$.  And when we take a\rproduct of many such terms, the product will tend to exponentially\rdecrease: the more terms, the smaller the product will be.  This is\rstarting to smell like a possible explanation for the vanishing\rgradient problem.",
            "To make this all a bit more explicit, let's compare the expression for\r$\\partial C / \\partial b_1$ to an expression for the gradient with\rrespect to a later bias, say $\\partial C / \\partial b_3$.  Of course,\rwe haven't explicitly worked out an expression for $\\partial C\r/ \\partial b_3$, but it follows the same pattern described above for\r$\\partial C / \\partial b_1$.  Here's the comparison of the two\rexpressions:The two expressions share many terms.  But the gradient $\\partial C\r/ \\partial b_1$ includes two extra terms each of the form $w_j\r\\sigma'(z_j)$. As we've seen, such terms are typically less than $1/4$\rin magnitude.  And so the gradient $\\partial C / \\partial b_1$ will\rusually be a factor of $16$ (or more) smaller than $\\partial C\r/ \\partial b_3$.  This is the essential origin of the vanishing\rgradient problem.",
            "Of course, this is an informal argument, not a rigorous proof that the\rvanishing gradient problem will occur.  There are several possible\rescape clauses.  In particular, we might wonder whether the weights\r$w_j$ could grow during training.  If they do, it's possible the terms\r$w_j \\sigma'(z_j)$ in the product will no longer satisfy $|w_j\r\\sigma'(z_j)| < 1/4$.  Indeed, if the terms get large enough -\rgreater than $1$ - then we will no longer have a vanishing gradient\rproblem.  Instead, the gradient will actually grow exponentially as we\rmove backward through the layers.  Instead of a vanishing gradient\rproblem, we'll have an exploding gradient problem.",
            "The exploding gradient problem:Let's look at an explicit\rexample where exploding gradients occur.  The example is somewhat\rcontrived: I'm going to fix parameters in the network in just the\rright way to ensure we get an exploding gradient.  But even though the\rexample is contrived, it has the virtue of firmly establishing that\rexploding gradients aren't merely a hypothetical possibility, they\rreally can happen.",
            "There are two steps to getting an exploding gradient.  First, we\rchoose all the weights in the network to be large, say $w_1 = w_2 =\rw_3 = w_4 = 100$.  Second, we'll choose the biases so that the\r$\\sigma'(z_j)$ terms are not too small.  That's actually pretty easy\rto do: all we need do is choose the biases to ensure that the weighted\rinput to each neuron is $z_j = 0$ (and so $\\sigma'(z_j) = 1/4$).  So,\rfor instance, we want $z_1 = w_1 a_0 + b_1 = 0$.  We can achieve this\rby setting $b_1 = -100 * a_0$.  We can use the same idea to select the\rother biases.  When we do this, we see that all the terms $w_j\r\\sigma'(z_j)$ are equal to $100 * \\frac{1}{4} = 25$.  With these\rchoices we get an exploding gradient.",
            "The unstable gradient problem:The fundamental problem here\risn't so much the vanishing gradient problem or the exploding gradient\rproblem.  It's that the gradient in early layers is the product of\rterms from all the later layers.  When there are many layers, that's\ran intrinsically unstable situation.  The only way all layers can\rlearn at close to the same speed is if all those products of terms\rcome close to balancing out.  Without some mechanism or underlying\rreason for that balancing to occur, it's highly unlikely to happen\rsimply by chance.  In short, the real problem here is that neural\rnetworks suffer from anunstable gradient problem.  As a\rresult, if we use standard gradient-based learning techniques,\rdifferent layers in the network will tend to learn at wildly different\rspeeds.",
            "ExerciseIn our discussion of the vanishing gradient problem, we made use\r  of the fact that $|\\sigma'(z)| < 1/4$.  Suppose we used a different\r  activation function, one whose derivative could be much larger.\r  Would that help us avoid the unstable gradient problem?",
            "The prevalence of the vanishing gradient problem:We've seen\rthat the gradient can either vanish or explode in the early layers of\ra deep network.  In fact, when using sigmoid neurons the gradient will\rusually vanish.  To see why, consider again the expression $|w\r\\sigma'(z)|$.  To avoid the vanishing gradient problem we need $|w\r\\sigma'(z)| \\geq 1$.  You might think this could happen easily if $w$\ris very large.  However, it's more difficult than it looks.  The\rreason is that the $\\sigma'(z)$ term also depends on $w$: $\\sigma'(z)\r= \\sigma'(wa +b)$, where $a$ is the input activation.  So when we make\r$w$ large, we need to be careful that we're not simultaneously making\r$\\sigma'(wa+b)$ small.  That turns out to be a considerable\rconstraint.  The reason is that when we make $w$ large we tend to make\r$wa+b$ very large.  Looking at the graph of $\\sigma'$ you can see that\rthis puts us off in the \"wings\" of the $\\sigma'$ function, where it\rtakes very small values.  The only way to avoid this is if the input\ractivation falls within a fairly narrow range of values (this\rqualitative explanation is made quantitative in the first problem\rbelow).  Sometimes that will chance to happen.  More often, though, it\rdoes not happen.  And so in the generic case we have vanishing\rgradients.",
            "ProblemsConsider the product $|w \\sigma'(wa+b)|$.  Suppose $|w\r  \\sigma'(wa+b)| \\geq 1$.  (1) Argue that this can only ever occur if\r  $|w| \\geq 4$.  (2) Supposing that $|w| \\geq 4$, consider the set of\r  input activations $a$ for which $|w \\sigma'(wa+b)| \\geq 1$.  Show\r  that the set of $a$ satisfying that constraint can range over an\r  interval no greater in width than\\begin{eqnarray}\r    \\frac{2}{|w|} \\ln\\left( \\frac{|w|(1+\\sqrt{1-4/|w|})}{2}-1\\right).\r  \\tag{123}\\end{eqnarray}\r  (3) Show numerically that the above expression bounding the width of\r  the range is greatest at $|w| \\approx 6.9$, where it takes a value\r  $\\approx 0.45$.  And so even given that everything lines up just\r  perfectly, we still have a fairly narrow range of input activations\r  which can avoid the vanishing gradient problem.",
            "Identity neuron:Consider a\r  neuron with a single input, $x$, a corresponding weight, $w_1$, a\r  bias $b$, and a weight $w_2$ on the output.  Show that by choosing\r  the weights and bias appropriately, we can ensure $w_2 \\sigma(w_1\r  x+b) \\approx x$ for $x \\in [0, 1]$.  Such a neuron can thus be used\r  as a kind of identity neuron, that is, a neuron whose output is the\r  same (up to rescaling by a weight factor) as its input.Hint:\r    It helps to rewrite $x = 1/2+\\Delta$, to assume $w_1$ is small,\r    and to use a Taylor series expansion in $w_1 \\Delta$.",
            "Unstable gradients in more complex networks"
        ],
        "Unstable gradients in more complex networks": [
            "We've been studying toy networks, with just one neuron in each hidden\rlayer.  What about more complex deep networks, with many neurons in\reach hidden layer?",
            "",
            "In fact, much the same behaviour occurs in such networks.  In the\rearlier chapter on backpropagation we saw that the gradient in the\r$l$th layer of an $L$ layer networkis given by:",
            "\\begin{eqnarray}\r  \\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\Sigma'(z^{l+1}) (w^{l+2})^T \\ldots\r  \\Sigma'(z^L) \\nabla_a C\r\\tag{124}\\end{eqnarray}",
            "Here, $\\Sigma'(z^l)$ is a diagonal matrix whose entries are the\r$\\sigma'(z)$ values for the weighted inputs to the $l$th layer.  The\r$w^l$ are the weight matrices for the different layers.  And $\\nabla_a\rC$ is the vector of partial derivatives of $C$ with respect to the\routput activations.",
            "This is a much more complicated expression than in the single-neuron\rcase.  Still, if you look closely, the essential form is very similar,\rwith lots of pairs of the form $(w^j)^T \\Sigma'(z^j)$.  What's more,\rthe matrices $\\Sigma'(z^j)$ have small entries on the diagonal, none\rlarger than $\\frac{1}{4}$. Provided the weight matrices $w^j$ aren't\rtoo large, each additional term $(w^j)^T \\Sigma'(z^l)$ tends to make\rthe gradient vector smaller, leading to a vanishing gradient.  More\rgenerally, the large number of terms in the product tends to lead to\ran unstable gradient, just as in our earlier example.  In practice,\rempirically it is typically found in sigmoid networks that gradients\rvanish exponentially quickly in earlier layers.  As a result, learning\rslows down in those layers.  This slowdown isn't merely an accident or\ran inconvenience: it's a fundamental consequence of the approach we're\rtaking to learning.",
            "Other obstacles to deep learning"
        ],
        "Other obstacles to deep learning": [
            "In this chapter we've focused on vanishing gradients - and, more\rgenerally, unstable gradients - as an obstacle to deep learning.  In\rfact, unstable gradients are just one obstacle to deep learning,\ralbeit an important fundamental obstacle.  Much ongoing research aims\rto better understand the challenges that can occur when training deep\rnetworks.  I won't comprehensively summarize that work here, but just\rwant to briefly mention a couple of papers, to give you the flavor of\rsome of the questions people are asking.",
            "As a first example, in 2010 Glorot and\rBengio**Understanding\r    the difficulty of training deep feedforward neural networks, by\r  Xavier Glorot and Yoshua Bengio (2010). See also the earlier\r  discussion of the use of sigmoids inEfficient\r    BackProp, by Yann LeCun, Léon Bottou,\r  Genevieve Orr and Klaus-Robert Müller (1998).found evidence suggesting that the use of sigmoid activation functions\rcan cause problems training deep networks.  In particular, they found\revidence that the use of sigmoids will cause the activations in the\rfinal hidden layer to saturate near $0$ early in training,\rsubstantially slowing down learning.  They suggested some alternative\ractivation functions, which appear not to suffer as much from this\rsaturation problem.",
            "As a second example, in 2013 Sutskever, Martens, Dahl and\rHinton**On\r    the importance of initialization and momentum in deep learning,\r  by Ilya Sutskever, James Martens, George Dahl and Geoffrey Hinton\r  (2013).studied the impact on deep learning of both the random\rweight initialization and the momentum schedule in momentum-based\rstochastic gradient descent.  In both cases, making good choices made\ra substantial difference in the ability to train deep networks.",
            "These examples suggest that \"What makes deep networks hard to\rtrain?\" is a complex question.  In this chapter, we've focused on the\rinstabilities associated to gradient-based learning in deep networks.\rThe results in the last two paragraphs suggest that there is also a\rrole played by the choice of activation function, the way weights are\rinitialized, and even details of how learning by gradient descent is\rimplemented.  And, of course, choice of network architecture and other\rhyper-parameters is also important. Thus, many factors can play a role\rin making deep networks hard to train, and understanding all those\rfactors is still a subject of ongoing research.  This all seems rather\rdownbeat and pessimism-inducing.  But the good news is that in the\rnext chapter we'll turn that around, and develop several approaches to\rdeep learning that to some extent manage to overcome or route around\rall these challenges.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ]
    },
    "CHAPTER 6": {
        "Introducing convolutional networks": [
            "In earlier chapters, we taught our neural networks to do a pretty good\rjob recognizing images of handwritten digits:",
            "",
            "We did this using networks in which adjacent network layers are fully\rconnected to one another.  That is, every neuron in the network is\rconnected to every neuron in adjacent layers:",
            "",
            "In particular, for each pixel in the input image, we encoded the\rpixel's intensity as the value for a corresponding neuron in the input\rlayer.  For the $28 \\times 28$ pixel images we've been using, this\rmeans our network has $784$ ($= 28 \\times 28$) input neurons.  We then\rtrained the network's weights and biases so that the network's output\rwould - we hope! - correctly identify the input image: '0', '1',\r'2', ..., '8', or '9'.",
            "Our earlier networks work pretty well: we'veobtained a classification accuracy better\r  than 98 percent, using training and test data from theMNIST handwritten\r  digit data set.  But upon reflection, it's strange to use networks\rwith fully-connected layers to classify images.  The reason is that\rsuch a network architecture does not take into account the spatial\rstructure of the images.  For instance, it treats input pixels which\rare far apart and close together on exactly the same footing.  Such\rconcepts of spatial structure must instead be inferred from the\rtraining data.  But what if, instead of starting with a network\rarchitecture which istabula rasa, we used an architecture\rwhich tries to take advantage of the spatial structure?  In this\rsection I describeconvolutional neural networks**The\r  origins of convolutional neural networks go back to the 1970s.  But\r  the seminal paper establishing the modern subject of convolutional\r  networks was a 1998 paper,\"Gradient-based\r    learning applied to document recognition\", by Yann LeCun,\r  Léon Bottou, Yoshua Bengio, and Patrick Haffner.\r  LeCun has since made an interestingremarkon the terminology for convolutional nets: \"The [biological] neural\r  inspiration in models like convolutional nets is very\r  tenuous. That's why I call them 'convolutional nets' not\r  'convolutional neural nets', and why we call the nodes 'units' and\r  not 'neurons' \".  Despite this remark, convolutional nets use many\r  of the same ideas as the neural networks we've studied up to now:\r  ideas such as backpropagation, gradient descent, regularization,\r  non-linear activation functions, and so on.  And so we will follow\r  common practice, and consider them a type of neural network.  I will\r  use the terms \"convolutional neural network\" and \"convolutional\r  net(work)\" interchangeably.  I will also use the terms\r  \"[artificial] neuron\" and \"unit\" interchangeably..  These\rnetworks use a special architecture which is particularly well-adapted\rto classify images.  Using this architecture makes convolutional\rnetworks fast to train.  This, in turn, helps us train deep,\rmany-layer networks, which are very good at classifying images.\rToday, deep convolutional networks or some close variant are used in\rmost neural networks for image recognition.",
            "Convolutional neural networks use three basic ideas:local\r  receptive fields,shared weights, andpooling.  Let's\rlook at each of these ideas in turn.",
            "Local receptive fields:In the fully-connected layers shown\rearlier, the inputs were depicted as a vertical line of neurons.  In a\rconvolutional net, it'll help to think instead of the inputs as a $28\r\\times 28$ square of neurons, whose values correspond to the $28\r\\times 28$ pixel intensities we're using as inputs:",
            "",
            "As per usual, we'll connect the input pixels to a layer of hidden\rneurons.  But we won't connect every input pixel to every hidden\rneuron.  Instead, we only make connections in small, localized regions\rof the input image.",
            "To be more precise, each neuron in the first hidden layer will be\rconnected to a small region of the input neurons, say, for example, a\r$5 \\times 5$ region, corresponding to $25$ input pixels.  So, for a\rparticular hidden neuron, we might have connections that look like\rthis:",
            "That region in the input image is called thelocal receptive\r  fieldfor the hidden neuron.  It's a little window on the input\rpixels.  Each connection learns a weight.  And the hidden neuron\rlearns an overall bias as well.  You can think of that particular\rhidden neuron as learning to analyze its particular local receptive\rfield.",
            "We then slide the local receptive field across the entire input image.\rFor each local receptive field, there is a different hidden neuron in\rthe first hidden layer.  To illustrate this concretely, let's start\rwith a local receptive field in the top-left corner:",
            "Then we slide the local receptive field over by one pixel to the right\r(i.e., by one neuron), to connect to a second hidden neuron:",
            "",
            "And so on, building up the first hidden layer.  Note that if we have a\r$28 \\times 28$ input image, and $5 \\times 5$ local receptive fields,\rthen there will be $24 \\times 24$ neurons in the hidden layer.  This\ris because we can only move the local receptive field $23$ neurons\racross (or $23$ neurons down), before colliding with the right-hand\rside (or bottom) of the input image.",
            "I've shown the local receptive field being moved by one pixel at a\rtime.  In fact, sometimes a differentstride lengthis used.\rFor instance, we might move the local receptive field $2$ pixels to\rthe right (or down), in which case we'd say a stride length of $2$ is\rused.  In this chapter we'll mostly stick with stride length $1$, but\rit's worth knowing that people sometimes experiment with different\rstride lengths**As was done in earlier chapters, if we're\r  interested in trying different stride lengths then we can use\r  validation data to pick out the stride length which gives the best\r  performance.  For more details, see theearlier\r    discussionof how to choose hyper-parameters in a neural network.\r  The same approach may also be used to choose the size of the local\r  receptive field - there is, of course, nothing special about using\r  a $5 \\times 5$ local receptive field.  In general, larger local\r  receptive fields tend to be helpful when the input images are\r  significantly larger than the $28 \\times 28$ pixel MNIST images..",
            "Shared weights and biases:I've said that each hidden neuron\rhas a bias and $5 \\times 5$ weights connected to its local receptive\rfield.  What I did not yet mention is that we're going to use thesameweights and bias for each of the $24 \\times 24$ hidden\rneurons.  In other words, for the $j, k$th hidden neuron, the output\ris:\\begin{eqnarray} \r  \\sigma\\left(b + \\sum_{l=0}^4 \\sum_{m=0}^4  w_{l,m} a_{j+l, k+m} \\right).\r\\tag{125}\\end{eqnarray}\rHere, $\\sigma$ is the neural activation function - perhaps thesigmoid functionwe used in\rearlier chapters.  $b$ is the shared value for the bias.  $w_{l,m}$ is\ra $5 \\times 5$ array of shared weights.  And, finally, we use $a_{x,\r  y}$ to denote the input activation at position $x, y$.",
            "This means that all the neurons in the first hidden layer detect\rexactly the same feature**I haven't precisely defined the\r  notion of a feature.  Informally, think of the feature detected by a\r  hidden neuron as the kind of input pattern that will cause the\r  neuron to activate: it might be an edge in the image, for instance,\r  or maybe some other type of shape., just at different locations in\rthe input image.  To see why this makes sense, suppose the weights and\rbias are such that the hidden neuron can pick out, say, a vertical\redge in a particular local receptive field.  That ability is also\rlikely to be useful at other places in the image.  And so it is useful\rto apply the same feature detector everywhere in the image.  To put it\rin slightly more abstract terms, convolutional networks are well\radapted to the translation invariance of images: move a picture of a\rcat (say) a little ways, and it's still an image of a cat**In\r  fact, for the MNIST digit classification problem we've been\r  studying, the images are centered and size-normalized.  So MNIST has\r  less translation invariance than images found \"in the wild\", so to\r  speak.  Still, features like edges and corners are likely to be\r  useful across much of the input space..",
            "For this reason, we sometimes call the map from the input layer to the\rhidden layer afeature map.  We call the weights defining the\rfeature map theshared weights.  And we call the bias defining\rthe feature map in this way theshared bias.  The shared\rweights and bias are often said to define akernelorfilter.  In the literature, people sometimes use these terms in\rslightly different ways, and for that reason I'm not going to be more\rprecise; rather, in a moment, we'll look at some concrete examples.",
            "",
            "The network structure I've described so far can detect just a single\rkind of localized feature.  To do image recognition we'll need more\rthan one feature map.  And so a complete convolutional layer consists\rof several different feature maps:",
            "In the example shown, there are $3$ feature maps.  Each feature map is\rdefined by a set of $5 \\times 5$ shared weights, and a single shared\rbias.  The result is that the network can detect $3$ different kinds\rof features, with each feature being detectable across the entire\rimage.",
            "",
            "I've shown just $3$ feature maps, to keep the diagram above simple.\rHowever, in practice convolutional networks may use more (and perhaps\rmany more) feature maps.  One of the early convolutional networks,\rLeNet-5, used $6$ feature maps, each associated to a $5 \\times 5$\rlocal receptive field, to recognize MNIST digits.  So the example\rillustrated above is actually pretty close to LeNet-5.  In the\rexamples we develop later in the chapter we'll use convolutional\rlayers with $20$ and $40$ feature maps.  Let's take a quick peek at\rsome of the features which are learned**The feature maps\r  illustrated come from the final convolutional network we train, seehere.:",
            "",
            "The $20$ images correspond to $20$ different feature maps (or filters,\ror kernels).  Each map is represented as a $5 \\times 5$ block image,\rcorresponding to the $5 \\times 5$ weights in the local receptive\rfield.  Whiter blocks mean a smaller (typically, more negative)\rweight, so the feature map responds less to corresponding input\rpixels.  Darker blocks mean a larger weight, so the feature map\rresponds more to the corresponding input pixels.  Very roughly\rspeaking, the images above show the type of features the convolutional\rlayer responds to.",
            "So what can we conclude from these feature maps?  It's clear there is\rspatial structure here beyond what we'd expect at random: many of the\rfeatures have clear sub-regions of light and dark.  That shows our\rnetwork really is learning things related to the spatial structure.\rHowever, beyond that, it's difficult to see what these feature\rdetectors are learning.  Certainly, we're not learning (say) theGabor filterswhich\rhave been used in many traditional approaches to image recognition.\rIn fact, there's now a lot of work on better understanding the\rfeatures learnt by convolutional networks.  If you're interested in\rfollowing up on that work, I suggest starting with the paperVisualizing and Understanding\r  Convolutional Networksby Matthew Zeiler and Rob Fergus (2013).",
            "",
            "A big advantage of sharing weights and biases is that it greatly\rreduces the number of parameters involved in a convolutional network.\rFor each feature map we need $25 = 5 \\times 5$ shared weights, plus a\rsingle shared bias. So each feature map requires $26$ parameters.  If\rwe have $20$ feature maps that's a total of $20 \\times 26 = 520$\rparameters defining the convolutional layer.  By comparison, suppose\rwe had a fully connected first layer, with $784 = 28 \\times 28$ input\rneurons, and a relatively modest $30$ hidden neurons, as we used in\rmany of the examples earlier in the book.  That's a total of $784\r\\times 30$ weights, plus an extra $30$ biases, for a total of $23,550$\rparameters.  In other words, the fully-connected layer would have more\rthan $40$ times as many parameters as the convolutional layer.",
            "Of course, we can't really do a direct comparison between the number\rof parameters, since the two models are different in essential ways.\rBut, intuitively, it seems likely that the use of translation\rinvariance by the convolutional layer will reduce the number of\rparameters it needs to get the same performance as the fully-connected\rmodel.  That, in turn, will result in faster training for the\rconvolutional model, and, ultimately, will help us build deep networks\rusing convolutional layers.",
            "",
            "Incidentally, the nameconvolutionalcomes from the fact that\rthe operation in Equation(125)\\begin{eqnarray} \r  \\sigma\\left(b + \\sum_{l=0}^4 \\sum_{m=0}^4  w_{l,m} a_{j+l, k+m} \\right) \\nonumber\\end{eqnarray}is sometimes known as aconvolution.  A little more precisely, people sometimes write\rthat equation as $a^1 = \\sigma(b + w * a^0)$, where $a^1$ denotes the\rset of output activations from one feature map, $a^0$ is the set of\rinput activations, and $*$ is called a convolution operation.  We're\rnot going to make any deep use of the mathematics of convolutions, so\ryou don't need to worry too much about this connection.  But it's\rworth at least knowing where the name comes from.",
            "",
            "",
            "",
            "",
            "Pooling layers:In addition to the convolutional layers just\rdescribed, convolutional neural networks also containpooling\r  layers.  Pooling layers are usually used immediately after\rconvolutional layers.  What the pooling layers do is simplify the\rinformation in the output from the convolutional layer.",
            "",
            "In detail, a pooling layer takes each feature map**The\r  nomenclature is being used loosely here.  In particular, I'm using\r  \"feature map\" to mean not the function computed by the\r  convolutional layer, but rather the activation of the hidden neurons\r  output from the layer.  This kind of mild abuse of nomenclature is\r  pretty common in the research literature.output from the\rconvolutional layer and prepares a condensed feature map.  For\rinstance, each unit in the pooling layer may summarize a region of\r(say) $2 \\times 2$ neurons in the previous layer.  As a concrete\rexample, one common procedure for pooling is known asmax-pooling.  In max-pooling, a pooling unit simply outputs the\rmaximum activation in the $2 \\times 2$ input region, as illustrated in\rthe following diagram:",
            "",
            "Note that since we have $24 \\times 24$ neurons output from the\rconvolutional layer, after pooling we have $12 \\times 12$ neurons.",
            "As mentioned above, the convolutional layer usually involves more than\ra  single feature  map.   We  apply max-pooling  to  each feature  map\rseparately.   So  if  there  were three  feature  maps,  the  combined\rconvolutional and max-pooling layers would look like:",
            "",
            "We can think of max-pooling as a way for the network to ask whether a\rgiven feature is found anywhere in a region of the image.  It then\rthrows away the exact positional information.  The intuition is that\ronce a feature has been found, its exact location isn't as important\ras its rough location relative to other features.  A big benefit is\rthat there are many fewer pooled features, and so this helps reduce\rthe number of parameters needed in later layers.",
            "",
            "Max-pooling isn't the only technique used for pooling.  Another common\rapproach is known asL2 pooling.  Here, instead of taking the\rmaximum activation of a $2 \\times 2$ region of neurons, we take the\rsquare root of the sum of the squares of the activations in the $2\r\\times 2$ region.  While the details are different, the intuition is\rsimilar to max-pooling: L2 pooling is a way of condensing information\rfrom the convolutional layer.  In practice, both techniques have been\rwidely used.  And sometimes people use other types of pooling\roperation.  If you're really trying to optimize performance, you may\ruse validation data to compare several different approaches to\rpooling, and choose the approach which works best.  But we're not\rgoing to worry about that kind of detailed optimization.",
            "",
            "Putting it all together:We can now put all these ideas\rtogether to form a complete convolutional neural network.  It's\rsimilar to the architecture we were just looking at, but has the\raddition of a layer of $10$ output neurons, corresponding to the $10$\rpossible values for MNIST digits ('0', '1', '2',etc):",
            "",
            "The network begins with $28 \\times 28$ input neurons, which are used\rto encode the pixel intensities for the MNIST image.  This is then\rfollowed by a convolutional layer using a $5 \\times 5$ local receptive\rfield and $3$ feature maps.  The result is a layer of $3 \\times 24\r\\times 24$ hidden feature neurons.  The next step is a max-pooling\rlayer, applied to $2 \\times 2$ regions, across each of the $3$ feature\rmaps.  The result is a layer of $3 \\times 12 \\times 12$ hidden feature\rneurons.",
            "The final layer of connections in the network is a fully-connected\rlayer.  That is, this layer connectseveryneuron from the\rmax-pooled layer to every one of the $10$ output neurons.  This\rfully-connected architecture is the same as we used in earlier\rchapters.  Note, however, that in the diagram above, I've used a\rsingle arrow, for simplicity, rather than showing all the connections.\rOf course, you can easily imagine the connections.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "This convolutional architecture is quite different to the\rarchitectures used in earlier chapters.  But the overall picture is\rsimilar: a network made of many simple units, whose behaviors are\rdetermined by their weights and biases.  And the overall goal is still\rthe same: to use training data to train the network's weights and\rbiases so that the network does a good job classifying input digits.",
            "In particular, just as earlier in the book, we will train our network\rusing stochastic gradient descent and backpropagation.  This mostly\rproceeds in exactly the same way as in earlier chapters.  However, we\rdo need to make a few modifications to the backpropagation procedure.\rThe reason is that our earlierderivation of\r  backpropagationwas for networks with fully-connected layers.\rFortunately, it's straightforward to modify the derivation for\rconvolutional and max-pooling layers.  If you'd like to understand the\rdetails, then I invite you to work through the following problem.  Be\rwarned that the problem will take some time to work through, unless\ryou've really internalized theearlier derivation of\r  backpropagation(in which case it's easy).",
            "ProblemBackpropagation in a convolutional networkThe core equations\r  of backpropagation in a network with fully-connected layers\r  are(BP1)\\begin{eqnarray} \r  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) \\nonumber\\end{eqnarray}-(BP4)\\begin{eqnarray}  \r  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\nonumber\\end{eqnarray}(link).  Suppose we have a\r  network containing a convolutional layer, a max-pooling layer, and a\r  fully-connected output layer, as in the network discussed above.\r  How are the equations of backpropagation modified?",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "Convolutional neural networks in practice"
        ],
        "Convolutional neural networks in practice": [
            "We've now seen the core ideas behind convolutional neural networks.\rLet's look at how they work in practice, by implementing some\rconvolutional networks, and applying them to the MNIST digit\rclassification problem.  The program we'll use to do this is callednetwork3.py, and it's an improved version of the programsnetwork.pyandnetwork2.pydeveloped in earlier\rchapters**Note also thatnetwork3.pyincorporates ideas\r  from the Theano library's documentation on convolutional neural nets\r  (notably the implementation ofLeNet-5), from\r  Misha Denil'simplementation of dropout,\r  and fromChris Olah..  If you wish\rto follow along, the code is availableon\r  GitHub.  Note that we'll work through the code fornetwork3.pyitself in the next section.  In this section, we'll\rusenetwork3.pyas a library to build convolutional networks.",
            "",
            "The programsnetwork.pyandnetwork2.pywere implemented\rusing Python and the matrix library Numpy.  Those programs worked from\rfirst principles, and got right down into the details of\rbackpropagation, stochastic gradient descent, and so on.  But now that\rwe understand those details, fornetwork3.pywe're going to use\ra machine learning library known asTheano**SeeTheano:\r    A CPU and GPU Math Expression Compiler in Python, by James\r  Bergstra, Olivier Breuleux, Frederic Bastien, Pascal Lamblin, Ravzan\r  Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley,\r  and Yoshua Bengio (2010).  Theano is also the basis for the popularPylearn2andKerasneural networks libraries. Other\r  popular neural nets libraries at the time of this writing includeCaffeandTorch..  Using Theano makes it easy to\rimplement backpropagation for convolutional neural networks, since it\rautomatically computes all the mappings involved.  Theano is also\rquite a bit faster than our earlier code (which was written to be easy\rto understand, not fast), and this makes it practical to train more\rcomplex networks.  In particular, one great feature of Theano is that\rit can run code on either a CPU or, if available, a GPU.  Running on a\rGPU provides a substantial speedup and, again, helps make it practical\rto train more complex networks.",
            "",
            "If you wish to follow along, then you'll need to get Theano running on\ryour system.  To install Theano, follow the instructions at the\rproject'shomepage.\rThe examples which follow were run using Theano 0.6**As I\r  release this chapter, the current version of Theano has changed to\r  version 0.7.  I've actually rerun the examples under Theano 0.7 and\r  get extremely similar results to those reported in the text..  Some\rwere run under Mac OS X Yosemite, with no GPU.  Some were run on\rUbuntu 14.04, with an NVIDIA GPU.  And some of the experiments were run\runder both.  To getnetwork3.pyrunning you'll need to set theGPUflag to eitherTrueorFalse(as appropriate)\rin thenetwork3.pysource.  Beyond that, to get Theano up and\rrunning on a GPU you may findthe\r  instructions herehelpful.  There are also tutorials on the web,\reasily found using Google, which can help you get things working.  If\ryou don't have a GPU available locally, then you may wish to look intoAmazon Web ServicesEC2 G2 spot instances.  Note that even with a GPU the code will take\rsome time to execute.  Many of the experiments take from minutes to\rhours to run.  On a CPU it may take days to run the most complex of\rthe experiments.  As in earlier chapters, I suggest setting things\rrunning, and continuing to read, occasionally coming back to check the\routput from the code.  If you're using a CPU, you may wish to reduce\rthe number of training epochs for the more complex experiments, or\rperhaps omit them entirely.",
            "To get a baseline, we'll start with a shallow architecture using just\ra single hidden layer, containing $100$ hidden neurons.  We'll train\rfor $60$ epochs, using a learning rate of $\\eta = 0.1$, a mini-batch\rsize of $10$, and no regularization.  Here we go**Code for the\r  experiments in this section may be foundin\r    this script.  Note that the code in the script simply duplicates\r  and parallels the discussion in this section.Note also that\r  throughout the section I've explicitly specified the number of\r  training epochs.  I've done this for clarity about how we're\r  training.  In practice, it's worth usingearly stopping, that is,\r  tracking accuracy on the validation set, and stopping training when\r  we are confident the validation accuracy has stopped improving.:",
            ">>>importnetwork3>>>fromnetwork3importNetwork>>>fromnetwork3importConvPoolLayer,FullyConnectedLayer,SoftmaxLayer>>>training_data,validation_data,test_data=network3.load_data_shared()>>>mini_batch_size=10>>>net=Network([FullyConnectedLayer(n_in=784,n_out=100),SoftmaxLayer(n_in=100,n_out=10)],mini_batch_size)>>>net.SGD(training_data,60,mini_batch_size,0.1,validation_data,test_data)",
            "",
            "I obtained a best classification accuracy of $97.80$ percent. This is\rthe classification accuracy on thetest_data, evaluated at the\rtraining epoch where we get the best classification accuracy on thevalidation_data. Using the validation data to decide when to\revaluate the test accuracy helps avoid overfitting to the test data\r(see thisearlier\r  discussionof the use of validation data).  We will follow this\rpractice below.  Your results may vary slightly, since the network's\rweights and biases are randomly initialized**In fact, in this\r  experiment I actually did three separate runs training a network\r  with this architecture.  I then reported the test accuracy which\r  corresponded to the best validation accuracy from any of the three\r  runs.  Using multiple runs helps reduce variation in results, which\r  is useful when comparing many architectures, as we are doing.  I've\r  followed this procedure below, except where noted.  In practice, it\r  made little difference to the results obtained..",
            "This $97.80$ percent accuracy is close to the $98.04$ percent accuracy\robtained back inChapter 3,\rusing a similar network architecture and learning hyper-parameters.\rIn particular, both examples used a shallow network, with a single\rhidden layer containing $100$ hidden neurons.  Both also trained for\r$60$ epochs, used a mini-batch size of $10$, and a learning rate of\r$\\eta = 0.1$.",
            "There were, however, two differences in the earlier network.  First,\rweregularizedthe earlier network, to help reduce the effects of\roverfitting. Regularizing the current network does improve the\raccuracies, but the gain is only small, and so we'll hold off worrying\rabout regularization until later.  Second, while the final layer in\rthe earlier network used sigmoid activations and the cross-entropy\rcost function, the current network uses a softmax final layer, and the\rlog-likelihood cost function.  Asexplainedin Chapter 3 this isn't a big\rchange.  I haven't made this switch for any particularly deep reason\r- mostly, I've done it because softmax plus log-likelihood cost is\rmore common in modern image classification networks.",
            "Can we do better than these results using a deeper network\rarchitecture?",
            "Let's begin by inserting a convolutional layer, right at the beginning\rof the network.  We'll use $5$ by $5$ local receptive fields, a stride\rlength of $1$, and $20$ feature maps.  We'll also insert a max-pooling\rlayer, which combines the features using $2$ by $2$ pooling windows.\rSo the overall network architecture looks much like the architecture\rdiscussed in the last section, but with an extra fully-connected\rlayer:",
            "",
            "In this architecture, we can think of the convolutional and pooling\rlayers as learning about local spatial structure in the input training\rimage, while the later, fully-connected layer learns at a more\rabstract level, integrating global information from across the entire\rimage.  This is a common pattern in convolutional neural networks.",
            "Let's train such a network, and see how it performs**I've\r  continued to use a mini-batch size of $10$ here.  In fact, as wediscussed earlierit may be\r  possible to speed up training using larger mini-batches.  I've\r  continued to use the same mini-batch size mostly for consistency\r  with the experiments in earlier chapters.:",
            ">>>net=Network([ConvPoolLayer(image_shape=(mini_batch_size,1,28,28),filter_shape=(20,1,5,5),poolsize=(2,2)),FullyConnectedLayer(n_in=20*12*12,n_out=100),SoftmaxLayer(n_in=100,n_out=10)],mini_batch_size)>>>net.SGD(training_data,60,mini_batch_size,0.1,validation_data,test_data)",
            "",
            "",
            "That gets us to $98.78$ percent accuracy, which is a considerable\rimprovement over any of our previous results.  Indeed, we've reduced\rour error rate by better than a third, which is a great improvement.",
            "In specifying the network structure, I've treated the convolutional\rand pooling layers as a single layer.  Whether they're regarded as\rseparate layers or as a single layer is to some extent a matter of\rtaste.network3.pytreats them as a single layer because it\rmakes the code fornetwork3.pya little more compact.  However,\rit is easy to modifynetwork3.pyso the layers can be specified\rseparately, if desired.",
            "ExerciseWhat classification accuracy do you get if you omit the\r  fully-connected layer, and just use the convolutional-pooling layer\r  and softmax layer?  Does the inclusion of the fully-connected layer\r  help?",
            "Can we improve on the $98.78$ percent classification accuracy?",
            "Let's try inserting a second convolutional-pooling layer.  We'll make\rthe insertion between the existing convolutional-pooling layer and the\rfully-connected hidden layer.  Again, we'll use a $5 \\times 5$ local\rreceptive field, and pool over $2 \\times 2$ regions.  Let's see what\rhappens when we train using similar hyper-parameters to before:",
            ">>>net=Network([ConvPoolLayer(image_shape=(mini_batch_size,1,28,28),filter_shape=(20,1,5,5),poolsize=(2,2)),ConvPoolLayer(image_shape=(mini_batch_size,20,12,12),filter_shape=(40,20,5,5),poolsize=(2,2)),FullyConnectedLayer(n_in=40*4*4,n_out=100),SoftmaxLayer(n_in=100,n_out=10)],mini_batch_size)>>>net.SGD(training_data,60,mini_batch_size,0.1,validation_data,test_data)",
            "",
            "",
            "Once again, we get an improvement: we're now at $99.06$ percent\rclassification accuracy!",
            "There's two natural questions to ask at this point.  The first\rquestion is: what does it even mean to apply a second\rconvolutional-pooling layer?  In fact, you can think of the second\rconvolutional-pooling layer as having as input $12 \\times 12$\r\"images\", whose \"pixels\" represent the presence (or absence) of\rparticular localized features in the original input image.  So you can\rthink of this layer as having as input a version of the original input\rimage.  That version is abstracted and condensed, but still has a lot\rof spatial structure, and so it makes sense to use a second\rconvolutional-pooling layer.",
            "That's a satisfying point of view, but gives rise to a second\rquestion.  The output from the previous layer involves $20$ separate\rfeature maps, and so there are $20 \\times 12 \\times 12$ inputs to the\rsecond convolutional-pooling layer.  It's as though we've got $20$\rseparate images input to the convolutional-pooling layer, not a single\rimage, as was the case for the first convolutional-pooling layer.  How\rshould neurons in the second convolutional-pooling layer respond to\rthese multiple input images?  In fact, we'll allow each neuron in this\rlayer to learn fromall$20 \\times 5 \\times 5$ input neurons in\rits local receptive field.  More informally: the feature detectors in\rthe second convolutional-pooling layer have access toallthe\rfeatures from the previous layer, but only within their particular\rlocal receptive field**This issue would have arisen in the\r  first layer if the input images were in color.  In that case we'd\r  have 3 input features for each pixel, corresponding to red, green\r  and blue channels in the input image.  So we'd allow the feature\r  detectors to have access to all color information, but only within a\r  given local receptive field..",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "ProblemUsing the tanh activation functionSeveral times earlier in the\r  book I've mentioned arguments that thetanh\r    functionmay be a better activation function than the sigmoid\r  function.  We've never acted on those suggestions, since we were\r  already making plenty of progress with the sigmoid.  But now let's\r  try some experiments with tanh as our activation function.  Try\r  training the network with tanh activations in the convolutional and\r  fully-connected layers**Note that you can passactivation_fn=tanhas a parameter to theConvPoolLayerandFullyConnectedLayerclasses..\r  Begin with the same hyper-parameters as for the sigmoid network, but\r  train for $20$ epochs instead of $60$.  How well does your network\r  perform?  What if you continue out to $60$ epochs?  Try plotting the\r  per-epoch validation accuracies for both tanh- and sigmoid-based\r  networks, all the way out to $60$ epochs.  If your results are\r  similar to mine, you'll find the tanh networks train a little\r  faster, but the final accuracies are very similar.  Can you explain\r  why the tanh network might train faster?  Can you get a similar\r  training speed with the sigmoid, perhaps by changing the learning\r  rate, or doing some rescaling**You may perhaps find\r    inspiration in recalling that $\\sigma(z) = (1+\\tanh(z/2))/2$.?\r  Try a half-dozen iterations on the learning hyper-parameters or\r  network architecture, searching for ways that tanh may be superior\r  to the sigmoid.Note: This is an open-ended problem.\r    Personally, I did not find much advantage in switching to tanh,\r    although I haven't experimented exhaustively, and perhaps you may\r    find a way.  In any case, in a moment we will find an advantage in\r    switching to the rectified linear activation function, and so we\r    won't go any deeper into the use of tanh.",
            "",
            "Using rectified linear units:The network we've developed at\rthis point is actually a variant of one of the networks used in the\rseminal 1998\rpaper**\"Gradient-based\r    learning applied to document recognition\", by Yann LeCun,\r  Léon Bottou, Yoshua Bengio, and Patrick Haffner\r  (1998).  There are many differences of detail, but broadly speaking\r  our network is quite similar to the networks described in the\r  paper.introducing the MNIST problem, a network known as LeNet-5.\rIt's a good foundation for further experimentation, and for building\rup understanding and intuition.  In particular, there are many ways we\rcan vary the network in an attempt to improve our results.",
            "As a beginning, let's change our neurons so that instead of using a\rsigmoid activation function, we userectified\r  linear units.  That is, we'll use the activation function $f(z)\r\\equiv \\max(0, z)$.  We'll train for $60$ epochs, with a learning rate\rof $\\eta = 0.03$.  I also found that it helps a little to use somel2\r  regularization, with regularization parameter $\\lambda = 0.1$:",
            ">>>fromnetwork3importReLU>>>net=Network([ConvPoolLayer(image_shape=(mini_batch_size,1,28,28),filter_shape=(20,1,5,5),poolsize=(2,2),activation_fn=ReLU),ConvPoolLayer(image_shape=(mini_batch_size,20,12,12),filter_shape=(40,20,5,5),poolsize=(2,2),activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4,n_out=100,activation_fn=ReLU),SoftmaxLayer(n_in=100,n_out=10)],mini_batch_size)>>>net.SGD(training_data,60,mini_batch_size,0.03,validation_data,test_data,lmbda=0.1)",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "I obtained a classification accuracy of $99.23$ percent.  It's a\rmodest improvement over the sigmoid results ($99.06$).  However,\racross all my experiments I found that networks based on rectified\rlinear units consistently outperformed networks based on sigmoid\ractivation functions.  There appears to be a real gain in moving to\rrectified linear units for this problem.",
            "What makes the rectified linear activation function better than the\rsigmoid or tanh functions?  At present, we have a poor understanding\rof the answer to this question.  Indeed, rectified linear units have\ronly begun to be widely used in the past few years.  The reason for\rthat recent adoption is empirical: a few people tried rectified linear\runits, often on the basis of hunches or heuristic arguments**A\r  common justification is that $\\max(0, z)$ doesn't saturate in the\r  limit of large $z$, unlike sigmoid neurons, and this helps rectified\r  linear units continue learning.  The argument is fine, as far it\r  goes, but it's hardly a detailed justification, more of a just-so\r  story.  Note that we discussed the problems with saturation back inChapter 2..  They got good\rresults classifying benchmark data sets, and the practice has spread.\rIn an ideal world we'd have a theory telling us which activation\rfunction to pick for which application.  But at present we're a long\rway from such a world.  I should not be at all surprised if further\rmajor improvements can be obtained by an even better choice of\ractivation function.  And I also expect that in coming decades a\rpowerful theory of activation functions will be developed.  Today, we\rstill have to rely on poorly understood rules of thumb and experience.",
            "Expanding the training data:Another way we may hope to\rimprove our results is by algorithmically expanding the training data.\rA simple way of expanding the training data is to displace each\rtraining image by a single pixel, either up one pixel, down one pixel,\rleft one pixel, or right one pixel.  We can do this by running the\rprogramexpand_mnist.pyfrom the shell prompt**The code\r  forexpand_mnist.pyis availablehere.:",
            "$ python expand_mnist.py",
            "Running this program takes the $50,000$ MNIST training images, and\rprepares an expanded training set, with $250,000$ training images.  We\rcan then use those training images to train our network.  We'll use\rthe same network as above, with rectified linear units.  In my initial\rexperiments I reduced the number of training epochs - this made\rsense, since we're training with $5$ times as much data.  But, in\rfact, expanding the data turned out to considerably reduce the effect\rof overfitting.  And so, after some experimentation, I eventually went\rback to training for $60$ epochs.  In any case, let's train:",
            ">>>expanded_training_data,_,_=network3.load_data_shared(\"../data/mnist_expanded.pkl.gz\")>>>net=Network([ConvPoolLayer(image_shape=(mini_batch_size,1,28,28),filter_shape=(20,1,5,5),poolsize=(2,2),activation_fn=ReLU),ConvPoolLayer(image_shape=(mini_batch_size,20,12,12),filter_shape=(40,20,5,5),poolsize=(2,2),activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4,n_out=100,activation_fn=ReLU),SoftmaxLayer(n_in=100,n_out=10)],mini_batch_size)>>>net.SGD(expanded_training_data,60,mini_batch_size,0.03,validation_data,test_data,lmbda=0.1)",
            "",
            "Using the expanded training data I obtained a $99.37$ percent training\raccuracy.  So this almost trivial change gives a substantial\rimprovement in classification accuracy.  Indeed, as wediscussed\r  earlierthis idea of algorithmically expanding the data can be\rtaken further.  Just to remind you of the flavour of some of the\rresults in that earlier discussion: in 2003 Simard, Steinkraus and\rPlatt**Best\r    Practices for Convolutional Neural Networks Applied to Visual\r    Document Analysis, by Patrice Simard, Dave Steinkraus, and John\r  Platt (2003).improved their MNIST performance to $99.6$ percent\rusing a neural network otherwise very similar to ours, using two\rconvolutional-pooling layers, followed by a hidden fully-connected\rlayer with $100$ neurons.  There were a few differences of detail in\rtheir architecture - they didn't have the advantage of using\rrectified linear units, for instance - but the key to their improved\rperformance was expanding the training data.  They did this by\rrotating, translating, and skewing the MNIST training images.  They\ralso developed a process of \"elastic distortion\", a way of emulating\rthe random oscillations hand muscles undergo when a person is writing.\rBy combining all these processes they substantially increased the\reffective size of their training data, and that's how they achieved\r$99.6$ percent accuracy.",
            "ProblemThe idea of convolutional layers is to behave in an invariant\r  way across images.  It may seem surprising, then, that our network\r  can learn more when all we've done is translate the input data.  Can\r  you explain why this is actually quite reasonable?",
            "Inserting an extra fully-connected layer:Can we do even\rbetter?  One possibility is to use exactly the same procedure as\rabove, but to expand the size of the fully-connected layer.  I tried\rwith $300$ and $1,000$ neurons, obtaining results of $99.46$ and\r$99.43$ percent, respectively.  That's interesting, but not really a\rconvincing win over the earlier result ($99.37$ percent).",
            "What about adding an extra fully-connected layer?  Let's try inserting\ran extra fully-connected layer, so that we have two $100$-hidden\rneuron fully-connected layers:>>>net=Network([ConvPoolLayer(image_shape=(mini_batch_size,1,28,28),filter_shape=(20,1,5,5),poolsize=(2,2),activation_fn=ReLU),ConvPoolLayer(image_shape=(mini_batch_size,20,12,12),filter_shape=(40,20,5,5),poolsize=(2,2),activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4,n_out=100,activation_fn=ReLU),FullyConnectedLayer(n_in=100,n_out=100,activation_fn=ReLU),SoftmaxLayer(n_in=100,n_out=10)],mini_batch_size)>>>net.SGD(expanded_training_data,60,mini_batch_size,0.03,validation_data,test_data,lmbda=0.1)",
            "Doing this, I obtained a test accuracy of $99.43$ percent.  Again, the\rexpanded net isn't helping so much.  Running similar experiments with\rfully-connected layers containing $300$ and $1,000$ neurons yields\rresults of $99.48$ and $99.47$ percent.  That's encouraging, but still\rfalls short of a really decisive win.",
            "",
            "What's going on here?  Is it that the expanded or extra\rfully-connected layers really don't help with MNIST?  Or might it be\rthat our network has the capacity to do better, but we're going about\rlearning the wrong way?  For instance, maybe we could use stronger\rregularization techniques to reduce the tendency to overfit.  One\rpossibility is thedropouttechnique introduced back in Chapter 3.  Recall that the basic idea of\rdropout is to remove individual activations at random while training\rthe network.  This makes the model more robust to the loss of\rindividual pieces of evidence, and thus less likely to rely on\rparticular idiosyncracies of the training data.  Let's try applying\rdropout to the final fully-connected layers:",
            ">>>net=Network([ConvPoolLayer(image_shape=(mini_batch_size,1,28,28),filter_shape=(20,1,5,5),poolsize=(2,2),activation_fn=ReLU),ConvPoolLayer(image_shape=(mini_batch_size,20,12,12),filter_shape=(40,20,5,5),poolsize=(2,2),activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4,n_out=1000,activation_fn=ReLU,p_dropout=0.5),FullyConnectedLayer(n_in=1000,n_out=1000,activation_fn=ReLU,p_dropout=0.5),SoftmaxLayer(n_in=1000,n_out=10,p_dropout=0.5)],mini_batch_size)>>>net.SGD(expanded_training_data,40,mini_batch_size,0.03,validation_data,test_data)",
            "Using this, we obtain an accuracy of $99.60$ percent, which is a\rsubstantial improvement over our earlier results, especially our main\rbenchmark, the network with $100$ hidden neurons, where we achieved\r$99.37$ percent.",
            "There are two changes worth noting.",
            "First, I reduced the number of training epochs to $40$: dropout\rreduced overfitting, and so we learned faster.",
            "Second, the fully-connected hidden layers have $1,000$ neurons, not\rthe $100$ used earlier. Of course, dropout effectively omits many of\rthe neurons while training, so some expansion is to be expected.  In\rfact, I tried experiments with both $300$ and $1,000$ hidden neurons,\rand obtained (very slightly) better validation performance with\r$1,000$ hidden neurons.",
            "Using an ensemble of networks:An easy way to improve\rperformance still further is to create several neural networks, and\rthen get them to vote to determine the best classification.  Suppose,\rfor example, that we trained $5$ different neural networks using the\rprescription above, with each achieving accuracies near to $99.6$\rpercent.  Even though the networks would all have similar accuracies,\rthey might well make different errors, due to the different random\rinitializations.  It's plausible that taking a vote amongst our $5$\rnetworks might yield a classification better than any individual\rnetwork.",
            "This sounds too good to be true, but this kind of ensembling is a\rcommon trick with both neural networks and other machine learning\rtechniques.  And it does in fact yield further improvements: we end up\rwith $99.67$ percent accuracy.  In other words, our ensemble of\rnetworks classifies all but $33$ of the $10,000$ test images\rcorrectly.",
            "The remaining errors in the test set are shown below.  The label in\rthe top right is the correct classification, according to the MNIST\rdata, while in the bottom right is the label output by our ensemble of\rnets:",
            "",
            "It's worth looking through these in detail. The first two digits, a 6\rand a 5, are genuine errors by our ensemble.  However, they're also\runderstandable errors, the kind a human could plausibly make. That 6\rreally does look a lot like a 0, and the 5 looks a lot like a 3.  The\rthird image, supposedly an 8, actually looks to me more like a 9.  So\rI'm siding with the network ensemble here: I think it's done a better\rjob than whoever originally drew the digit.  On the other hand, the\rfourth image, the 6, really does seem to be classified badly by our\rnetworks.",
            "And so on.  In most cases our networks' choices seem at least\rplausible, and in some cases they've done a better job classifying\rthan the original person did writing the digit.  Overall, our networks\roffer exceptional performance, especially when you consider that they\rcorrectly classified 9,967 images which aren't shown.  In that\rcontext, the few clear errors here seem quite understandable.  Even a\rcareful human makes the occasional mistake.  And so I expect that only\ran extremely careful and methodical human would do much better.  Our\rnetwork is getting near to human performance.",
            "Why we only applied dropout to the fully-connected layers:If\ryou look carefully at the code above, you'll notice that we applied\rdropout only to the fully-connected section of the network, not to the\rconvolutional layers.  In principle we could apply a similar procedure\rto the convolutional layers.  But, in fact, there's no need: the\rconvolutional layers have considerable inbuilt resistance to\roverfitting.  The reason is that the shared weights mean that\rconvolutional filters are forced to learn from across the entire\rimage.  This makes them less likely to pick up on local idiosyncracies\rin the training data.  And so there is less need to apply other\rregularizers, such as dropout.",
            "Going further:It's possible to improve performance on MNIST\rstill further. Rodrigo Benenson has compiled aninformative\r  summary page, showing progress over the years, with links to\rpapers.  Many of these papers use deep convolutional networks along\rlines similar to the networks we've been using.  If you dig through\rthe papers you'll find many interesting techniques, and you may enjoy\rimplementing some of them.  If you do so it's wise to start\rimplementation with a simple network that can be trained quickly,\rwhich will help you more rapidly understand what is going on.",
            "For the most part, I won't try to survey this recent work.  But I\rcan't resist making one exception.  It's a 2010 paper by Cireșan,\rMeier, Gambardella, and\rSchmidhuber**Deep, Big,\r    Simple Neural Nets Excel on Handwritten Digit Recognition, by Dan\r  Claudiu Cireșan, Ueli Meier, Luca Maria Gambardella, and Jürgen\r  Schmidhuber (2010)..  What I like about this paper is how simple it\ris.  The network is a many-layer neural network, using only\rfully-connected layers (no convolutions).  Their most successful\rnetwork had hidden layers containing $2,500$, $2,000$, $1,500$,\r$1,000$, and $500$ neurons, respectively.  They used ideas similar to\rSimardet alto expand their training data.  But apart from\rthat, they used few other tricks, including no convolutional layers:\rit was a plain, vanilla network, of the kind that, with enough\rpatience, could have been trained in the 1980s (if the MNIST data set\rhad existed), given enough computing power.  They achieved a\rclassification accuracy of $99.65$ percent, more or less the same as\rours.  The key was to use a very large, very deep network, and to use\ra GPU to speed up training.  This let them train for many epochs.\rThey also took advantage of their long training times to gradually\rdecrease the learning rate from $10^{-3}$ to $10^{-6}$.  It's a fun\rexercise to try to match these results using an architecture like\rtheirs.",
            "Why are we able to train?We saw inthe\r  last chapterthat there are fundamental obstructions to training in\rdeep, many-layer neural networks.  In particular, we saw that the\rgradient tends to be quite unstable: as we move from the output layer\rto earlier layers the gradient tends to either vanish (the vanishing\rgradient problem) or explode (the exploding gradient problem).  Since\rthe gradient is the signal we use to train, this causes problems.",
            "How have we avoided those results?",
            "Of course, the answer is that we haven't avoided these results.\rInstead, we've done a few things that help us proceed anyway.  In\rparticular: (1) Using convolutional layers greatly reduces the number\rof parameters in those layers, making the learning problem much\reasier; (2) Using more powerful regularization techniques (notably\rdropout and convolutional layers) to reduce overfitting, which is\rotherwise more of a problem in more complex networks; (3) Using\rrectified linear units instead of sigmoid neurons, to speed up\rtraining - empirically, often by a factor of $3$-$5$; (4) Using GPUs\rand being willing to train for a long period of time.  In particular,\rin our final experiments we trained for $40$ epochs using a data set\r$5$ times larger than the raw MNIST training data.  Earlier in the\rbook we mostly trained for $30$ epochs using just the raw training\rdata.  Combining factors (3) and (4) it's as though we've trained a\rfactor perhaps $30$ times longer than before.",
            "Your response may be \"Is that it? Is that all we had to do to train\rdeep networks?  What's all the fuss about?\"",
            "Of course, we've used other ideas, too: making use of sufficiently\rlarge data sets (to help avoid overfitting); using the right cost\rfunction (toavoid a\r  learning slowdown); usinggood weight initializations(also to avoid a learning slowdown, due to neuron saturation);algorithmically\r  expanding the training data.  We discussed these and other ideas in\rearlier chapters, and have for the most part been able to reuse these\rideas with little comment in this chapter.",
            "With that said, this really is a rather simple set of ideas.  Simple,\rbut powerful, when used in concert.  Getting started with deep\rlearning has turned out to be pretty easy!",
            "How deep are these networks, anyway?Counting the\rconvolutional-pooling layers as single layers, our final architecture\rhas $4$ hidden layers.  Does such a network really deserve to be\rcalled adeepnetwork?  Of course, $4$ hidden layers is many\rmore than in the shallow networks we studied earlier.  Most of those\rnetworks only had a single hidden layer, or occasionally $2$ hidden\rlayers.  On the other hand, as of 2015 state-of-the-art deep networks\rsometimes have dozens of hidden layers.  I've occasionally heard\rpeople adopt a deeper-than-thou attitude, holding that if you're not\rkeeping-up-with-the-Joneses in terms of number of hidden layers, then\ryou're not really doing deep learning.  I'm not sympathetic to this\rattitude, in part because it makes the definition of deep learning\rinto something which depends upon the result-of-the-moment. The real\rbreakthrough in deep learning was to realize that it's practical to go\rbeyond the shallow $1$- and $2$-hidden layer networks that dominated\rwork until the mid-2000s.  That really was a significant breakthrough,\ropening up the exploration of much more expressive models.  But beyond\rthat, the number of layers is not of primary fundamental interest.\rRather, the use of deeper networks is a tool to use to help achieve\rother goals - like better classification accuracies.",
            "A word on procedure:In this section, we've smoothly moved\rfrom single hidden-layer shallow networks to many-layer convolutional\rnetworks.  It all seemed so easy!  We make a change and, for the\rmost part, we get an improvement.  If you start experimenting, I can\rguarantee things won't always be so smooth.  The reason is that I've\rpresented a cleaned-up narrative, omitting many experiments -\rincluding many failed experiments.  This cleaned-up narrative will\rhopefully help you get clear on the basic ideas.  But it also runs the\rrisk of conveying an incomplete impression.  Getting a good, working\rnetwork can involve a lot of trial and error, and occasional\rfrustration.  In practice, you should expect to engage in quite a bit\rof experimentation.  To speed that process up you may find it helpful\rto revisit Chapter 3's discussion ofhow\r  to choose a neural network's hyper-parameters, and perhaps also to\rlook at some of the further reading suggested in that section.",
            "",
            "",
            "",
            "The code for our convolutional networks"
        ],
        "The code for our convolutional networks": [
            "Alright, let's take a look at the code for our program,network3.py.  Structurally, it's similar tonetwork2.py,\rthe program we developed inChapter 3, although the\rdetails differ, due to the use of Theano.  We'll start by looking at\rtheFullyConnectedLayerclass, which is similar to the layers\rstudied earlier in the book.  Here's the code (discussion\rbelow)**Note added November 2016: several readers have noted\r  that in the line initializingself.w, I setscale=np.sqrt(1.0/n_out), when the arguments of Chapter 3\r  suggest a better initialization may bescale=np.sqrt(1.0/n_in).  This was simply a mistake on my\r  part.  In an ideal world I'd rerun all the examples in this chapter\r  with the correct code. Still, I've moved on to other projects, so am\r  going to let the error go.:",
            "classFullyConnectedLayer(object):def__init__(self,n_in,n_out,activation_fn=sigmoid,p_dropout=0.0):self.n_in=n_inself.n_out=n_outself.activation_fn=activation_fnself.p_dropout=p_dropout# Initialize weights and biasesself.w=theano.shared(np.asarray(np.random.normal(loc=0.0,scale=np.sqrt(1.0/n_out),size=(n_in,n_out)),dtype=theano.config.floatX),name='w',borrow=True)self.b=theano.shared(np.asarray(np.random.normal(loc=0.0,scale=1.0,size=(n_out,)),dtype=theano.config.floatX),name='b',borrow=True)self.params=[self.w,self.b]defset_inpt(self,inpt,inpt_dropout,mini_batch_size):self.inpt=inpt.reshape((mini_batch_size,self.n_in))self.output=self.activation_fn((1-self.p_dropout)*T.dot(self.inpt,self.w)+self.b)self.y_out=T.argmax(self.output,axis=1)self.inpt_dropout=dropout_layer(inpt_dropout.reshape((mini_batch_size,self.n_in)),self.p_dropout)self.output_dropout=self.activation_fn(T.dot(self.inpt_dropout,self.w)+self.b)defaccuracy(self,y):\"Return the accuracy for the mini-batch.\"returnT.mean(T.eq(y,self.y_out))",
            "Much of the__init__method is self-explanatory, but a few\rremarks may help clarify the code.  As per usual, we randomly\rinitialize the weights and biases as normal random variables with\rsuitable standard deviations.  The lines doing this look a little\rforbidding.  However, most of the complication is just loading the\rweights and biases into what Theano calls shared variables.  This\rensures that these variables can be processed on the GPU, if one is\ravailable.  We won't get too much into the details of this.  If you're\rinterested, you can dig into theTheano\r  documentation.  Note also that this weight and bias initialization\ris designed for the sigmoid activation function (asdiscussed earlier).\rIdeally, we'd initialize the weights and biases somewhat differently\rfor activation functions such as the tanh and rectified linear\rfunction.  This is discussed further in problems below.  The__init__method finishes withself.params = [self.w, self.b].  This is a handy way to bundle\rup all the learnable parameters associated to the layer.  Later on,\rtheNetwork.SGDmethod will useparamsattributes to\rfigure out what variables in aNetworkinstance can learn.",
            "Theset_inptmethod is used to set the input to the layer, and\rto compute the corresponding output.  I use the nameinptrather thaninputbecauseinputis a built-in function\rin Python, and messing with built-ins tends to cause unpredictable\rbehavior and difficult-to-diagnose bugs.  Note that we actually set\rthe input in two separate ways: asself.inptandself.inpt_dropout.  This is done because during training we may\rwant to use dropout.  If that's the case then we want to remove a\rfractionself.p_dropoutof the neurons.  That's what the\rfunctiondropout_layerin the second-last line of theset_inptmethod is doing.  Soself.inpt_dropoutandself.output_dropoutare used during training, whileself.inptandself.outputare used for all other\rpurposes, e.g., evaluating accuracy on the validation and test data.",
            "TheConvPoolLayerandSoftmaxLayerclass definitions are\rsimilar toFullyConnectedLayer.  Indeed, they're so close that\rI won't excerpt the code here.  If you're interested you can look at\rthe full listing fornetwork3.py, later in this section.",
            "However, a couple of minor differences of detail are worth mentioning.\rMost obviously, in bothConvPoolLayerandSoftmaxLayerwe compute the output activations in the way appropriate to that layer\rtype.  Fortunately, Theano makes that easy, providing built-in\roperations to compute convolutions, max-pooling, and the softmax\rfunction.",
            "Less obviously, when weintroduced the\r  softmax layer, we never discussed how to initialize the weights and\rbiases.  Elsewhere we've argued that for sigmoid layers we should\rinitialize the weights using suitably parameterized normal random\rvariables.  But that heuristic argument was specific to sigmoid\rneurons (and, with some amendment, to tanh neurons).  However, there's\rno particular reason the argument should apply to softmax layers.  So\rthere's noa priorireason to apply that initialization again.\rRather than do that, I shall initialize all the weights and biases to\rbe $0$.  This is a ratherad hocprocedure, but works well\renough in practice.",
            "Okay, we've looked at all the layer classes.  What about theNetworkclass?  Let's start by looking at the__init__method:",
            "classNetwork(object):def__init__(self,layers,mini_batch_size):\"\"\"Takes a list of `layers`, describing the network architecture, anda value for the `mini_batch_size` to be used during trainingby stochastic gradient descent.\"\"\"self.layers=layersself.mini_batch_size=mini_batch_sizeself.params=[paramforlayerinself.layersforparaminlayer.params]self.x=T.matrix(\"x\")self.y=T.ivector(\"y\")init_layer=self.layers[0]init_layer.set_inpt(self.x,self.x,self.mini_batch_size)forjinxrange(1,len(self.layers)):prev_layer,layer=self.layers[j-1],self.layers[j]layer.set_inpt(prev_layer.output,prev_layer.output_dropout,self.mini_batch_size)self.output=self.layers[-1].outputself.output_dropout=self.layers[-1].output_dropout",
            "Most of this is self-explanatory, or nearly so.  The lineself.params = [param for layer in ...]bundles up the\rparameters for each layer into a single list.  As anticipated above,\rtheNetwork.SGDmethod will useself.paramsto figure\rout what variables in theNetworkcan learn.  The linesself.x = T.matrix(\"x\")andself.y = T.ivector(\"y\")define Theano symbolic variables namedxandy.  These\rwill be used to represent the input and desired output from the\rnetwork.",
            "Now, this isn't a Theano tutorial, and so we won't get too deeply into\rwhat it means that these are symbolic variables**TheTheano\r    documentationprovides a good introduction to Theano.  And if you\r  get stuck, you may find it helpful to look at one of the other\r  tutorials available online.  For instance,this\r    tutorialcovers many basics..  But the rough idea is that these\rrepresent mathematical variables,notexplicit values.  We can\rdo all the usual things one would do with such variables: add,\rsubtract, and multiply them, apply functions, and so on.  Indeed,\rTheano provides many ways of manipulating such symbolic variables,\rdoing things like convolutions, max-pooling, and so on.  But the big\rwin is the ability to do fast symbolic differentiation, using a very\rgeneral form of the backpropagation algorithm.  This is extremely\ruseful for applying stochastic gradient descent to a wide variety of\rnetwork architectures.  In particular, the next few lines of code\rdefine symbolic outputs from the network.  We start by setting the\rinput to the initial layer, with the line",
            "init_layer.set_inpt(self.x,self.x,self.mini_batch_size)",
            "Note that the inputs are set one mini-batch at a time, which is why\rthe mini-batch size is there.  Note also that we pass the inputself.xin twice: this is because we may use the network in two\rdifferent ways (with or without dropout).  Theforloop then\rpropagates the symbolic variableself.xforward through the\rlayers of theNetwork. This allows us to define the finaloutputandoutput_dropoutattributes, which symbolically\rrepresent the output from theNetwork.",
            "Now that we've understood how aNetworkis initialized, let's\rlook at how it is trained, using theSGDmethod.  The code\rlooks lengthy, but its structure is actually rather simple.\rExplanatory comments after the code.",
            "defSGD(self,training_data,epochs,mini_batch_size,eta,validation_data,test_data,lmbda=0.0):\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"training_x,training_y=training_datavalidation_x,validation_y=validation_datatest_x,test_y=test_data# compute number of minibatches for training, validation and testingnum_training_batches=size(training_data)/mini_batch_sizenum_validation_batches=size(validation_data)/mini_batch_sizenum_test_batches=size(test_data)/mini_batch_size# define the (regularized) cost function, symbolic gradients, and updatesl2_norm_squared=sum([(layer.w**2).sum()forlayerinself.layers])cost=self.layers[-1].cost(self)+\\0.5*lmbda*l2_norm_squared/num_training_batchesgrads=T.grad(cost,self.params)updates=[(param,param-eta*grad)forparam,gradinzip(self.params,grads)]# define functions to train a mini-batch, and to compute the# accuracy in validation and test mini-batches.i=T.lscalar()# mini-batch indextrain_mb=theano.function([i],cost,updates=updates,givens={self.x:training_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size],self.y:training_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})validate_mb_accuracy=theano.function([i],self.layers[-1].accuracy(self.y),givens={self.x:validation_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size],self.y:validation_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})test_mb_accuracy=theano.function([i],self.layers[-1].accuracy(self.y),givens={self.x:test_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size],self.y:test_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})self.test_mb_predictions=theano.function([i],self.layers[-1].y_out,givens={self.x:test_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})# Do the actual trainingbest_validation_accuracy=0.0forepochinxrange(epochs):forminibatch_indexinxrange(num_training_batches):iteration=num_training_batches*epoch+minibatch_indexifiterationprint(\"Training mini-batch number {0}\".format(iteration))cost_ij=train_mb(minibatch_index)if(iteration+1)validation_accuracy=np.mean([validate_mb_accuracy(j)forjinxrange(num_validation_batches)])print(\"Epoch {0}: validation accuracy {1:.2epoch,validation_accuracy))ifvalidation_accuracy>=best_validation_accuracy:print(\"This is the best validation accuracy to date.\")best_validation_accuracy=validation_accuracybest_iteration=iterationiftest_data:test_accuracy=np.mean([test_mb_accuracy(j)forjinxrange(num_test_batches)])print('The corresponding test accuracy is {0:.2test_accuracy))print(\"Finished training network.\")print(\"Best validation accuracy of {0:.2best_validation_accuracy,best_iteration))print(\"Corresponding test accuracy of {0:.2",
            "The first few lines are straightforward, separating the datasets into\r$x$ and $y$ components, and computing the number of mini-batches used\rin each dataset.  The next few lines are more interesting, and show\rsome of what makes Theano fun to work with.  Let's explicitly excerpt\rthe lines here:",
            "# define the (regularized) cost function, symbolic gradients, and updatesl2_norm_squared=sum([(layer.w**2).sum()forlayerinself.layers])cost=self.layers[-1].cost(self)+\\0.5*lmbda*l2_norm_squared/num_training_batchesgrads=T.grad(cost,self.params)updates=[(param,param-eta*grad)forparam,gradinzip(self.params,grads)]",
            "In these lines we symbolically set up the regularized log-likelihood\rcost function, compute the corresponding derivatives in the gradient\rfunction, as well as the corresponding parameter updates.  Theano lets\rus achieve all of this in just these few lines.  The only thing hidden\ris that computing thecostinvolves a call to thecostmethod for the output layer; that code is elsewhere innetwork3.py.  But that code is short and simple, anyway.  With\rall these things defined, the stage is set to define thetrain_mbfunction, a Theano symbolic function which uses theupdatesto update theNetworkparameters, given a\rmini-batch index.  Similarly,validate_mb_accuracyandtest_mb_accuracycompute the accuracy of theNetworkon\rany given mini-batch of validation or test data.  By averaging over\rthese functions, we will be able to compute accuracies on the entire\rvalidation and test data sets.",
            "The remainder of theSGDmethod is self-explanatory - we\rsimply iterate over the epochs, repeatedly training the network on\rmini-batches of training data, and computing the validation and test\raccuracies.",
            "Okay, we've now understood the most important pieces of code innetwork3.py.  Let's take a brief look at the entire program.\rYou don't need to read through this in detail, but you may enjoy\rglancing over it, and perhaps diving down into any pieces that strike\ryour fancy.  The best way to really understand it is, of course, by\rmodifying it, adding extra features, or refactoring anything you think\rcould be done more elegantly.  After the code, there are some problems\rwhich contain a few starter suggestions for things to do.  Here's the\rcode**Using Theano on a GPU can be a little tricky.  In\r  particular, it's easy to make the mistake of pulling data off the\r  GPU, which can slow things down a lot.  I've tried to avoid this.\r  With that said, this code can certainly be sped up quite a bit\r  further with careful optimization of Theano's configuration.  See\r  the Theano documentation for more details.:",
            "\"\"\"network3.py~~~~~~~~~~~~~~A Theano-based program for training and running simple neuralnetworks.Supports several layer types (fully connected, convolutional, maxpooling, softmax), and activation functions (sigmoid, tanh, andrectified linear units, with more easily added).When run on a CPU, this program is much faster than network.py andnetwork2.py.  However, unlike network.py and network2.py it can alsobe run on a GPU, which makes it faster still.Because the code is based on Theano, the code is different in manyways from network.py and network2.py.  However, where possible I havetried to maintain consistency with the earlier programs.  Inparticular, the API is similar to network2.py.  Note that I havefocused on making the code simple, easily readable, and easilymodifiable.  It is not optimized, and omits many desirable features.This program incorporates ideas from the Theano documentation onconvolutional neural nets (notably,http://deeplearning.net/tutorial/lenet.html ), from Misha Denil'simplementation of dropout (https://github.com/mdenil/dropout ), andfrom Chris Olah (http://colah.github.io ).Written for Theano 0.6 and 0.7, needs some changes for more recentversions of Theano.\"\"\"#### Libraries# Standard libraryimportcPickleimportgzip# Third-party librariesimportnumpyasnpimporttheanoimporttheano.tensorasTfromtheano.tensor.nnetimportconvfromtheano.tensor.nnetimportsoftmaxfromtheano.tensorimportshared_randomstreamsfromtheano.tensor.signalimportdownsample# Activation functions for neuronsdeflinear(z):returnzdefReLU(z):returnT.maximum(0.0,z)fromtheano.tensor.nnetimportsigmoidfromtheano.tensorimporttanh#### ConstantsGPU=TrueifGPU:print\"Trying to run under a GPU.  If this is not desired, then modify \"+\\\"network3.py\\nto set the GPU flag to False.\"try:theano.config.device='gpu'except:pass# it's already settheano.config.floatX='float32'else:print\"Running with a CPU.  If this is not desired, then the modify \"+\\\"network3.py to set\\nthe GPU flag to True.\"#### Load the MNIST datadefload_data_shared(filename=\"../data/mnist.pkl.gz\"):f=gzip.open(filename,'rb')training_data,validation_data,test_data=cPickle.load(f)f.close()defshared(data):\"\"\"Place the data into shared variables.  This allows Theano to copythe data to the GPU, if one is available.\"\"\"shared_x=theano.shared(np.asarray(data[0],dtype=theano.config.floatX),borrow=True)shared_y=theano.shared(np.asarray(data[1],dtype=theano.config.floatX),borrow=True)returnshared_x,T.cast(shared_y,\"int32\")return[shared(training_data),shared(validation_data),shared(test_data)]#### Main class used to construct and train networksclassNetwork(object):def__init__(self,layers,mini_batch_size):\"\"\"Takes a list of `layers`, describing the network architecture, anda value for the `mini_batch_size` to be used during trainingby stochastic gradient descent.\"\"\"self.layers=layersself.mini_batch_size=mini_batch_sizeself.params=[paramforlayerinself.layersforparaminlayer.params]self.x=T.matrix(\"x\")self.y=T.ivector(\"y\")init_layer=self.layers[0]init_layer.set_inpt(self.x,self.x,self.mini_batch_size)forjinxrange(1,len(self.layers)):prev_layer,layer=self.layers[j-1],self.layers[j]layer.set_inpt(prev_layer.output,prev_layer.output_dropout,self.mini_batch_size)self.output=self.layers[-1].outputself.output_dropout=self.layers[-1].output_dropoutdefSGD(self,training_data,epochs,mini_batch_size,eta,validation_data,test_data,lmbda=0.0):\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"training_x,training_y=training_datavalidation_x,validation_y=validation_datatest_x,test_y=test_data# compute number of minibatches for training, validation and testingnum_training_batches=size(training_data)/mini_batch_sizenum_validation_batches=size(validation_data)/mini_batch_sizenum_test_batches=size(test_data)/mini_batch_size# define the (regularized) cost function, symbolic gradients, and updatesl2_norm_squared=sum([(layer.w**2).sum()forlayerinself.layers])cost=self.layers[-1].cost(self)+\\0.5*lmbda*l2_norm_squared/num_training_batchesgrads=T.grad(cost,self.params)updates=[(param,param-eta*grad)forparam,gradinzip(self.params,grads)]# define functions to train a mini-batch, and to compute the# accuracy in validation and test mini-batches.i=T.lscalar()# mini-batch indextrain_mb=theano.function([i],cost,updates=updates,givens={self.x:training_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size],self.y:training_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})validate_mb_accuracy=theano.function([i],self.layers[-1].accuracy(self.y),givens={self.x:validation_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size],self.y:validation_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})test_mb_accuracy=theano.function([i],self.layers[-1].accuracy(self.y),givens={self.x:test_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size],self.y:test_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})self.test_mb_predictions=theano.function([i],self.layers[-1].y_out,givens={self.x:test_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size]})# Do the actual trainingbest_validation_accuracy=0.0forepochinxrange(epochs):forminibatch_indexinxrange(num_training_batches):iteration=num_training_batches*epoch+minibatch_indexifiteration%1000==0:print(\"Training mini-batch number {0}\".format(iteration))cost_ij=train_mb(minibatch_index)if(iteration+1)%num_training_batches==0:validation_accuracy=np.mean([validate_mb_accuracy(j)forjinxrange(num_validation_batches)])print(\"Epoch {0}: validation accuracy {1:.2%}\".format(epoch,validation_accuracy))ifvalidation_accuracy>=best_validation_accuracy:print(\"This is the best validation accuracy to date.\")best_validation_accuracy=validation_accuracybest_iteration=iterationiftest_data:test_accuracy=np.mean([test_mb_accuracy(j)forjinxrange(num_test_batches)])print('The corresponding test accuracy is {0:.2%}'.format(test_accuracy))print(\"Finished training network.\")print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(best_validation_accuracy,best_iteration))print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))#### Define layer typesclassConvPoolLayer(object):\"\"\"Used to create a combination of a convolutional and a max-poolinglayer.  A more sophisticated implementation would separate thetwo, but for our purposes we'll always use them together, and itsimplifies the code, so it makes sense to combine them.\"\"\"def__init__(self,filter_shape,image_shape,poolsize=(2,2),activation_fn=sigmoid):\"\"\"`filter_shape` is a tuple of length 4, whose entries are the numberof filters, the number of input feature maps, the filter height, and thefilter width.`image_shape` is a tuple of length 4, whose entries are themini-batch size, the number of input feature maps, the imageheight, and the image width.`poolsize` is a tuple of length 2, whose entries are the y andx pooling sizes.\"\"\"self.filter_shape=filter_shapeself.image_shape=image_shapeself.poolsize=poolsizeself.activation_fn=activation_fn# initialize weights and biasesn_out=(filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))self.w=theano.shared(np.asarray(np.random.normal(loc=0,scale=np.sqrt(1.0/n_out),size=filter_shape),dtype=theano.config.floatX),borrow=True)self.b=theano.shared(np.asarray(np.random.normal(loc=0,scale=1.0,size=(filter_shape[0],)),dtype=theano.config.floatX),borrow=True)self.params=[self.w,self.b]defset_inpt(self,inpt,inpt_dropout,mini_batch_size):self.inpt=inpt.reshape(self.image_shape)conv_out=conv.conv2d(input=self.inpt,filters=self.w,filter_shape=self.filter_shape,image_shape=self.image_shape)pooled_out=downsample.max_pool_2d(input=conv_out,ds=self.poolsize,ignore_border=True)self.output=self.activation_fn(pooled_out+self.b.dimshuffle('x',0,'x','x'))self.output_dropout=self.output# no dropout in the convolutional layersclassFullyConnectedLayer(object):def__init__(self,n_in,n_out,activation_fn=sigmoid,p_dropout=0.0):self.n_in=n_inself.n_out=n_outself.activation_fn=activation_fnself.p_dropout=p_dropout# Initialize weights and biasesself.w=theano.shared(np.asarray(np.random.normal(loc=0.0,scale=np.sqrt(1.0/n_out),size=(n_in,n_out)),dtype=theano.config.floatX),name='w',borrow=True)self.b=theano.shared(np.asarray(np.random.normal(loc=0.0,scale=1.0,size=(n_out,)),dtype=theano.config.floatX),name='b',borrow=True)self.params=[self.w,self.b]defset_inpt(self,inpt,inpt_dropout,mini_batch_size):self.inpt=inpt.reshape((mini_batch_size,self.n_in))self.output=self.activation_fn((1-self.p_dropout)*T.dot(self.inpt,self.w)+self.b)self.y_out=T.argmax(self.output,axis=1)self.inpt_dropout=dropout_layer(inpt_dropout.reshape((mini_batch_size,self.n_in)),self.p_dropout)self.output_dropout=self.activation_fn(T.dot(self.inpt_dropout,self.w)+self.b)defaccuracy(self,y):\"Return the accuracy for the mini-batch.\"returnT.mean(T.eq(y,self.y_out))classSoftmaxLayer(object):def__init__(self,n_in,n_out,p_dropout=0.0):self.n_in=n_inself.n_out=n_outself.p_dropout=p_dropout# Initialize weights and biasesself.w=theano.shared(np.zeros((n_in,n_out),dtype=theano.config.floatX),name='w',borrow=True)self.b=theano.shared(np.zeros((n_out,),dtype=theano.config.floatX),name='b',borrow=True)self.params=[self.w,self.b]defset_inpt(self,inpt,inpt_dropout,mini_batch_size):self.inpt=inpt.reshape((mini_batch_size,self.n_in))self.output=softmax((1-self.p_dropout)*T.dot(self.inpt,self.w)+self.b)self.y_out=T.argmax(self.output,axis=1)self.inpt_dropout=dropout_layer(inpt_dropout.reshape((mini_batch_size,self.n_in)),self.p_dropout)self.output_dropout=softmax(T.dot(self.inpt_dropout,self.w)+self.b)defcost(self,net):\"Return the log-likelihood cost.\"return-T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]),net.y])defaccuracy(self,y):\"Return the accuracy for the mini-batch.\"returnT.mean(T.eq(y,self.y_out))#### Miscellaneadefsize(data):\"Return the size of the dataset `data`.\"returndata[0].get_value(borrow=True).shape[0]defdropout_layer(layer,p_dropout):srng=shared_randomstreams.RandomStreams(np.random.RandomState(0).randint(999999))mask=srng.binomial(n=1,p=1-p_dropout,size=layer.shape)returnlayer*T.cast(mask,theano.config.floatX)",
            "Problems",
            "At present, theSGDmethod requires the user to manually\r  choose the number of epochs to train for.  Earlier in the book we\r  discussed an automated way of selecting the number of epochs to\r  train for, known asearly\r    stopping.  Modifynetwork3.pyto implement early stopping.",
            "Add aNetworkmethod to return the accuracy on an\r  arbitrary data set.",
            "Modify theSGDmethod to allow the learning rate $\\eta$\r  to be a function of the epoch number.Hint: After working on\r    this problem for a while, you may find it useful to see the\r    discussion atthis\r      link.",
            "Earlier in the chapter I described a technique for expanding the\r  training data by applying (small) rotations, skewing, and\r  translation.  Modifynetwork3.pyto incorporate all these\r  techniques.Note: Unless you have a tremendous amount of\r    memory, it is not practical to explicitly generate the entire\r    expanded data set.  So you should consider alternate approaches.",
            "Add the ability to load and save networks tonetwork3.py.",
            "A shortcoming of the current code is that it provides few\r  diagnostic tools.  Can you think of any diagnostics to add that\r  would make it easier to understand to what extent a network is\r  overfitting?  Add them.",
            "We've used the same initialization procedure for rectified\r  linear units as for sigmoid (and tanh) neurons.  Ourargument for that\r    initializationwas specific to the sigmoid function.  Consider a\r  network made entirely of rectified linear units (including outputs).\r  Show that rescaling all the weights in the network by a constant\r  factor $c > 0$ simply rescales the outputs by a factor $c^{L-1}$,\r  where $L$ is the number of layers.  How does this change if the\r  final layer is a softmax?  What do you think of using the sigmoid\r  initialization procedure for the rectified linear units?  Can you\r  think of a better initialization procedure?Note: This is a\r    very open-ended problem, not something with a simple\r    self-contained answer.  Still, considering the problem will help\r    you better understand networks containing rectified linear units.",
            "Ouranalysisof the unstable gradient problem was for sigmoid neurons. How does\r  the analysis change for networks made up of rectified linear units?\r  Can you think of a good way of modifying such a network so it\r  doesn't suffer from the unstable gradient problem?Note: The\r    word good in the second part of this makes the problem a research\r    problem.  It's actually easy to think of ways of making such\r    modifications.  But I haven't investigated in enough depth to know\r    of a really good technique.",
            "Recent progress in image recognition"
        ],
        "Recent progress in image recognition": [
            "In 1998, the year MNIST was introduced, it took weeks to train a\rstate-of-the-art workstation to achieve accuracies substantially worse\rthan those we can achieve using a GPU and less than an hour of\rtraining. Thus, MNIST is no longer a problem that pushes the limits of\ravailable technique; rather, the speed of training means that it is a\rproblem good for teaching and learning purposes.  Meanwhile, the focus\rof research has moved on, and modern work involves much more\rchallenging image recognition problems.  In this section, I briefly\rdescribe some recent work on image recognition using neural networks.",
            "The section is different to most of the book.  Through the book I've\rfocused on ideas likely to be of lasting interest - ideas such as\rbackpropagation, regularization, and convolutional networks.  I've\rtried to avoid results which are fashionable as I write, but whose\rlong-term value is unknown. In science, such results are more often\rthan not ephemera which fade and have little lasting impact.  Given\rthis, a skeptic might say: \"well, surely the recent progress in image\rrecognition is an example of such ephemera?  In another two or three\ryears, things will have moved on.  So surely these results are only of\rinterest to a few specialists who want to compete at the absolute\rfrontier?  Why bother discussing it?\"",
            "Such a skeptic is right that some of the finer details of recent\rpapers will gradually diminish in perceived importance.  With that\rsaid, the past few years have seen extraordinary improvements using\rdeep nets to attack extremely difficult image recognition tasks.\rImagine a historian of science writing about computer vision in the\ryear 2100.  They will identify the years 2011 to 2015 (and probably a\rfew years beyond) as a time of huge breakthroughs, driven by deep\rconvolutional nets.  That doesn't mean deep convolutional nets will\rstill be used in 2100, much less detailed ideas such as dropout,\rrectified linear units, and so on.  But it does mean that an important\rtransition is taking place, right now, in the history of ideas.  It's\ra bit like watching the discovery of the atom, or the invention of\rantibiotics: invention and discovery on a historic scale.  And so\rwhile we won't dig down deep into details, it's worth getting some\ridea of the exciting discoveries currently being made.",
            "The 2012 LRMD paper:Let me start with a 2012\rpaper**Building\r    high-level features using large scale unsupervised learning, by\r  Quoc Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai\r  Chen, Greg Corrado, Jeff Dean, and Andrew Ng (2012).  Note that the\r  detailed architecture of the network used in the paper differed in\r  many details from the deep convolutional networks we've been\r  studying.  Broadly speaking, however, LRMD is based on many similar\r  ideas.from a group of researchers from Stanford and Google.  I'll\rrefer to this paper as LRMD, after the last names of the first four\rauthors. LRMD used a neural network to classify images fromImageNet, a very challenging image\rrecognition problem.  The 2011 ImageNet data that they used included\r16 million full color images, in 20 thousand categories.  The images\rwere crawled from the open net, and classified by workers from\rAmazon's Mechanical Turk service.  Here's a few ImageNet\rimages**These are from the 2014 dataset, which is somewhat\r  changed from 2011.  Qualitatively, however, the dataset is extremely\r  similar.  Details about ImageNet are available in the original\r  ImageNet paper,ImageNet:\r    a large-scale hierarchical image database, by Jia Deng, Wei Dong,\r  Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei (2009).:",
            "",
            "These are, respectively, in the categories for beading plane, brown\rroot rot fungus, scalded milk, and the common roundworm.  If you're\rlooking for a challenge, I encourage you to visit ImageNet's list ofhand tools,\rwhich distinguishes between beading planes, block planes, chamfer\rplanes, and about a dozen other types of plane, amongst other\rcategories.  I don't know about you, but I cannot confidently\rdistinguish between all these tool types.  This is obviously a much\rmore challenging image recognition task than MNIST!  LRMD's network\robtained a respectable $15.8$ percent accuracy for correctly\rclassifying ImageNet images.  That may not sound impressive, but it\rwas a huge improvement over the previous best result of $9.3$ percent\raccuracy.  That jump suggested that neural networks might offer a\rpowerful approach to very challenging image recognition tasks, such as\rImageNet.",
            "The 2012 KSH paper:The work of LRMD was followed by a 2012\rpaper of Krizhevsky, Sutskever and Hinton\r(KSH)**ImageNet\r    classification with deep convolutional neural networks, by Alex\r  Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton (2012).. KSH\rtrained and tested a deep convolutional neural network using a\rrestricted subset of the ImageNet data. The subset they used came from\ra popular machine learning competition - the ImageNet Large-Scale\rVisual Recognition Challenge (ILSVRC).  Using a competition dataset\rgave them a good way of comparing their approach to other leading\rtechniques.  The ILSVRC-2012 training set contained about 1.2 million\rImageNet images, drawn from 1,000 categories.  The validation and test\rsets contained 50,000 and 150,000 images, respectively, drawn from the\rsame 1,000 categories.",
            "One difficulty in running the ILSVRC competition is that many ImageNet\rimages contain multiple objects.  Suppose an image shows a labrador\rretriever chasing a soccer ball.  The so-called \"correct\" ImageNet\rclassification of the image might be as a labrador retriever.  Should\ran algorithm be penalized if it labels the image as a soccer ball?\rBecause of this ambiguity, an algorithm was considered correct if the\ractual ImageNet classification was among the $5$ classifications the\ralgorithm considered most likely.  By this top-$5$ criterion, KSH's\rdeep convolutional network achieved an accuracy of $84.7$ percent,\rvastly better than the next-best contest entry, which achieved an\raccuracy of $73.8$ percent.  Using the more restrictive metric of\rgetting the label exactly right, KSH's network achieved an accuracy of\r$63.3$ percent.",
            "It's worth briefly describing KSH's network, since it has inspired\rmuch subsequent work.  It's also, as we shall see, closely related to\rthe networks we trained earlier in this chapter, albeit more\relaborate.  KSH used a deep convolutional neural network, trained on\rtwo GPUs.  They used two GPUs because the particular type of GPU they\rwere using (an NVIDIA GeForce GTX 580) didn't have enough on-chip\rmemory to store their entire network.  So they split the network into\rtwo parts, partitioned across the two GPUs.",
            "The KSH network has $7$ layers of hidden neurons.  The first $5$\rhidden layers are convolutional layers (some with max-pooling), while\rthe next $2$ layers are fully-connected layers.  The output layer is a\r$1,000$-unit softmax layer, corresponding to the $1,000$ image\rclasses. Here's a sketch of the network, taken from the KSH\rpaper**Thanks to Ilya Sutskever..  The details are explained\rbelow.  Note that many layers are split into $2$ parts, corresponding\rto the $2$ GPUs.",
            "",
            "The input layer contains $3 \\times 224 \\times 224$ neurons,\rrepresenting the RGB values for a $224 \\times 224$ image.  Recall\rthat, as mentioned earlier, ImageNet contains images of varying\rresolution.  This poses a problem, since a neural network's input\rlayer is usually of a fixed size.  KSH dealt with this by rescaling\reach image so the shorter side had length $256$. They then cropped out\ra $256 \\times 256$ area in the center of the rescaled image.  Finally,\rKSH extracted random $224 \\times 224$ subimages (and horizontal\rreflections) from the $256 \\times 256$ images.  They did this random\rcropping as a way of expanding the training data, and thus reducing\roverfitting.  This is particularly helpful in a large network such as\rKSH's.  It was these $224 \\times 224$ images which were used as inputs\rto the network.  In most cases the cropped image still contains the\rmain object from the uncropped image.",
            "Moving on to the hidden layers in KSH's network, the first hidden\rlayer is a convolutional layer, with a max-pooling step.  It uses\rlocal receptive fields of size $11 \\times 11$, and a stride length of\r$4$ pixels.  There are a total of $96$ feature maps.  The feature maps\rare split into two groups of $48$ each, with the first $48$ feature\rmaps residing on one GPU, and the second $48$ feature maps residing on\rthe other GPU.  The max-pooling in this and later layers is done in $3\r\\times 3$ regions, but the pooling regions are allowed to overlap, and\rare just $2$ pixels apart.",
            "The second hidden layer is also a convolutional layer, with a\rmax-pooling step.  It uses $5 \\times 5$ local receptive fields, and\rthere's a total of $256$ feature maps, split into $128$ on each GPU.\rNote that the feature maps only use $48$ input channels, not the full\r$96$ output from the previous layer (as would usually be the case).\rThis is because any single feature map only uses inputs from the same\rGPU.  In this sense the network departs from the convolutional\rarchitecture we described earlier in the chapter, though obviously the\rbasic idea is still the same.",
            "The third, fourth and fifth hidden layers are convolutional layers,\rbut unlike the previous layers, they do not involve max-pooling.\rTheir respectives parameters are: (3) $384$ feature maps, with $3\r\\times 3$ local receptive fields, and $256$ input channels; (4) $384$\rfeature maps, with $3 \\times 3$ local receptive fields, and $192$\rinput channels; and (5) $256$ feature maps, with $3 \\times 3$ local\rreceptive fields, and $192$ input channels.  Note that the third layer\rinvolves some inter-GPU communication (as depicted in the figure) in\rorder that the feature maps use all $256$ input channels.",
            "The sixth and seventh hidden layers are fully-connected layers, with\r$4,096$ neurons in each layer.",
            "The output layer is a $1,000$-unit softmax layer.",
            "The KSH network takes advantage of many techniques.  Instead of using\rthe sigmoid or tanh activation functions, KSH use rectified linear\runits, which sped up training significantly.  KSH's network had\rroughly 60 million learned parameters, and was thus, even with the\rlarge training set, susceptible to overfitting.  To overcome this,\rthey expanded the training set using the random cropping strategy we\rdiscussed above.  They also further addressed overfitting by using a\rvariant ofl2 regularization, anddropout.\rThe network itself was trained usingmomentum-basedmini-batch stochastic gradient descent.",
            "That's an overview of many of the core ideas in the KSH paper.  I've\romitted some details, for which you should look at the paper.  You can\ralso look at Alex Krizhevsky'scuda-convnet(and\rsuccessors), which contains code implementing many of the ideas.  A\rTheano-based implementation has also been\rdeveloped**Theano-based\r    large-scale visual recognition with multiple GPUs, by Weiguang\r  Ding, Ruoyan Wang, Fei Mao, and Graham Taylor (2014)., with the\rcode availablehere.  The\rcode is recognizably along similar lines to that developed in this\rchapter, although the use of multiple GPUs complicates things\rsomewhat.  The Caffe neural nets framework also includes a version of\rthe KSH network, see theirModel Zoofor\rdetails.",
            "The 2014 ILSVRC competition:Since 2012, rapid progress\rcontinues to be made.  Consider the 2014 ILSVRC competition.  As in\r2012, it involved a training set of $1.2$ million images, in $1,000$\rcategories, and the figure of merit was whether the top $5$\rpredictions included the correct category.  The winning team, based\rprimarily at\rGoogle**Going deeper\r    with convolutions, by Christian Szegedy, Wei Liu, Yangqing Jia,\r  Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,\r  Vincent Vanhoucke, and Andrew Rabinovich (2014)., used a deep\rconvolutional network with $22$ layers of neurons.  They called their\rnetwork GoogLeNet, as a homage to LeNet-5.  GoogLeNet achieved a top-5\raccuracy of $93.33$ percent, a giant improvement over the 2013 winner\r(Clarifai, with $88.3$ percent), and\rthe 2012 winner (KSH, with $84.7$ percent).",
            "Just how good is GoogLeNet's $93.33$ percent accuracy?  In 2014 a team\rof researchers wrote a survey paper about the ILSVRC\rcompetition**ImageNet\r    large scale visual recognition challenge, by Olga Russakovsky,\r  Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\r  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,\r  Alexander C. Berg, and Li Fei-Fei (2014)..  One of the questions\rthey address is how well humans perform on ILSVRC.  To do this, they\rbuilt a system which lets humans classify ILSVRC images.  As one of\rthe authors, Andrej Karpathy, explains in an informativeblog\r  post, it was a lot of trouble to get the humans up to GoogLeNet's\rperformance:",
            "...the task of labeling images with 5 out of 1000\r  categories quickly turned out to be extremely challenging, even for\r  some friends in the lab who have been working on ILSVRC and its\r  classes for a while. First we thought we would put it up on [Amazon\r  Mechanical Turk]. Then we thought we could recruit paid\r  undergrads. Then I organized a labeling party of intense labeling\r  effort only among the (expert labelers) in our lab. Then I developed\r  a modified interface that used GoogLeNet predictions to prune the\r  number of categories from 1000 to only about 100. It was still too\r  hard - people kept missing categories and getting up to ranges of\r  13-15% error rates. In the end I realized that to get anywhere\r  competitively close to GoogLeNet, it was most efficient if I sat\r  down and went through the painfully long training process and the\r  subsequent careful annotation process myself... The labeling\r  happened at a rate of about 1 per minute, but this decreased over\r  time... Some images are easily recognized, while some images (such\r  as those of fine-grained breeds of dogs, birds, or monkeys) can\r  require multiple minutes of concentrated effort. I became very good\r  at identifying breeds of dogs... Based on the sample of images I\r  worked on, the GoogLeNet classification error turned out to be\r  6.8%... My own error in the end turned out to be 5.1%,\r  approximately 1.7% better.",
            "In other words, an expert human, working painstakingly, was with great\reffort able to narrowly beat the deep neural network.  In fact,\rKarpathy reports that a second human expert, trained on a smaller\rsample of images, was only able to attain a $12.0$ percent top-5 error\rrate, significantly below GoogLeNet's performance.  About half the\rerrors were due to the expert \"failing to spot and consider the\rground truth label as an option\".",
            "These are astonishing results.  Indeed, since this work, several teams\rhave reported systems whose top-5 error rate is actuallybetterthan 5.1%.  This has sometimes been reported in the media as the\rsystems having better-than-human vision.  While the results are\rgenuinely exciting, there are many caveats that make it misleading to\rthink of the systems as having better-than-human vision.  The ILSVRC\rchallenge is in many ways a rather limited problem - a crawl of the\ropen web is not necessarily representative of images found in\rapplications!  And, of course, the top-$5$ criterion is quite\rartificial.  We are still a long way from solving the problem of image\rrecognition or, more broadly, computer vision.  Still, it's extremely\rencouraging to see so much progress made on such a challenging\rproblem, over just a few years.",
            "Other activity:I've focused on ImageNet, but there's a\rconsiderable amount of other activity using neural nets to do image\rrecognition.  Let me briefly describe a few interesting recent\rresults, just to give the flavour of some current work.",
            "One encouraging practical set of results comes from a team at Google,\rwho applied deep convolutional networks to the problem of recognizing\rstreet numbers in Google's Street View\rimagery**Multi-digit\r    Number Recognition from Street View Imagery using Deep\r    Convolutional Neural Networks, by Ian J. Goodfellow, Yaroslav\r  Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet (2013)..  In\rtheir paper, they report detecting and automatically transcribing\rnearly 100 million street numbers at an accuracy similar to that of a\rhuman operator.  The system is fast: their system transcribed all of\rStreet View's images of street numbers in France in less than an hour!\rThey say: \"Having this new dataset significantly increased the\rgeocoding quality of Google Maps in several countries especially the\rones that did not already have other sources of good geocoding.\"  And\rthey go on to make the broader claim: \"We believe with this model we\rhave solved [optical character recognition] for short sequences [of\rcharacters] for many applications.\"",
            "I've perhaps given the impression that it's all a parade of\rencouraging results.  Of course, some of the most interesting work\rreports on fundamental things we don't yet understand.  For instance,\ra 2013 paper**Intriguing\r    properties of neural networks, by Christian Szegedy, Wojciech\r  Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\r  and Rob Fergus (2013)showed that deep networks may suffer from\rwhat are effectively blind spots.  Consider the lines of images below.\rOn the left is an ImageNet image classified correctly by their\rnetwork.  On the right is a slightly perturbed image (the perturbation\ris in the middle) which is classifiedincorrectlyby the\rnetwork.  The authors found that there are such \"adversarial\" images\rfor every sample image, not just a few special ones.",
            "",
            "This is a disturbing result.  The paper used a network based on the\rsame code as KSH's network - that is, just the type of network that\ris being increasingly widely used.  While such neural networks compute\rfunctions which are, in principle, continuous, results like this\rsuggest that in practice they're likely to compute functions which are\rvery nearly discontinuous.  Worse, they'll be discontinuous in ways\rthat violate our intuition about what is reasonable behavior.  That's\rconcerning.  Furthermore, it's not yet well understood what's causing\rthe discontinuity: is it something about the loss function?  The\ractivation functions used?  The architecture of the network?\rSomething else?  We don't yet know.",
            "Now, these results are not quite as bad as they sound.  Although such\radversarial images are common, they're also unlikely in practice.  As\rthe paper notes:",
            "The existence of the adversarial negatives appears to be in\r  contradiction with the network’s ability to achieve high\r  generalization performance. Indeed, if the network can generalize\r  well, how can it be confused by these adversarial negatives, which\r  are indistinguishable from the regular examples? The explanation is\r  that the set of adversarial negatives is of extremely low\r  probability, and thus is never (or rarely) observed in the test set,\r  yet it is dense (much like the rational numbers), and so it is found\r  near virtually every test case.",
            "Nonetheless, it is distressing that we understand neural nets so\rpoorly that this kind of result should be a recent discovery.  Of\rcourse, a major benefit of the results is that they have stimulated\rmuch followup work.  For example, one recent\rpaper**Deep Neural\r    Networks are Easily Fooled: High Confidence Predictions for\r    Unrecognizable Images, by Anh Nguyen, Jason Yosinski, and Jeff\r  Clune (2014).shows that given a trained network it's possible to\rgenerate images which look to a human like white noise, but which the\rnetwork classifies as being in a known category with a very high\rdegree of confidence.  This is another demonstration that we have a\rlong way to go in understanding neural networks and their use in image\rrecognition.",
            "Despite results like this, the overall picture is encouraging.  We're\rseeing rapid progress on extremely difficult benchmarks, like\rImageNet.  We're also seeing rapid progress in the solution of\rreal-world problems, like recognizing street numbers in StreetView.\rBut while this is encouraging it's not enough just to see improvements\ron benchmarks, or even real-world applications.  There are fundamental\rphenomena which we still understand poorly, such as the existence of\radversarial images.  When such fundamental problems are still being\rdiscovered (never mind solved), it is premature to say that we're near\rsolving the problem of image recognition.  At the same time such\rproblems are an exciting stimulus to further work.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "Other approaches to deep neural nets"
        ],
        "Other approaches to deep neural nets": [
            "Through this book, we've concentrated on a single problem: classifying\rthe MNIST digits.  It's a juicy problem which forced us to understand\rmany powerful ideas: stochastic gradient descent, backpropagation,\rconvolutional nets, regularization, and more.  But it's also a narrow\rproblem.  If you read the neural networks literature, you'll run into\rmany ideas we haven't discussed: recurrent neural networks, Boltzmann\rmachines, generative models, transfer learning, reinforcement\rlearning, and so on, on and on $\\ldots$ and on!  Neural networks is a\rvast field.  However, many important ideas are variations on ideas\rwe've already discussed, and can be understood with a little effort.\rIn this section I provide a glimpse of these as yet unseen vistas.\rThe discussion isn't detailed, nor comprehensive - that would\rgreatly expand the book.  Rather, it's impressionistic, an attempt to\revoke the conceptual richness of the field, and to relate some of\rthose riches to what we've already seen.  Through the section, I'll\rprovide a few links to other sources, as entrees to learn more.  Of\rcourse, many of these links will soon be superseded, and you may wish\rto search out more recent literature.  That point notwithstanding, I\rexpect many of the underlying ideas to be of lasting interest.",
            "Recurrent neural networks (RNNs):In the feedforward nets\rwe've been using there is a single input which completely determines\rthe activations of all the neurons through the remaining layers.  It's\ra very static picture: everything in the network is fixed, with a\rfrozen, crystalline quality to it.  But suppose we allow the elements\rin the network to keep changing in a dynamic way.  For instance, the\rbehaviour of hidden neurons might not just be determined by the\ractivations in previous hidden layers, but also by the activations at\rearlier times.  Indeed, a neuron's activation might be determined in\rpart by its own activation at an earlier time.  That's certainly not\rwhat happens in a feedforward network.  Or perhaps the activations of\rhidden and output neurons won't be determined just by the current\rinput to the network, but also by earlier inputs.",
            "Neural networks with this kind of time-varying behaviour are known asrecurrent neural networksorRNNs.  There are many\rdifferent ways of mathematically formalizing the informal description\rof recurrent nets given in the last paragraph.  You can get the\rflavour of some of these mathematical models by glancing atthe\r  Wikipedia article on RNNs.  As I write, that page lists no fewer\rthan 13 different models.  But mathematical details aside, the broad\ridea is that RNNs are neural networks in which there is some notion of\rdynamic change over time.  And, not surprisingly, they're particularly\ruseful in analysing data or processes that change over time.  Such\rdata and processes arise naturally in problems such as speech or\rnatural language, for example.",
            "One way RNNs are currently being used is to connect neural networks\rmore closely to traditional ways of thinking about algorithms, ways of\rthinking based on concepts such as Turing machines and (conventional)\rprogramming languages.A 2014\r  paperdeveloped an RNN which could take as input a\rcharacter-by-character description of a (very, very simple!) Python\rprogram, and use that description to predict the output.  Informally,\rthe network is learning to \"understand\" certain Python programs.A second paper, also from 2014,\rused RNNs as a starting point to develop what they called a neural\rTuring machine (NTM).  This is a universal computer whose entire\rstructure can be trained using gradient descent.  They trained their\rNTM to infer algorithms for several simple problems, such as sorting\rand copying.",
            "As it stands, these are extremely simple toy models.  Learning to\rexecute the Python programprint(398345+42598)doesn't make a\rnetwork into a full-fledged Python interpreter!  It's not clear how\rmuch further it will be possible to push the ideas.  Still, the\rresults are intriguing.  Historically, neural networks have done well\rat pattern recognition problems where conventional algorithmic\rapproaches have trouble.  Vice versa, conventional algorithmic\rapproaches are good at solving problems that neural nets aren't so\rgood at.  No-one today implements a web server or a database program\rusing a neural network!  It'd be great to develop unified models that\rintegrate the strengths of both neural networks and more traditional\rapproaches to algorithms.  RNNs and ideas inspired by RNNs may help us\rdo that.",
            "RNNs have also been used in recent years to attack many other\rproblems.  They've been particularly useful in speech recognition.\rApproaches based on RNNs have, for example,set records for the accuracy of\r  phoneme recognition.  They've also been used to developimproved\r  models of the language people use while speaking.  Better language\rmodels help disambiguate utterances that otherwise sound alike.  A\rgood language model will, for example, tell us that \"to infinity and\rbeyond\" is much more likely than \"two infinity and beyond\", despite\rthe fact that the phrases sound identical.  RNNs have been used to set\rnew records for certain language benchmarks.",
            "This work is, incidentally, part of a broader use of deep neural nets\rof all types, not just RNNs, in speech recognition.  For example, an\rapproach based on deep nets has achievedoutstanding results on large\r  vocabulary continuous speech recognition.  And another system based\ron deep nets has been deployed inGoogle's\r  Android operating system(for related technical work, seeVincent\r  Vanhoucke's 2012-2015 papers).",
            "I've said a little about what RNNs can do, but not so much about how\rthey work.  It perhaps won't surprise you to learn that many of the\rideas used in feedforward networks can also be used in RNNs.  In\rparticular, we can train RNNs using straightforward modifications to\rgradient descent and backpropagation.  Many other ideas used in\rfeedforward nets, ranging from regularization techniques to\rconvolutions to the activation and cost functions used, are also\ruseful in recurrent nets.  And so many of the techniques we've\rdeveloped in the book can be adapted for use with RNNs.",
            "",
            "",
            "",
            "Long short-term memory units (LSTMs):One challenge affecting\rRNNs is that early models turned out to be very difficult to train,\rharder even than deep feedforward networks.  The reason is the\runstable gradient problem discussed inChapter 5.\rRecall that the usual manifestation of this problem is that the\rgradient gets smaller and smaller as it is propagated back through\rlayers.  This makes learning in early layers extremely slow.  The\rproblem actually gets worse in RNNs, since gradients aren't just\rpropagated backward through layers, they're propagated backward\rthrough time.  If the network runs for a long time that can make the\rgradient extremely unstable and hard to learn from.  Fortunately, it's\rpossible to incorporate an idea known as long short-term memory units\r(LSTMs) into RNNs.  The units were introduced byHochreiter and\r  Schmidhuber in 1997with the explicit purpose of helping address\rthe unstable gradient problem.  LSTMs make it much easier to get good\rresults when training RNNs, and many recent papers (including many\rthat I linked above) make use of LSTMs or related ideas.",
            "Deep belief nets, generative models, and Boltzmann machines:Modern interest in deep learning began in 2006, with papers explaining\rhow to train a type of neural network known as adeep belief\r  network(DBN)**SeeA fast\r    learning algorithm for deep belief nets, by Geoffrey Hinton,\r  Simon Osindero, and Yee-Whye Teh (2006), as well as the related work\r  inReducing\r    the dimensionality of data with neural networks, by Geoffrey\r  Hinton and Ruslan Salakhutdinov (2006)..  DBNs were influential for\rseveral years, but have since lessened in popularity, while models\rsuch as feedforward networks and recurrent neural nets have become\rfashionable.  Despite this, DBNs have several properties that make\rthem interesting.",
            "One reason DBNs are interesting is that they're an example of what's\rcalled agenerative model.  In a feedforward network, we\rspecify the input activations, and they determine the activations of\rthe feature neurons later in the network.  A generative model like a\rDBN can be used in a similar way, but it's also possible to specify\rthe values of some of the feature neurons and then \"run the network\rbackward\", generating values for the input activations.  More\rconcretely, a DBN trained on images of handwritten digits can\r(potentially, and with some care) also be used to generate images that\rlook like handwritten digits.  In other words, the DBN would in some\rsense be learning to write.  In this, a generative model is much like\rthe human brain: not only can it read digits, it can also write them.\rIn Geoffrey Hinton's memorable phrase,to\r  recognize shapes, first learn to generate images.",
            "A second reason DBNs are interesting is that they can do unsupervised\rand semi-supervised learning.  For instance, when trained with image\rdata, DBNs can learn useful features for understanding other images,\reven if the training images are unlabelled.  And the ability to do\runsupervised learning is extremely interesting both for fundamental\rscientific reasons, and - if it can be made to work well enough -\rfor practical applications.",
            "Given these attractive features, why have DBNs lessened in popularity\ras models for deep learning?  Part of the reason is that models such\ras feedforward and recurrent nets have achieved many spectacular\rresults, such as their breakthroughs on image and speech recognition\rbenchmarks.  It's not surprising and quite right that there's now lots\rof attention being paid to these models.  There's an unfortunate\rcorollary, however.  The marketplace of ideas often functions in a\rwinner-take-all fashion, with nearly all attention going to the\rcurrent fashion-of-the-moment in any given area.  It can become\rextremely difficult for people to work on momentarily unfashionable\rideas, even when those ideas are obviously of real long-term interest.\rMy personal opinion is that DBNs and other generative models likely\rdeserve more attention than they are currently receiving.  And I won't\rbe surprised if DBNs or a related model one day surpass the currently\rfashionable models.  For an introduction to DBNs, seethis\r  overview.  I've also foundthis\r  articlehelpful.  It isn't primarily about deep belief nets,per se, but does contain much useful information about\rrestricted Boltzmann machines, which are a key component of DBNs.",
            "Other ideas:What else is going on in neural networks and\rdeep learning?  Well, there's a huge amount of other fascinating work.\rActive areas of research include using neural networks to donatural\r  language processing(seealso\r  this informative review paper),machine translation, as well as\rperhaps more surprising applications such asmusic\r  informatics.  There are, of course, many other areas too.  In many\rcases, having read this book you should be able to begin following\rrecent work, although (of course) you'll need to fill in gaps in\rpresumed background knowledge.",
            "Let me finish this section by mentioning a particularly fun paper.  It\rcombines deep convolutional networks with a technique known as\rreinforcement learning in order to learn toplay video games\r  well(see alsothis\r  followup).  The idea is to use the convolutional network to\rsimplify the pixel data from the game screen, turning it into a\rsimpler set of features, which can be used to decide which action to\rtake: \"go left\", \"go down\", \"fire\", and so on.  What is\rparticularly interesting is that a single network learned to play\rseven different classic video games pretty well, outperforming human\rexperts on three of the games.  Now, this all sounds like a stunt, and\rthere's no doubt the paper was well marketed, with the title \"Playing\rAtari with reinforcement learning\".  But looking past the surface\rgloss, consider that this system is taking raw pixel data - it\rdoesn't even know the game rules! - and from that data learning to\rdo high-quality decision-making in several very different and very\radversarial environments, each with its own complex set of rules.\rThat's pretty neat.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "On the future of neural networks"
        ],
        "On the future of neural networks": [
            "Intention-driven user interfaces:There's an old joke in\rwhich an impatient professor tells a confused student: \"don't listen\rto what I say; listen to what Imean\".  Historically,\rcomputers have often been, like the confused student, in the dark\rabout what their users mean.  But this is changing.  I still remember\rmy surprise the first time I misspelled a Google search query, only to\rhave Google say \"Did you mean [corrected query]?\" and to offer the\rcorresponding search results.  Google CEO Larry Pageonce\r  described the perfect search engine as understanding exactly what\r  [your queries] mean and giving you back exactly what you want.",
            "This is a vision of anintention-driven user interface.  In\rthis vision, instead of responding to users' literal queries, search\rwill use machine learning to take vague user input, discern precisely\rwhat was meant, and take action on the basis of those insights.",
            "The idea of intention-driven interfaces can be applied far more\rbroadly than search.  Over the next few decades, thousands of\rcompanies will build products which use machine learning to make user\rinterfaces that can tolerate imprecision, while discerning and acting\ron the user's true intent.  We're already seeing early examples of\rsuch intention-driven interfaces: Apple's Siri; Wolfram Alpha; IBM's\rWatson; systems which canannotate photos and videos; and\rmuch more.",
            "Most of these products will fail. Inspired user interface design is\rhard, and I expect many companies will take powerful machine learning\rtechnology and use it to build insipid user interfaces.  The best\rmachine learning in the world won't help if your user interface\rconcept stinks.  But there will be a residue of products which\rsucceed.  Over time that will cause a profound change in how we relate\rto computers.  Not so long ago - let's say, 2005 - users took it\rfor granted that they needed precision in most interactions with\rcomputers.  Indeed, computer literacy to a great extent meant\rinternalizing the idea that computers are extremely literal; a single\rmisplaced semi-colon may completely change the nature of an\rinteraction with a computer.  But over the next few decades I expect\rwe'll develop many successful intention-driven user interfaces, and\rthat will dramatically change what we expect when interacting with\rcomputers.",
            "Machine learning, data science, and the virtuous circle of\r  innovation:Of course, machine learning isn't just being used to\rbuild intention-driven interfaces.  Another notable application is in\rdata science, where machine learning is used to find the \"known\runknowns\" hidden in data.  This is already a fashionable area, and\rmuch has been written about it, so I won't say much.  But I do want to\rmention one consequence of this fashion that is not so often remarked:\rover the long run it's possible the biggest breakthrough in machine\rlearning won't be any single conceptual breakthrough. Rather, the\rbiggest breakthrough will be that machine learning research becomes\rprofitable, through applications to data science and other areas.  If\ra company can invest 1 dollar in machine learning research and get 1\rdollar and 10 cents back reasonably rapidly, then a lot of money will\rend up in machine learning research.  Put another way, machine\rlearning is an engine driving the creation of several major new\rmarkets and areas of growth in technology. The result will be large\rteams of people with deep subject expertise, and with access to\rextraordinary resources.  That will propel machine learning further\rforward, creating more markets and opportunities, a virtuous circle of\rinnovation.",
            "The role of neural networks and deep learning:I've been\rtalking broadly about machine learning as a creator of new\ropportunities for technology.  What will be the specific role of\rneural networks and deep learning in all this?",
            "To answer the question, it helps to look at history.  Back in the\r1980s there was a great deal of excitement and optimism about neural\rnetworks, especially after backpropagation became widely known.  That\rexcitement faded, and in the 1990s the machine learning baton passed\rto other techniques, such as support vector machines.  Today, neural\rnetworks are again riding high, setting all sorts of records,\rdefeating all comers on many problems.  But who is to say that\rtomorrow some new approach won't be developed that sweeps neural\rnetworks away again?  Or perhaps progress with neural networks will\rstagnate, and nothing will immediately arise to take their place?",
            "For this reason, it's much easier to think broadly about the future of\rmachine learning than about neural networks specifically.  Part of the\rproblem is that we understand neural networks so poorly.  Why is it\rthat neural networks can generalize so well?  How is it that they\ravoid overfitting as well as they do, given the very large number of\rparameters they learn?  Why is it that stochastic gradient descent\rworks as well as it does?  How well will neural networks perform as\rdata sets are scaled?  For instance, if ImageNet was expanded by a\rfactor of $10$, would neural networks' performance improve more or\rless than other machine learning techniques?  These are all simple,\rfundamental questions.  And, at present, we understand the answers to\rthese questions very poorly.  While that's the case, it's difficult to\rsay what role neural networks will play in the future of machine\rlearning.",
            "I will make one prediction: I believe deep learning is here to stay.\rThe ability to learn hierarchies of concepts, building up multiple\rlayers of abstraction, seems to be fundamental to making sense of the\rworld.  This doesn't mean tomorrow's deep learners won't be radically\rdifferent than today's.  We could see major changes in the constituent\runits used, in the architectures, or in the learning algorithms.\rThose changes may be dramatic enough that we no longer think of the\rresulting systems as neural networks.  But they'd still be doing deep\rlearning.",
            "",
            "Will neural networks and deep learning soon lead to artificial\r  intelligence?In this book we've focused on using neural nets to\rdo specific tasks, such as classifying images.  Let's broaden our\rambitions, and ask: what about general-purpose thinking computers?\rCan neural networks and deep learning help us solve the problem of\r(general) artificial intelligence (AI)?  And, if so, given the rapid\rrecent progress of deep learning, can we expect general AI any time\rsoon?",
            "Addressing these questions comprehensively would take a separate book.\rInstead, let me offer one observation.  It's based on an idea known asConway's law:Any organization that designs a system... will inevitably produce a\r  design whose structure is a copy of the organization's communication\r  structure.So, for example, Conway's law suggests that the design of a Boeing 747\raircraft will mirror the extended organizational structure of Boeing\rand its contractors at the time the 747 was designed.  Or for a\rsimple, specific example, consider a company building a complex\rsoftware application.  If the application's dashboard is supposed to\rbe integrated with some machine learning algorithm, the person\rbuilding the dashboard better be talking to the company's machine\rlearning expert.  Conway's law is merely that observation, writ large.",
            "Upon first hearing Conway's law, many people respond either \"Well,\risn't that banal and obvious?\" or \"Isn't that wrong?\"  Let me start\rwith the objection that it's wrong.  As an instance of this objection,\rconsider the question: where does Boeing's accounting department show\rup in the design of the 747?  What about their janitorial department?\rTheir internal catering?  And the answer is that these parts of the\rorganization probably don't show up explicitly anywhere in the 747.\rSo we should understand Conway's law as referring only to those parts\rof an organization concerned explicitly with design and engineering.",
            "What about the other objection, that Conway's law is banal and\robvious?  This may perhaps be true, but I don't think so, for\rorganizations too often act with disregard for Conway's law.  Teams\rbuilding new products are often bloated with legacy hires or,\rcontrariwise, lack a person with some crucial expertise.  Think of all\rthe products which have useless complicating features.  Or think of\rall the products which have obvious major deficiencies - e.g., a\rterrible user interface.  Problems in both classes are often caused by\ra mismatch between the team that was needed to produce a good product,\rand the team that was actually assembled.  Conway's law may be\robvious, but that doesn't mean people don't routinely ignore it.",
            "Conway's law applies to the design and engineering of systems where we\rstart out with a pretty good understanding of the likely constituent\rparts, and how to build them.  It can't be applied directly to the\rdevelopment of artificial intelligence, because AI isn't (yet) such a\rproblem: we don't know what the constituent parts are.  Indeed, we're\rnot even sure what basic questions to be asking.  In others words, at\rthis point AI is more a problem of science than of engineering.\rImagine beginning the design of the 747 without knowing about jet\rengines or the principles of aerodynamics.  You wouldn't know what\rkinds of experts to hire into your organization.  As Wernher von Braun\rput it, \"basic research is what I'm doing when I don't know what I'm\rdoing\".  Is there a version of Conway's law that applies to problems\rwhich are more science than engineering?",
            "To gain insight into this question, consider the history of medicine.\rIn the early days, medicine was the domain of practitioners like Galen\rand Hippocrates, who studied the entire body.  But as our knowledge\rgrew, people were forced to specialize.  We discovered many deep new\rideas**My apologies for overloading \"deep\".  I won't define\r  \"deep ideas\" precisely, but loosely I mean the kind of idea which\r  is the basis for a rich field of enquiry.  The backpropagation\r  algorithm and the germ theory of disease are both good examples.:\rthink of things like the germ theory of disease, for instance, or the\runderstanding of how antibodies work, or the understanding that the\rheart, lungs, veins and arteries form a complete cardiovascular\rsystem.  Such deep insights formed the basis for subfields such as\repidemiology, immunology, and the cluster of inter-linked fields\raround the cardiovascular system.  And so the structure of our\rknowledge has shaped the social structure of medicine.  This is\rparticularly striking in the case of immunology: realizing the immune\rsystem exists and is a system worthy of study is an extremely\rnon-trivial insight.  So we have an entire field of medicine - with\rspecialists, conferences, even prizes, and so on - organized around\rsomething which is not just invisible, it's arguably not a distinct\rthing at all.",
            "This is a common pattern that has been repeated in many\rwell-established sciences: not just medicine, but physics,\rmathematics, chemistry, and others.  The fields start out monolithic,\rwith just a few deep ideas.  Early experts can master all those ideas.\rBut as time passes that monolithic character changes.  We discover\rmany deep new ideas, too many for any one person to really master.  As\ra result, the social structure of the field re-organizes and divides\raround those ideas.  Instead of a monolith, we have fields within\rfields within fields, a complex, recursive, self-referential social\rstructure, whose organization mirrors the connections between our\rdeepest insights.And so the structure of our knowledge shapes\r  the social organization of science.  But that social shape in turn\r  constrains and helps determine what we can discover.This is the\r  scientific analogue of Conway's law.",
            "",
            "So what's this got to do with deep learning or AI?",
            "Well, since the early days of AI there have been arguments about it\rthat go, on one side, \"Hey, it's not going to be so hard, we've got\r[super-special weapon] on our side\", countered by \"[super-special\rweapon] won't be enough\".  Deep learning is the latest super-special\rweapon I've heard used in such arguments**Interestingly, often\r  not by leading experts in deep learning, who have been quite\r  restrained.  See, for example, thisthoughtful\r    postby Yann LeCun. This is a difference from many earlier\r  incarnations of the argument.; earlier versions of the argument\rused logic, or Prolog, or expert systems, or whatever the most\rpowerful technique of the day was.  The problem with such arguments is\rthat they don't give you any good way of saying just how powerful any\rgiven candidate super-special weapon is.  Of course, we've just spent\ra chapter reviewing evidence that deep learning can solve extremely\rchallenging problems.  It certainly looks very exciting and promising.\rBut that was also true of systems like Prolog orEuriskoor expert systems\rin their day.  And so the mere fact that a set of ideas looks very\rpromising doesn't mean much.  How can we tell if deep learning is\rtruly different from these earlier ideas?  Is there some way of\rmeasuring how powerful and promising a set of ideas is?  Conway's law\rsuggests that as a rough and heuristic proxy metric we can evaluate\rthe complexity of the social structure associated to those ideas.",
            "So, there are two questions to ask.  First, how powerful a set of\rideas are associated to deep learning, according to this metric of\rsocial complexity?  Second, how powerful a theory will we need, in\rorder to be able to build a general artificial intelligence?",
            "As to the first question: when we look at deep learning today, it's an\rexciting and fast-paced but also relatively monolithic field.  There\rare a few deep ideas, and a few main conferences, with substantial\roverlap between several of the conferences.  And there is paper after\rpaper leveraging the same basic set of ideas: using stochastic\rgradient descent (or a close variation) to optimize a cost function.\rIt's fantastic those ideas are so successful.  But what we don't yet\rsee is lots of well-developed subfields, each exploring their own sets\rof deep ideas, pushing deep learning in many directions.  And so,\raccording to the metric of social complexity, deep learning is, if\ryou'll forgive the play on words, still a rather shallow field.  It's\rstill possible for one person to master most of the deepest ideas in\rthe field.",
            "On the second question: how complex and powerful a set of ideas will\rbe needed to obtain AI?  Of course, the answer to this question is:\rno-one knows for sure.  But in theappendixI examine\rsome of the existing evidence on this question.  I conclude that, even\rrather optimistically, it's going to take many, many deep ideas to\rbuild an AI.  And so Conway's law suggests that to get to such a point\rwe will necessarily see the emergence of many interrelating\rdisciplines, with a complex and surprising structure mirroring the\rstructure in our deepest insights.  We don't yet see this rich social\rstructure in the use of neural networks and deep learning.  And so, I\rbelieve that we are several decades (at least) from using deep\rlearning to develop general AI.",
            "I've gone to a lot of trouble to construct an argument which is\rtentative, perhaps seems rather obvious, and which has an indefinite\rconclusion.  This will no doubt frustrate people who crave certainty.\rReading around online, I see many people who loudly assert very\rdefinite, very strongly held opinions about AI, often on the basis of\rflimsy reasoning and non-existent evidence.  My frank opinion is this:\rit's too early to say.  As the old joke goes, if you ask a scientist\rhow far away some discovery is and they say \"10 years\" (or more),\rwhat they mean is \"I've got no idea\".  AI, like controlled fusion\rand a few other technologies, has been 10 years away for 60 plus\ryears.  On the flipside, what we definitely do have in deep learning\ris a powerful technique whose limits have not yet been found, and many\rwide-open fundamental problems.  That's an exciting creative\ropportunity.",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ]
    }
}